{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Model",
      "provenance": [],
      "collapsed_sections": [
        "kxL5oVlOj0yG",
        "_BPUojn71Pbi",
        "lLJF0LJ1cl42",
        "6KmduYKb_OGE",
        "g0cs52sdnnvI",
        "aIIKwIH7ttGF",
        "_5_VFBfdAGtU",
        "zNFtaGe_OcBd",
        "w5ECIHgicxSD",
        "j0K4yBPBILNB",
        "FRM1Jrfqntz7"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ENEE439d-TEXTML/TextML/blob/master/Sentiment_Model%20copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "kxL5oVlOj0yG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tutorial site: \n",
        "\n",
        "https://towardsdatascience.com/sentiment-analysis-using-lstm-and-glove-embeddings-99223a87fe8e\n",
        "\n",
        "https://towardsdatascience.com/word-embeddings-for-sentiment-analysis-65f42ea5d26e"
      ],
      "metadata": {
        "id": "_BPUojn71Pbi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJXWN-MVX38W",
        "outputId": "957fab97-7aef-4571-ffea-ca5d846fb0b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data  = pd.read_csv(\"/content/drive/MyDrive/UMD - senior year/spring 2022/439D/project/data.csv\")"
      ],
      "metadata": {
        "id": "j2p0QDZ1YpJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineeing: Fix Up Data"
      ],
      "metadata": {
        "id": "lLJF0LJ1cl42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup necessary libraries\n",
        "> the imports and stuff"
      ],
      "metadata": {
        "id": "6KmduYKb_OGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import all necessary libraries for this tutorial\n",
        "\n",
        "import re\n",
        "import collections\n",
        "\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras import models\n",
        "from keras import layers\n",
        "import keras"
      ],
      "metadata": {
        "id": "EjWwnQx51f3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "9NeRxCOV305A",
        "outputId": "47133627-c8af-4629-ad7e-9db2fa40be77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   no     paper cited_paper  label  \\\n",
              "0   0  A00-1043    A00-2024      0   \n",
              "1   1  H05-1033    A00-2024      0   \n",
              "2   2  I05-2009    A00-2024      0   \n",
              "3   3  I05-2009    A00-2024      0   \n",
              "4   4  I05-2009    A00-2024      0   \n",
              "\n",
              "                                                text  \n",
              "0  We analyzed a set of articles and identified s...  \n",
              "1  Table 3: Example compressions Compression AvgL...  \n",
              "2  5.3 Related works and discussion Our two-step ...  \n",
              "3  (1999) proposed a summarization system based o...  \n",
              "4  We found that the deletion of lead parts did n...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d84158f1-95fe-47f4-94f0-3d799d8f839e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>no</th>\n",
              "      <th>paper</th>\n",
              "      <th>cited_paper</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A00-1043</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We analyzed a set of articles and identified s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>H05-1033</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>Table 3: Example compressions Compression AvgL...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>I05-2009</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>5.3 Related works and discussion Our two-step ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>I05-2009</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>(1999) proposed a summarization system based o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>I05-2009</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We found that the deletion of lead parts did n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d84158f1-95fe-47f4-94f0-3d799d8f839e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d84158f1-95fe-47f4-94f0-3d799d8f839e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d84158f1-95fe-47f4-94f0-3d799d8f839e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing non-alphabetic characters\n",
        "> remove words and characters that aren't useful from sentences in dataset"
      ],
      "metadata": {
        "id": "g0cs52sdnnvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define functions to things with no sentiment value (irrelevant words)\n",
        "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \n",
        "             \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\",\n",
        "             \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \n",
        "             \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\",\n",
        "             \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\",\n",
        "             \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \n",
        "             \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\",\n",
        "             \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\",\n",
        "             \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\",\n",
        "             \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\",\n",
        "             \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "characters = [\"\"]\n",
        "\n",
        "#function to remove stopwords\n",
        "def remove_stopwords(data):\n",
        "  data['review_without_stopwords'] = data['text'].apply(lambda x : ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
        "  return data\n",
        "\n",
        "#function to remove non-alphabetical tags\n",
        "def remove_tags(string):\n",
        "    #result = re.sub('[\\d<.*?:>()-,;|/@!#$%^&*~`_=+]','',string)\n",
        "\n",
        "    pattern = re.compile('[\\W_0-9]+')\n",
        "    dirty_list = string.split()\n",
        "    clean_list = [pattern.sub('', word) for word in dirty_list]\n",
        "    result = ' '.join(clean_list)\n",
        "    \n",
        "    # result = re.sub('[\\W_0-9]+','',string)    #see https://blog.finxter.com/how-to-remove-all-non-alphabet-characters-from-a-string/ for explanation\n",
        "    # result = re.sub('  ',' ',result)\n",
        "    return result"
      ],
      "metadata": {
        "id": "4Xfp9GbE6s9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove stopwords\n",
        "clean_dataset = remove_stopwords(data)\n",
        "clean_dataset['w/o stopwords or tags']= clean_dataset['review_without_stopwords'].apply(lambda cw : remove_tags(cw))\n",
        "clean_dataset"
      ],
      "metadata": {
        "id": "Tlq-liJ-Gpb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f817559a-3aea-4d41-da31-0160d2110e31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      no     paper cited_paper  label  \\\n",
              "0      0  A00-1043    A00-2024      0   \n",
              "1      1  H05-1033    A00-2024      0   \n",
              "2      2  I05-2009    A00-2024      0   \n",
              "3      3  I05-2009    A00-2024      0   \n",
              "4      4  I05-2009    A00-2024      0   \n",
              "..   ...       ...         ...    ...   \n",
              "994  994  N09-1053    J92-4003      0   \n",
              "995  995  P01-1046    J92-4003      0   \n",
              "996  996  P01-1046    J92-4003      0   \n",
              "997  997  P01-1068    J92-4003      0   \n",
              "998  998  P02-1016    J92-4003      0   \n",
              "\n",
              "                                                  text  \\\n",
              "0    We analyzed a set of articles and identified s...   \n",
              "1    Table 3: Example compressions Compression AvgL...   \n",
              "2    5.3 Related works and discussion Our two-step ...   \n",
              "3    (1999) proposed a summarization system based o...   \n",
              "4    We found that the deletion of lead parts did n...   \n",
              "..                                                 ...   \n",
              "994  While we can only compare class models with wo...   \n",
              "995  (1999) and Lee (1999)) can be generally divide...   \n",
              "996  Classes can be induced directly from the corpu...   \n",
              "997  And we consider that word pairs that have a sm...   \n",
              "998  Words are encoded through an automatic cluster...   \n",
              "\n",
              "                              review_without_stopwords  \\\n",
              "0    We analyzed set articles identified six major ...   \n",
              "1    Table 3: Example compressions Compression AvgL...   \n",
              "2    5.3 Related works discussion Our two-step mode...   \n",
              "3    (1999) proposed summarization system based dra...   \n",
              "4    We found deletion lead parts not occur often s...   \n",
              "..                                                 ...   \n",
              "994  While can compare class models word models lar...   \n",
              "995  (1999) Lee (1999)) can generally divided three...   \n",
              "996  Classes can induced directly corpus (Pereira e...   \n",
              "997  And consider word pairs small distance vectors...   \n",
              "998  Words encoded automatic clustering algorithm (...   \n",
              "\n",
              "                                 w/o stopwords or tags  \n",
              "0    We analyzed set articles identified six major ...  \n",
              "1    Table  Example compressions Compression AvgLen...  \n",
              "2     Related works discussion Our twostep model es...  \n",
              "3     proposed summarization system based draft rev...  \n",
              "4    We found deletion lead parts not occur often s...  \n",
              "..                                                 ...  \n",
              "994  While can compare class models word models lar...  \n",
              "995   Lee  can generally divided three types discou...  \n",
              "996  Classes can induced directly corpus Pereira et...  \n",
              "997  And consider word pairs small distance vectors...  \n",
              "998  Words encoded automatic clustering algorithm B...  \n",
              "\n",
              "[999 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-19335d9d-7a6e-4164-8d03-918c80ac5d18\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>no</th>\n",
              "      <th>paper</th>\n",
              "      <th>cited_paper</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>review_without_stopwords</th>\n",
              "      <th>w/o stopwords or tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A00-1043</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We analyzed a set of articles and identified s...</td>\n",
              "      <td>We analyzed set articles identified six major ...</td>\n",
              "      <td>We analyzed set articles identified six major ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>H05-1033</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>Table 3: Example compressions Compression AvgL...</td>\n",
              "      <td>Table 3: Example compressions Compression AvgL...</td>\n",
              "      <td>Table  Example compressions Compression AvgLen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>I05-2009</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>5.3 Related works and discussion Our two-step ...</td>\n",
              "      <td>5.3 Related works discussion Our two-step mode...</td>\n",
              "      <td>Related works discussion Our twostep model es...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>I05-2009</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>(1999) proposed a summarization system based o...</td>\n",
              "      <td>(1999) proposed summarization system based dra...</td>\n",
              "      <td>proposed summarization system based draft rev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>I05-2009</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We found that the deletion of lead parts did n...</td>\n",
              "      <td>We found deletion lead parts not occur often s...</td>\n",
              "      <td>We found deletion lead parts not occur often s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>994</td>\n",
              "      <td>N09-1053</td>\n",
              "      <td>J92-4003</td>\n",
              "      <td>0</td>\n",
              "      <td>While we can only compare class models with wo...</td>\n",
              "      <td>While can compare class models word models lar...</td>\n",
              "      <td>While can compare class models word models lar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>995</td>\n",
              "      <td>P01-1046</td>\n",
              "      <td>J92-4003</td>\n",
              "      <td>0</td>\n",
              "      <td>(1999) and Lee (1999)) can be generally divide...</td>\n",
              "      <td>(1999) Lee (1999)) can generally divided three...</td>\n",
              "      <td>Lee  can generally divided three types discou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>996</td>\n",
              "      <td>P01-1046</td>\n",
              "      <td>J92-4003</td>\n",
              "      <td>0</td>\n",
              "      <td>Classes can be induced directly from the corpu...</td>\n",
              "      <td>Classes can induced directly corpus (Pereira e...</td>\n",
              "      <td>Classes can induced directly corpus Pereira et...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>997</td>\n",
              "      <td>P01-1068</td>\n",
              "      <td>J92-4003</td>\n",
              "      <td>0</td>\n",
              "      <td>And we consider that word pairs that have a sm...</td>\n",
              "      <td>And consider word pairs small distance vectors...</td>\n",
              "      <td>And consider word pairs small distance vectors...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>998</td>\n",
              "      <td>P02-1016</td>\n",
              "      <td>J92-4003</td>\n",
              "      <td>0</td>\n",
              "      <td>Words are encoded through an automatic cluster...</td>\n",
              "      <td>Words encoded automatic clustering algorithm (...</td>\n",
              "      <td>Words encoded automatic clustering algorithm B...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>999 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19335d9d-7a6e-4164-8d03-918c80ac5d18')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-19335d9d-7a6e-4164-8d03-918c80ac5d18 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-19335d9d-7a6e-4164-8d03-918c80ac5d18');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nnYYCly6BWQA",
        "outputId": "76e7052c-3cf3-4980-81f3-c1c032ee270a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      no     paper cited_paper  label  \\\n",
              "0      0  A00-1043    A00-2024      0   \n",
              "1      1  H05-1033    A00-2024      0   \n",
              "2      2  I05-2009    A00-2024      0   \n",
              "3      3  I05-2009    A00-2024      0   \n",
              "4      4  I05-2009    A00-2024      0   \n",
              "..   ...       ...         ...    ...   \n",
              "994  994  N09-1053    J92-4003      0   \n",
              "995  995  P01-1046    J92-4003      0   \n",
              "996  996  P01-1046    J92-4003      0   \n",
              "997  997  P01-1068    J92-4003      0   \n",
              "998  998  P02-1016    J92-4003      0   \n",
              "\n",
              "                                                  text  \\\n",
              "0    We analyzed a set of articles and identified s...   \n",
              "1    Table 3: Example compressions Compression AvgL...   \n",
              "2    5.3 Related works and discussion Our two-step ...   \n",
              "3    (1999) proposed a summarization system based o...   \n",
              "4    We found that the deletion of lead parts did n...   \n",
              "..                                                 ...   \n",
              "994  While we can only compare class models with wo...   \n",
              "995  (1999) and Lee (1999)) can be generally divide...   \n",
              "996  Classes can be induced directly from the corpu...   \n",
              "997  And we consider that word pairs that have a sm...   \n",
              "998  Words are encoded through an automatic cluster...   \n",
              "\n",
              "                              review_without_stopwords  \\\n",
              "0    We analyzed set articles identified six major ...   \n",
              "1    Table 3: Example compressions Compression AvgL...   \n",
              "2    5.3 Related works discussion Our two-step mode...   \n",
              "3    (1999) proposed summarization system based dra...   \n",
              "4    We found deletion lead parts not occur often s...   \n",
              "..                                                 ...   \n",
              "994  While can compare class models word models lar...   \n",
              "995  (1999) Lee (1999)) can generally divided three...   \n",
              "996  Classes can induced directly corpus (Pereira e...   \n",
              "997  And consider word pairs small distance vectors...   \n",
              "998  Words encoded automatic clustering algorithm (...   \n",
              "\n",
              "                                 w/o stopwords or tags  \n",
              "0    We analyzed set articles identified six major ...  \n",
              "1    Table  Example compressions Compression AvgLen...  \n",
              "2     Related works discussion Our twostep model es...  \n",
              "3     proposed summarization system based draft rev...  \n",
              "4    We found deletion lead parts not occur often s...  \n",
              "..                                                 ...  \n",
              "994  While can compare class models word models lar...  \n",
              "995   Lee  can generally divided three types discou...  \n",
              "996  Classes can induced directly corpus Pereira et...  \n",
              "997  And consider word pairs small distance vectors...  \n",
              "998  Words encoded automatic clustering algorithm B...  \n",
              "\n",
              "[999 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-84c50b7e-0151-4c8f-851f-65392a118af4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>no</th>\n",
              "      <th>paper</th>\n",
              "      <th>cited_paper</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>review_without_stopwords</th>\n",
              "      <th>w/o stopwords or tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A00-1043</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We analyzed a set of articles and identified s...</td>\n",
              "      <td>We analyzed set articles identified six major ...</td>\n",
              "      <td>We analyzed set articles identified six major ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>H05-1033</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>Table 3: Example compressions Compression AvgL...</td>\n",
              "      <td>Table 3: Example compressions Compression AvgL...</td>\n",
              "      <td>Table  Example compressions Compression AvgLen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>I05-2009</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>5.3 Related works and discussion Our two-step ...</td>\n",
              "      <td>5.3 Related works discussion Our two-step mode...</td>\n",
              "      <td>Related works discussion Our twostep model es...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>I05-2009</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>(1999) proposed a summarization system based o...</td>\n",
              "      <td>(1999) proposed summarization system based dra...</td>\n",
              "      <td>proposed summarization system based draft rev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>I05-2009</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We found that the deletion of lead parts did n...</td>\n",
              "      <td>We found deletion lead parts not occur often s...</td>\n",
              "      <td>We found deletion lead parts not occur often s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>994</td>\n",
              "      <td>N09-1053</td>\n",
              "      <td>J92-4003</td>\n",
              "      <td>0</td>\n",
              "      <td>While we can only compare class models with wo...</td>\n",
              "      <td>While can compare class models word models lar...</td>\n",
              "      <td>While can compare class models word models lar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>995</td>\n",
              "      <td>P01-1046</td>\n",
              "      <td>J92-4003</td>\n",
              "      <td>0</td>\n",
              "      <td>(1999) and Lee (1999)) can be generally divide...</td>\n",
              "      <td>(1999) Lee (1999)) can generally divided three...</td>\n",
              "      <td>Lee  can generally divided three types discou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>996</td>\n",
              "      <td>P01-1046</td>\n",
              "      <td>J92-4003</td>\n",
              "      <td>0</td>\n",
              "      <td>Classes can be induced directly from the corpu...</td>\n",
              "      <td>Classes can induced directly corpus (Pereira e...</td>\n",
              "      <td>Classes can induced directly corpus Pereira et...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>997</td>\n",
              "      <td>P01-1068</td>\n",
              "      <td>J92-4003</td>\n",
              "      <td>0</td>\n",
              "      <td>And we consider that word pairs that have a sm...</td>\n",
              "      <td>And consider word pairs small distance vectors...</td>\n",
              "      <td>And consider word pairs small distance vectors...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>998</td>\n",
              "      <td>P02-1016</td>\n",
              "      <td>J92-4003</td>\n",
              "      <td>0</td>\n",
              "      <td>Words are encoded through an automatic cluster...</td>\n",
              "      <td>Words encoded automatic clustering algorithm (...</td>\n",
              "      <td>Words encoded automatic clustering algorithm B...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>999 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84c50b7e-0151-4c8f-851f-65392a118af4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-84c50b7e-0151-4c8f-851f-65392a118af4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-84c50b7e-0151-4c8f-851f-65392a118af4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove data that is too large"
      ],
      "metadata": {
        "id": "aIIKwIH7ttGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get maxLen for the maximum length of a text \n",
        "maxLen = max(clean_dataset['w/o stopwords or tags'].apply(len))\n",
        "\n",
        "#the row of the maxLen text\n",
        "clean_dataset.loc[clean_dataset['w/o stopwords or tags'].apply(len) == max(clean_dataset['w/o stopwords or tags'].apply(len))]"
      ],
      "metadata": {
        "id": "pTcUebCepIyB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "f428dfd4-fc0b-4fc9-c736-0b122b87f706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      no     paper cited_paper  label  \\\n",
              "225  225  W99-0505    C94-2113      0   \n",
              "\n",
              "                                                  text  \\\n",
              "225  Towards a Meaning-Full Comparison of Lexieal R...   \n",
              "\n",
              "                              review_without_stopwords  \\\n",
              "225  Towards Meaning-Full Comparison Lexieal Resour...   \n",
              "\n",
              "                                 w/o stopwords or tags  \n",
              "225  Towards MeaningFull Comparison Lexieal Resourc...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b937ea22-55d2-4f6c-ae5d-bac0ff608690\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>no</th>\n",
              "      <th>paper</th>\n",
              "      <th>cited_paper</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>review_without_stopwords</th>\n",
              "      <th>w/o stopwords or tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>225</td>\n",
              "      <td>W99-0505</td>\n",
              "      <td>C94-2113</td>\n",
              "      <td>0</td>\n",
              "      <td>Towards a Meaning-Full Comparison of Lexieal R...</td>\n",
              "      <td>Towards Meaning-Full Comparison Lexieal Resour...</td>\n",
              "      <td>Towards MeaningFull Comparison Lexieal Resourc...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b937ea22-55d2-4f6c-ae5d-bac0ff608690')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b937ea22-55d2-4f6c-ae5d-bac0ff608690 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b937ea22-55d2-4f6c-ae5d-bac0ff608690');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sizes of texts \n",
        "clean_dataset['w/o stopwords or tags'].apply(len).sort_values()"
      ],
      "metadata": {
        "id": "ZvZBe8J2uu0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "987af3ed-b939-468e-b85b-501af8531bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "144        9\n",
              "127       11\n",
              "393       12\n",
              "704       13\n",
              "642       13\n",
              "       ...  \n",
              "637      781\n",
              "321      835\n",
              "908     1717\n",
              "909     3000\n",
              "225    22682\n",
              "Name: w/o stopwords or tags, Length: 999, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#try removing thousand word lines from dataset to make training easier\n",
        "smaller_clean_dataset = clean_dataset.loc[clean_dataset['w/o stopwords or tags'].apply(len) < 1000]\n",
        "smaller_clean_dataset['w/o stopwords or tags'].apply(len).sort_values()"
      ],
      "metadata": {
        "id": "IuizTg6gjNf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c19d90e-4a0b-43f0-c684-524994181620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "144      9\n",
              "127     11\n",
              "393     12\n",
              "704     13\n",
              "642     13\n",
              "      ... \n",
              "896    596\n",
              "907    730\n",
              "561    749\n",
              "637    781\n",
              "321    835\n",
              "Name: w/o stopwords or tags, Length: 996, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#redefine maxlen as length of new longest sentence\n",
        "maxLen = max(smaller_clean_dataset['w/o stopwords or tags'].apply(len))"
      ],
      "metadata": {
        "id": "zTYu-8LfuuN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One hot encode labels"
      ],
      "metadata": {
        "id": "_5_VFBfdAGtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#copy original dataset as backup if mess up\n",
        "smaller_clean_dataset_orig = smaller_clean_dataset.copy()\n",
        "\n",
        "#add categorical labels for label to original dataset \n",
        "smaller_clean_dataset = pd.concat([smaller_clean_dataset, pd.get_dummies(smaller_clean_dataset['label'], prefix='label_')], axis=1)\n",
        "smaller_clean_dataset.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "qAuJ9ErWC0U7",
        "outputId": "1dab0ea7-4812-4a05-81bc-90094b51113c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   no     paper cited_paper  label  \\\n",
              "0   0  A00-1043    A00-2024      0   \n",
              "1   1  H05-1033    A00-2024      0   \n",
              "2   2  I05-2009    A00-2024      0   \n",
              "3   3  I05-2009    A00-2024      0   \n",
              "4   4  I05-2009    A00-2024      0   \n",
              "\n",
              "                                                text  \\\n",
              "0  We analyzed a set of articles and identified s...   \n",
              "1  Table 3: Example compressions Compression AvgL...   \n",
              "2  5.3 Related works and discussion Our two-step ...   \n",
              "3  (1999) proposed a summarization system based o...   \n",
              "4  We found that the deletion of lead parts did n...   \n",
              "\n",
              "                            review_without_stopwords  \\\n",
              "0  We analyzed set articles identified six major ...   \n",
              "1  Table 3: Example compressions Compression AvgL...   \n",
              "2  5.3 Related works discussion Our two-step mode...   \n",
              "3  (1999) proposed summarization system based dra...   \n",
              "4  We found deletion lead parts not occur often s...   \n",
              "\n",
              "                               w/o stopwords or tags  label__-1  label__0  \\\n",
              "0  We analyzed set articles identified six major ...          0         1   \n",
              "1  Table  Example compressions Compression AvgLen...          0         1   \n",
              "2   Related works discussion Our twostep model es...          0         1   \n",
              "3   proposed summarization system based draft rev...          0         1   \n",
              "4  We found deletion lead parts not occur often s...          0         1   \n",
              "\n",
              "   label__1  \n",
              "0         0  \n",
              "1         0  \n",
              "2         0  \n",
              "3         0  \n",
              "4         0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8e4ee97-351e-4eee-b9f2-5e8ea02a77ec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>no</th>\n",
              "      <th>paper</th>\n",
              "      <th>cited_paper</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>review_without_stopwords</th>\n",
              "      <th>w/o stopwords or tags</th>\n",
              "      <th>label__-1</th>\n",
              "      <th>label__0</th>\n",
              "      <th>label__1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A00-1043</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We analyzed a set of articles and identified s...</td>\n",
              "      <td>We analyzed set articles identified six major ...</td>\n",
              "      <td>We analyzed set articles identified six major ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>H05-1033</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>Table 3: Example compressions Compression AvgL...</td>\n",
              "      <td>Table 3: Example compressions Compression AvgL...</td>\n",
              "      <td>Table  Example compressions Compression AvgLen...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>I05-2009</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>5.3 Related works and discussion Our two-step ...</td>\n",
              "      <td>5.3 Related works discussion Our two-step mode...</td>\n",
              "      <td>Related works discussion Our twostep model es...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>I05-2009</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>(1999) proposed a summarization system based o...</td>\n",
              "      <td>(1999) proposed summarization system based dra...</td>\n",
              "      <td>proposed summarization system based draft rev...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>I05-2009</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We found that the deletion of lead parts did n...</td>\n",
              "      <td>We found deletion lead parts not occur often s...</td>\n",
              "      <td>We found deletion lead parts not occur often s...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8e4ee97-351e-4eee-b9f2-5e8ea02a77ec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c8e4ee97-351e-4eee-b9f2-5e8ea02a77ec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c8e4ee97-351e-4eee-b9f2-5e8ea02a77ec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smaller_clean_dataset.iloc[:, -3:].to_numpy().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiQVK-f3HKOm",
        "outputId": "ad8c8b18-1559-4be9-ad10-a7320a9ce736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(996, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Even out dataset by label\n",
        "> Scale up the non-neutral sentiment data\n"
      ],
      "metadata": {
        "id": "zNFtaGe_OcBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#scale up the non-neutral sentiment data\n",
        "smaller_clean_dataset['label'].value_counts()\n",
        "\n",
        "# add all the 1 sentiment data in 11x + first 27 \n",
        "# add all the 1 sentiment data in 42x + first 14"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z5-VQ8VPGvC",
        "outputId": "d1deb6c9-7189-43a8-c60b-ad55b01588ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 0    896\n",
              " 1     79\n",
              "-1     21\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# POSITIVE sentiment data -----\n",
        "#repeat positive sentiment data 10 times\n",
        "vertical_concat = smaller_clean_dataset.loc[smaller_clean_dataset['label'] == 1]\n",
        "for i in range(9):\n",
        "  vertical_concat = pd.concat([vertical_concat, smaller_clean_dataset.loc[smaller_clean_dataset['label'] == 1]], axis=0)\n",
        "\n",
        "#add first 27 entries to that dataset\n",
        "vertical_concat = pd.concat([vertical_concat, vertical_concat.iloc[:27]], axis=0)\n",
        "\n",
        "\n",
        "\n",
        "# NEGATIVE sentiment data -----\n",
        "#repeat positive sentiment data 40 times\n",
        "vertical_concat2 = smaller_clean_dataset.loc[smaller_clean_dataset['label'] == -1]\n",
        "for i in range(40):\n",
        "  vertical_concat2 = pd.concat([vertical_concat2, smaller_clean_dataset.loc[smaller_clean_dataset['label'] == -1]], axis=0)\n",
        "\n",
        "#add first 27 entries to that dataset\n",
        "vertical_concat2 = pd.concat([vertical_concat2, vertical_concat2.iloc[:14]], axis=0)\n",
        "\n",
        "\n",
        "\n",
        "print('new 1 label shape', vertical_concat.shape)\n",
        "print('new -1 label shape', vertical_concat2.shape)\n",
        "vertical_concat.head(3)\n",
        "# vertical_concat2.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "A4ME5mxiSXse",
        "outputId": "1abd6241-27ba-4f9d-a839-f902b3b7adf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new 1 label shape (817, 10)\n",
            "new -1 label shape (875, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    no     paper cited_paper  label  \\\n",
              "6    6  I08-2101    A00-2024      1   \n",
              "9    9  J02-4005    A00-2024      1   \n",
              "21  21  W03-1102    A00-2024      1   \n",
              "\n",
              "                                                 text  \\\n",
              "6   al., 1994), compression of sentences with Auto...   \n",
              "9   But in fact, the issue of editing in text summ...   \n",
              "21  The recent approach for editing extracted text...   \n",
              "\n",
              "                             review_without_stopwords  \\\n",
              "6   al., 1994), compression sentences Automatic Tr...   \n",
              "9   But fact, issue editing text summarization usu...   \n",
              "21  The recent approach editing extracted text spa...   \n",
              "\n",
              "                                w/o stopwords or tags  label__-1  label__0  \\\n",
              "6   al  compression sentences Automatic Translatio...          0         0   \n",
              "9   But fact issue editing text summarization usua...          0         0   \n",
              "21  The recent approach editing extracted text spa...          0         0   \n",
              "\n",
              "    label__1  \n",
              "6          1  \n",
              "9          1  \n",
              "21         1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c763a66-c148-4526-b74d-f4562150d0ff\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>no</th>\n",
              "      <th>paper</th>\n",
              "      <th>cited_paper</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>review_without_stopwords</th>\n",
              "      <th>w/o stopwords or tags</th>\n",
              "      <th>label__-1</th>\n",
              "      <th>label__0</th>\n",
              "      <th>label__1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>I08-2101</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>1</td>\n",
              "      <td>al., 1994), compression of sentences with Auto...</td>\n",
              "      <td>al., 1994), compression sentences Automatic Tr...</td>\n",
              "      <td>al  compression sentences Automatic Translatio...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>J02-4005</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>1</td>\n",
              "      <td>But in fact, the issue of editing in text summ...</td>\n",
              "      <td>But fact, issue editing text summarization usu...</td>\n",
              "      <td>But fact issue editing text summarization usua...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>W03-1102</td>\n",
              "      <td>A00-2024</td>\n",
              "      <td>1</td>\n",
              "      <td>The recent approach for editing extracted text...</td>\n",
              "      <td>The recent approach editing extracted text spa...</td>\n",
              "      <td>The recent approach editing extracted text spa...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c763a66-c148-4526-b74d-f4562150d0ff')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3c763a66-c148-4526-b74d-f4562150d0ff button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3c763a66-c148-4526-b74d-f4562150d0ff');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save old dataframe \n",
        "old_dataframe = smaller_clean_dataset.copy()\n",
        "\n",
        "#concatenate the dataframes\n",
        "non_neutral_data = pd.concat([vertical_concat, vertical_concat2], axis=0)\n",
        "smaller_clean_dataset = pd.concat([smaller_clean_dataset, non_neutral_data], axis=0)\n",
        "\n",
        "#shuffle data\n",
        "from sklearn.utils import shuffle\n",
        "smaller_clean_dataset = shuffle(smaller_clean_dataset, random_state=0)\n",
        "\n",
        "smaller_clean_dataset['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb4XR5LOWB0z",
        "outputId": "b5c60cf4-9de9-4cda-e144-5212ef88cf91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1    896\n",
              " 0    896\n",
              " 1    896\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Test Model"
      ],
      "metadata": {
        "id": "w5ECIHgicxSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing with multiple outputs"
      ],
      "metadata": {
        "id": "j0K4yBPBILNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split into test and training data\n",
        "X_train, X_test,Y_train, Y_test = train_test_split(smaller_clean_dataset['w/o stopwords or tags'], smaller_clean_dataset.iloc[:, -3:].to_numpy(), test_size=0.2, random_state = 45)"
      ],
      "metadata": {
        "id": "6kzXRcOQILNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Y_train 3D vector\n",
        "#Y_train\n",
        "\n",
        "#Y_train sentiment\n",
        "(np.argmax(Y_train, axis=1)[:300] - 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN3A1XB3eO68",
        "outputId": "c7379488-e466-459e-df08-709d1e882363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1,  1, -1,  1,  1, -1,  1,  0,  0,  0,  1,  0, -1,  1,  0,  0,  1,\n",
              "        1,  0,  0,  1,  0, -1,  1,  0, -1,  0,  1,  0,  1,  1, -1,  0,  1,\n",
              "        1, -1, -1,  0, -1,  0,  1,  1,  0, -1,  0,  1,  0,  1,  1,  1, -1,\n",
              "       -1, -1,  0, -1,  0,  1,  1,  0,  1, -1,  1, -1,  0,  1,  0, -1,  1,\n",
              "        0,  1,  1, -1, -1,  0,  1, -1, -1, -1, -1, -1,  0, -1,  0,  0, -1,\n",
              "       -1, -1,  0,  1,  1,  1,  1, -1, -1, -1, -1,  0,  1,  0, -1,  1, -1,\n",
              "        0, -1, -1,  0,  1, -1,  0, -1,  0,  0,  0,  1,  0, -1, -1,  1, -1,\n",
              "       -1,  0, -1,  0,  0,  1, -1, -1, -1, -1,  0, -1,  1,  0,  1,  1, -1,\n",
              "       -1,  1, -1,  1, -1, -1,  1, -1,  1,  1, -1,  1, -1, -1,  0, -1,  1,\n",
              "       -1,  0,  1,  1,  1,  1,  0,  0, -1, -1,  1,  1,  0,  0,  0,  1,  0,\n",
              "        0,  0,  0,  1,  1,  1, -1,  0,  1, -1, -1, -1,  1,  1,  0,  1, -1,\n",
              "        1,  1,  1,  0,  0,  0,  0, -1,  1, -1,  1, -1,  1,  1,  1,  1,  0,\n",
              "        0,  1,  1,  1,  1, -1, -1, -1,  0,  0,  1, -1, -1,  0,  1, -1,  0,\n",
              "        1, -1,  1, -1,  0,  0,  0, -1, -1,  1,  1,  0, -1,  1,  1, -1,  1,\n",
              "        1,  0,  0, -1,  0,  1, -1,  0,  0, -1,  1, -1,  0, -1,  1, -1, -1,\n",
              "       -1,  0, -1,  0, -1,  0, -1, -1,  1,  0,  0,  1,  0, -1, -1, -1,  1,\n",
              "       -1,  1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1,  0,  1,  0,  0,\n",
              "        0,  1, -1,  0, -1,  1, -1, -1, -1,  1,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create list of unique words in sentences\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "#creates dictionary of each {word: index}\n",
        "words_to_index = tokenizer.word_index\n",
        "words_to_index"
      ],
      "metadata": {
        "id": "CkCfFj2pILNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb2fbb7d-81d6-45be-b62d-9a95b374ddc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'al': 1,\n",
              " 'et': 2,\n",
              " 'church': 3,\n",
              " 'used': 4,\n",
              " 'information': 5,\n",
              " 'method': 6,\n",
              " 'not': 7,\n",
              " 'models': 8,\n",
              " 'hanks': 9,\n",
              " 'model': 10,\n",
              " 'tagging': 11,\n",
              " 'can': 12,\n",
              " 'word': 13,\n",
              " 'performance': 14,\n",
              " 'approach': 15,\n",
              " 'the': 16,\n",
              " 'cutting': 17,\n",
              " 'translation': 18,\n",
              " 'data': 19,\n",
              " 'words': 20,\n",
              " 'a': 21,\n",
              " 'based': 22,\n",
              " 'mutual': 23,\n",
              " 'wsd': 24,\n",
              " 'similarity': 25,\n",
              " 'smith': 26,\n",
              " 'language': 27,\n",
              " 'approaches': 28,\n",
              " 'this': 29,\n",
              " 'semantic': 30,\n",
              " 'lin': 31,\n",
              " 'results': 32,\n",
              " 'brown': 33,\n",
              " 'proposed': 34,\n",
              " 'similar': 35,\n",
              " 'using': 36,\n",
              " 'shown': 37,\n",
              " 'statistical': 38,\n",
              " 'dependency': 39,\n",
              " 'methods': 40,\n",
              " 'we': 41,\n",
              " 'also': 42,\n",
              " 'measure': 43,\n",
              " 'collocations': 44,\n",
              " 'wu': 45,\n",
              " 'pos': 46,\n",
              " 'markov': 47,\n",
              " 'turney': 48,\n",
              " 'for': 49,\n",
              " 'work': 50,\n",
              " 'in': 51,\n",
              " 'machine': 52,\n",
              " 'mi': 53,\n",
              " 'including': 54,\n",
              " 'one': 55,\n",
              " 'unsupervised': 56,\n",
              " 'two': 57,\n",
              " 'reported': 58,\n",
              " 'algorithm': 59,\n",
              " 'johnson': 60,\n",
              " 'large': 61,\n",
              " 'brants': 62,\n",
              " 'carpuat': 63,\n",
              " 'wikipedia': 64,\n",
              " 'kazama': 65,\n",
              " 'torisawa': 66,\n",
              " 'training': 67,\n",
              " 'applications': 68,\n",
              " 'available': 69,\n",
              " 'hidden': 70,\n",
              " 'collins': 71,\n",
              " 'eg': 72,\n",
              " 'thus': 73,\n",
              " 'well': 74,\n",
              " 'since': 75,\n",
              " 'use': 76,\n",
              " 'lexical': 77,\n",
              " 'mckeown': 78,\n",
              " 'jing': 79,\n",
              " 'systems': 80,\n",
              " 'corpora': 81,\n",
              " 'sentence': 82,\n",
              " 'developed': 83,\n",
              " 'patterns': 84,\n",
              " 'gazetteer': 85,\n",
              " 'svore': 86,\n",
              " 'distributional': 87,\n",
              " 'rulebased': 88,\n",
              " 'no': 89,\n",
              " 'syntactic': 90,\n",
              " 'many': 91,\n",
              " 'english': 92,\n",
              " 'system': 93,\n",
              " 'outperform': 94,\n",
              " 'lins': 95,\n",
              " 'text': 96,\n",
              " 'features': 97,\n",
              " 'modeling': 98,\n",
              " 'pointwise': 99,\n",
              " 'unlike': 100,\n",
              " 'b': 101,\n",
              " 'difficult': 102,\n",
              " 'hmm': 103,\n",
              " 'dagan': 104,\n",
              " 'taggers': 105,\n",
              " 'resources': 106,\n",
              " 'version': 107,\n",
              " 'partofspeech': 108,\n",
              " 'pairclass': 109,\n",
              " 'better': 110,\n",
              " 'processing': 111,\n",
              " 'baseline': 112,\n",
              " 'improve': 113,\n",
              " 'although': 114,\n",
              " 'networks': 115,\n",
              " 'problem': 116,\n",
              " 'score': 117,\n",
              " 'found': 118,\n",
              " 'corpus': 119,\n",
              " 'number': 120,\n",
              " 'chan': 121,\n",
              " 'disambiguation': 122,\n",
              " 'good': 123,\n",
              " 'several': 124,\n",
              " 'mihalcea': 125,\n",
              " 'jesscm': 126,\n",
              " 'relations': 127,\n",
              " 'uses': 128,\n",
              " 'widely': 129,\n",
              " 'applied': 130,\n",
              " 'sense': 131,\n",
              " 'however': 132,\n",
              " 'wordnet': 133,\n",
              " 'smt': 134,\n",
              " 'identify': 135,\n",
              " 'automatically': 136,\n",
              " 'brill': 137,\n",
              " 'techniques': 138,\n",
              " 'our': 139,\n",
              " 'learning': 140,\n",
              " 'classes': 141,\n",
              " 'koehn': 142,\n",
              " 'zhang': 143,\n",
              " 'recently': 144,\n",
              " 'class': 145,\n",
              " 'due': 146,\n",
              " 'hindle': 147,\n",
              " 'pairs': 148,\n",
              " 'different': 149,\n",
              " 'lee': 150,\n",
              " 'ramage': 151,\n",
              " 'trained': 152,\n",
              " 'mcdonald': 153,\n",
              " 'related': 154,\n",
              " 'quality': 155,\n",
              " 'recent': 156,\n",
              " 'jelinek': 157,\n",
              " 'hughes': 158,\n",
              " 'important': 159,\n",
              " 'much': 160,\n",
              " 'success': 161,\n",
              " 'introduction': 162,\n",
              " 'etc': 163,\n",
              " 'lexicon': 164,\n",
              " 'rapp': 165,\n",
              " 'provides': 166,\n",
              " 'past': 167,\n",
              " 'byrne': 168,\n",
              " 'daelemans': 169,\n",
              " 'weeds': 170,\n",
              " 'possible': 171,\n",
              " 'smadja': 172,\n",
              " 'measures': 173,\n",
              " 'achieved': 174,\n",
              " 'higherorder': 175,\n",
              " 'may': 176,\n",
              " 'ner': 177,\n",
              " 'equivalence': 178,\n",
              " 'notoriously': 179,\n",
              " 'cooccurrence': 180,\n",
              " 'schutze': 181,\n",
              " 'like': 182,\n",
              " 'task': 183,\n",
              " 'expected': 184,\n",
              " 'distributions': 185,\n",
              " 'structure': 186,\n",
              " 'nivre': 187,\n",
              " 'weischedel': 188,\n",
              " 'tasks': 189,\n",
              " 'wellknown': 190,\n",
              " 'context': 191,\n",
              " 'terms': 192,\n",
              " 'set': 193,\n",
              " 'result': 194,\n",
              " 'first': 195,\n",
              " 'annotated': 196,\n",
              " 's': 197,\n",
              " 'tags': 198,\n",
              " 'successfully': 199,\n",
              " 'best': 200,\n",
              " 'biemann': 201,\n",
              " 'marcu': 202,\n",
              " 'without': 203,\n",
              " 'hoang': 204,\n",
              " 'require': 205,\n",
              " 'test': 206,\n",
              " 'edge': 207,\n",
              " 'phrases': 208,\n",
              " 'note': 209,\n",
              " 'feature': 210,\n",
              " 'introduced': 211,\n",
              " 'generated': 212,\n",
              " 'derose': 213,\n",
              " 'clusters': 214,\n",
              " 'comparison': 215,\n",
              " 'alignment': 216,\n",
              " 'morphological': 217,\n",
              " 'toutanova': 218,\n",
              " 'memorybased': 219,\n",
              " 'promise': 220,\n",
              " 'as': 221,\n",
              " 'abney': 222,\n",
              " 'f': 223,\n",
              " 'extracted': 224,\n",
              " 'combination': 225,\n",
              " 'ambiguity': 226,\n",
              " 'among': 227,\n",
              " 'requiring': 228,\n",
              " 'representations': 229,\n",
              " 'property': 230,\n",
              " 'events': 231,\n",
              " 'general': 232,\n",
              " 'interesting': 233,\n",
              " 'association': 234,\n",
              " 'natural': 235,\n",
              " 'weights': 236,\n",
              " 'even': 237,\n",
              " 'representation': 238,\n",
              " 'outperformed': 239,\n",
              " 'thirdparty': 240,\n",
              " 'potential': 241,\n",
              " 'obtaining': 242,\n",
              " 'weir': 243,\n",
              " 'web': 244,\n",
              " 'pedersen': 245,\n",
              " 'straightforward': 246,\n",
              " 'estimation': 247,\n",
              " 'pereira': 248,\n",
              " 'various': 249,\n",
              " 'observed': 250,\n",
              " 'caveat': 251,\n",
              " 'reliance': 252,\n",
              " 'studies': 253,\n",
              " 'want': 254,\n",
              " 'outperforms': 255,\n",
              " 'discover': 256,\n",
              " 'contrast': 257,\n",
              " 'koo': 258,\n",
              " 'papers': 259,\n",
              " 'looked': 260,\n",
              " 'simpler': 261,\n",
              " 'procedure': 262,\n",
              " 'respectively': 263,\n",
              " 'existing': 264,\n",
              " 'smaller': 265,\n",
              " 'automatic': 266,\n",
              " 'suzuki': 267,\n",
              " 'amount': 268,\n",
              " 'wordnetbased': 269,\n",
              " 'lexicography': 270,\n",
              " 'detailed': 271,\n",
              " 'chains': 272,\n",
              " 'smoothing': 273,\n",
              " 'phrase': 274,\n",
              " 'others': 275,\n",
              " 'japanese': 276,\n",
              " 'stateoftheart': 277,\n",
              " 'interest': 278,\n",
              " 'publicly': 279,\n",
              " 'examined': 280,\n",
              " 'synpara': 281,\n",
              " 'formalized': 282,\n",
              " 'lund': 283,\n",
              " 'burgess': 284,\n",
              " 'edmonds': 285,\n",
              " 'lemaire': 286,\n",
              " 'denhiere': 287,\n",
              " 'senses': 288,\n",
              " 'increases': 289,\n",
              " 'preferred': 290,\n",
              " 'richer': 291,\n",
              " 'largest': 292,\n",
              " 'though': 293,\n",
              " 'statistically': 294,\n",
              " 'frequently': 295,\n",
              " 'slightly': 296,\n",
              " 'summarization': 297,\n",
              " 'marginal': 298,\n",
              " 'either': 299,\n",
              " 'supervised': 300,\n",
              " 'optimal': 301,\n",
              " 'approximately': 302,\n",
              " 'embeds': 303,\n",
              " 'edges': 304,\n",
              " 'cross': 305,\n",
              " 'offering': 306,\n",
              " 'entries': 307,\n",
              " 'see': 308,\n",
              " 'despite': 309,\n",
              " 'kumar': 310,\n",
              " 'unfortunately': 311,\n",
              " 'generalization': 312,\n",
              " 'frequency': 313,\n",
              " 'all': 314,\n",
              " 'problems': 315,\n",
              " 'parameter': 316,\n",
              " 'dropped': 317,\n",
              " 'freely': 318,\n",
              " 'following': 319,\n",
              " 'tested': 320,\n",
              " 'significance': 321,\n",
              " 'rare': 322,\n",
              " 'modest': 323,\n",
              " 'always': 324,\n",
              " 'understood': 325,\n",
              " 'conll': 326,\n",
              " 'showing': 327,\n",
              " 'modify': 328,\n",
              " 'manually': 329,\n",
              " 'unable': 330,\n",
              " 'carry': 331,\n",
              " 'mt': 332,\n",
              " 'accuracy': 333,\n",
              " 'length': 334,\n",
              " 'function': 335,\n",
              " 'hybrid': 336,\n",
              " 'research': 337,\n",
              " 'rules': 338,\n",
              " 'monotonic': 339,\n",
              " 'collocational': 340,\n",
              " 'brills': 341,\n",
              " 'demonstrate': 342,\n",
              " 'unacceptable': 343,\n",
              " 'lowcount': 344,\n",
              " 'beyond': 345,\n",
              " 'maximumentropy': 346,\n",
              " 'rathnaparki': 347,\n",
              " 'cyclic': 348,\n",
              " 'listing': 349,\n",
              " 'surprisingly': 350,\n",
              " 'example': 351,\n",
              " 'potentially': 352,\n",
              " 'status': 353,\n",
              " 'overemphasising': 354,\n",
              " 'it': 355,\n",
              " 'configurations': 356,\n",
              " 'purely': 357,\n",
              " 'inspect': 358,\n",
              " 'recognition': 359,\n",
              " 'together': 360,\n",
              " 'limitation': 361,\n",
              " 'points': 362,\n",
              " 'higher': 363,\n",
              " 'ngram': 364,\n",
              " 'simple': 365,\n",
              " 'sentences': 366,\n",
              " 't': 367,\n",
              " 'minnen': 368,\n",
              " 'sophisticated': 369,\n",
              " 'twoword': 370,\n",
              " 'arbitrary': 371,\n",
              " 'fscores': 372,\n",
              " 'researchers': 373,\n",
              " 'goes': 374,\n",
              " 'deals': 375,\n",
              " 'pantel': 376,\n",
              " 'on': 377,\n",
              " 'source': 378,\n",
              " 'phrasebased': 379,\n",
              " 'distribution': 380,\n",
              " 'poesio': 381,\n",
              " 'statistics': 382,\n",
              " 'external': 383,\n",
              " 'there': 384,\n",
              " 'lapata': 385,\n",
              " 'weight': 386,\n",
              " 'high': 387,\n",
              " 'decoding': 388,\n",
              " 'parsing': 389,\n",
              " 'previous': 390,\n",
              " 'tree': 391,\n",
              " 'space': 392,\n",
              " 'joint': 393,\n",
              " 'probabilities': 394,\n",
              " 'cucerzan': 395,\n",
              " 'retrieval': 396,\n",
              " 'search': 397,\n",
              " 'given': 398,\n",
              " 'm': 399,\n",
              " 'p': 400,\n",
              " 'zettlemoyer': 401,\n",
              " 'time': 402,\n",
              " 'explored': 403,\n",
              " 'years': 404,\n",
              " 'described': 405,\n",
              " 'liu': 406,\n",
              " 'gram': 407,\n",
              " 'whether': 408,\n",
              " 'xerox': 409,\n",
              " 'xy': 410,\n",
              " 'efficient': 411,\n",
              " 'selection': 412,\n",
              " 'additional': 413,\n",
              " 'acquisition': 414,\n",
              " 'useful': 415,\n",
              " 'table': 416,\n",
              " 'lead': 417,\n",
              " 'clustering': 418,\n",
              " 'knowledge': 419,\n",
              " 'functions': 420,\n",
              " 'hovy': 421,\n",
              " 'pair': 422,\n",
              " 'effective': 423,\n",
              " 'spaces': 424,\n",
              " 'yarowsky': 425,\n",
              " 'resnik': 426,\n",
              " 'and': 427,\n",
              " 'dolan': 428,\n",
              " 'noun': 429,\n",
              " 'charniak': 430,\n",
              " 'huang': 431,\n",
              " 'almuhareb': 432,\n",
              " 'speech': 433,\n",
              " 'include': 434,\n",
              " 'contextual': 435,\n",
              " 'ability': 436,\n",
              " 'informationtheoretic': 437,\n",
              " 'reordering': 438,\n",
              " 'advances': 439,\n",
              " 'improvement': 440,\n",
              " 'integrating': 441,\n",
              " 'bleu': 442,\n",
              " 'improvements': 443,\n",
              " 'parse': 444,\n",
              " 'component': 445,\n",
              " 'ways': 446,\n",
              " 'entropy': 447,\n",
              " 'starting': 448,\n",
              " 'metric': 449,\n",
              " 'editing': 450,\n",
              " 'factored': 451,\n",
              " 'process': 452,\n",
              " 'sets': 453,\n",
              " 'unlabeled': 454,\n",
              " 'show': 455,\n",
              " 'luk': 456,\n",
              " 'parsers': 457,\n",
              " 'analysis': 458,\n",
              " 'improved': 459,\n",
              " 'five': 460,\n",
              " 'maximum': 461,\n",
              " 'perform': 462,\n",
              " 'y': 463,\n",
              " 'classbased': 464,\n",
              " 'constructed': 465,\n",
              " 'mccarthy': 466,\n",
              " 'pmi': 467,\n",
              " 'negative': 468,\n",
              " 'kurland': 469,\n",
              " 'backoff': 470,\n",
              " 'instance': 471,\n",
              " 'category': 472,\n",
              " 'ravichandran': 473,\n",
              " 'usually': 474,\n",
              " 'values': 475,\n",
              " 'nlp': 476,\n",
              " 'powerful': 477,\n",
              " 'readily': 478,\n",
              " 'tagger': 479,\n",
              " 'tag': 480,\n",
              " 'commonly': 481,\n",
              " 'demonstrated': 482,\n",
              " 'particular': 483,\n",
              " 'makes': 484,\n",
              " 'variety': 485,\n",
              " 'd': 486,\n",
              " 'distributed': 487,\n",
              " 'framework': 488,\n",
              " 'eisner': 489,\n",
              " 'volk': 490,\n",
              " 'x': 491,\n",
              " 'wang': 492,\n",
              " 'size': 493,\n",
              " 'compute': 494,\n",
              " 'experiments': 495,\n",
              " 'goldwater': 496,\n",
              " 'will': 497,\n",
              " 'helps': 498,\n",
              " 'curran': 499,\n",
              " 'goel': 500,\n",
              " 'fraser': 501,\n",
              " 'describe': 502,\n",
              " 'ratio': 503,\n",
              " 'promising': 504,\n",
              " 'compared': 505,\n",
              " 'similaritybased': 506,\n",
              " 'tishby': 507,\n",
              " 'intuitively': 508,\n",
              " 'appealing': 509,\n",
              " 'per': 510,\n",
              " 'might': 511,\n",
              " 'dependencies': 512,\n",
              " 'dictionaries': 513,\n",
              " 'simplicity': 514,\n",
              " 'thesaurus': 515,\n",
              " 'log': 516,\n",
              " 'lopez': 517,\n",
              " 'literature': 518,\n",
              " 'known': 519,\n",
              " 'entity': 520,\n",
              " 'grammars': 521,\n",
              " 'algorithms': 522,\n",
              " 'chen': 523,\n",
              " 'evans': 524,\n",
              " 'knight': 525,\n",
              " 'cooccurrences': 526,\n",
              " 'name': 527,\n",
              " 'accurate': 528,\n",
              " 'states': 529,\n",
              " 'griffiths': 530,\n",
              " 'associations': 531,\n",
              " 'considerable': 532,\n",
              " 'pattern': 533,\n",
              " 'em': 534,\n",
              " 'according': 535,\n",
              " 'wflog': 536,\n",
              " 'countwf': 537,\n",
              " 'countf': 538,\n",
              " 'relatedness': 539,\n",
              " 'compare': 540,\n",
              " 'articles': 541,\n",
              " 'scale': 542,\n",
              " 'output': 543,\n",
              " 'hmms': 544,\n",
              " 'erkan': 545,\n",
              " 'radev': 546,\n",
              " 'linguistic': 547,\n",
              " 'previously': 548,\n",
              " 'directly': 549,\n",
              " 'scheme': 550,\n",
              " 'semisupervised': 551,\n",
              " 'relation': 552,\n",
              " 'concepts': 553,\n",
              " 'section': 554,\n",
              " 'showed': 555,\n",
              " 'named': 556,\n",
              " 'contexts': 557,\n",
              " 'resolution': 558,\n",
              " 'k': 559,\n",
              " 'typically': 560,\n",
              " 'classification': 561,\n",
              " 'computed': 562,\n",
              " 'done': 563,\n",
              " 'reranking': 564,\n",
              " 'issue': 565,\n",
              " 'mani': 566,\n",
              " 'main': 567,\n",
              " 'reduction': 568,\n",
              " 'less': 569,\n",
              " 'types': 570,\n",
              " 'page': 571,\n",
              " 'tarau': 572,\n",
              " 'while': 573,\n",
              " 'c': 574,\n",
              " 'range': 575,\n",
              " 'prediction': 576,\n",
              " 'great': 577,\n",
              " 'grammar': 578,\n",
              " 'obtained': 579,\n",
              " 'target': 580,\n",
              " 'instead': 581,\n",
              " 'grouping': 582,\n",
              " 'to': 583,\n",
              " 'nouns': 584,\n",
              " 'w': 585,\n",
              " 'side': 586,\n",
              " 'claim': 587,\n",
              " 'extremely': 588,\n",
              " 'whereas': 589,\n",
              " 'give': 590,\n",
              " 'now': 591,\n",
              " 'stochastic': 592,\n",
              " 'scoring': 593,\n",
              " 'latent': 594,\n",
              " 'moses': 595,\n",
              " 'evaluation': 596,\n",
              " 'article': 597,\n",
              " 'extract': 598,\n",
              " 'compression': 599,\n",
              " 'gates': 600,\n",
              " 'bloedorn': 601,\n",
              " 'coefficientmanning': 602,\n",
              " 'schueutze': 603,\n",
              " 'independent': 604,\n",
              " 'j': 605,\n",
              " 'positive': 606,\n",
              " 'n': 607,\n",
              " 'verbs': 608,\n",
              " 'partial': 609,\n",
              " 'wide': 610,\n",
              " 'then': 611,\n",
              " 'state': 612,\n",
              " 'study': 613,\n",
              " 'galley': 614,\n",
              " 'definition': 615,\n",
              " 'calculate': 616,\n",
              " 'employing': 617,\n",
              " 'ruge': 618,\n",
              " 'goodman': 619,\n",
              " 'relevant': 620,\n",
              " 'employed': 621,\n",
              " 'took': 622,\n",
              " 'groups': 623,\n",
              " 'able': 624,\n",
              " 'second': 625,\n",
              " 'by': 626,\n",
              " 'paper': 627,\n",
              " 'made': 628,\n",
              " 'sequencebased': 629,\n",
              " 'matrix': 630,\n",
              " 'cases': 631,\n",
              " 'schmid': 632,\n",
              " 'hand': 633,\n",
              " 'markovitch': 634,\n",
              " 'label': 635,\n",
              " 'inventory': 636,\n",
              " 'furthermore': 637,\n",
              " 'probability': 638,\n",
              " 'moore': 639,\n",
              " 'build': 640,\n",
              " 'but': 641,\n",
              " 'fact': 642,\n",
              " 'works': 643,\n",
              " 'gazetteers': 644,\n",
              " 'written': 645,\n",
              " 'robust': 646,\n",
              " 'verbobject': 647,\n",
              " 'frequencies': 648,\n",
              " 'distance': 649,\n",
              " 'produce': 650,\n",
              " 'bayesian': 651,\n",
              " 'brin': 652,\n",
              " 'kleinberg': 653,\n",
              " 'purpose': 654,\n",
              " 'shows': 655,\n",
              " 'annotation': 656,\n",
              " 'annotators': 657,\n",
              " 'involve': 658,\n",
              " 'identification': 659,\n",
              " 'create': 660,\n",
              " 'competitive': 661,\n",
              " 'train': 662,\n",
              " 'henderson': 663,\n",
              " 'pp': 664,\n",
              " 'toral': 665,\n",
              " 'coefficient': 666,\n",
              " 'except': 667,\n",
              " 'verb': 668,\n",
              " 'topic': 669,\n",
              " 'surpass': 670,\n",
              " 'pad': 671,\n",
              " 'categorisation': 672,\n",
              " 'equally': 673,\n",
              " 'effect': 674,\n",
              " 'chelba': 675,\n",
              " 'forestbased': 676,\n",
              " 'comparisons': 677,\n",
              " 'van': 678,\n",
              " 'yield': 679,\n",
              " 'integration': 680,\n",
              " 'alternative': 681,\n",
              " 'erk': 682,\n",
              " 'estimated': 683,\n",
              " 'form': 684,\n",
              " 'report': 685,\n",
              " 'metrics': 686,\n",
              " 'nist': 687,\n",
              " 'meteor': 688,\n",
              " 'news': 689,\n",
              " 'novel': 690,\n",
              " 'och': 691,\n",
              " 'refined': 692,\n",
              " 'quirk': 693,\n",
              " 'published': 694,\n",
              " 'grishman': 695,\n",
              " 'domain': 696,\n",
              " 'bilingual': 697,\n",
              " 'reason': 698,\n",
              " 'subjectverb': 699,\n",
              " 'motivation': 700,\n",
              " 'strube': 701,\n",
              " 'exceptions': 702,\n",
              " 'solve': 703,\n",
              " 'exploiting': 704,\n",
              " 'handbuilt': 705,\n",
              " 'incorporate': 706,\n",
              " 'structures': 707,\n",
              " 'solution': 708,\n",
              " 'ie': 709,\n",
              " 'hyperlinked': 710,\n",
              " 'pages': 711,\n",
              " 'complete': 712,\n",
              " 'sequences': 713,\n",
              " 'realized': 714,\n",
              " 'sahlgren': 715,\n",
              " 'baroni': 716,\n",
              " 'window': 717,\n",
              " 'arguably': 718,\n",
              " 'treebanks': 719,\n",
              " 'highly': 720,\n",
              " 'macken': 721,\n",
              " 'wikifs': 722,\n",
              " 'defined': 723,\n",
              " 'choose': 724,\n",
              " 'titov': 725,\n",
              " 'utilized': 726,\n",
              " 'luo': 727,\n",
              " 'iii': 728,\n",
              " 'significant': 729,\n",
              " 'inspiration': 730,\n",
              " 'pioneering': 731,\n",
              " 'fundamentally': 732,\n",
              " 'corelex': 733,\n",
              " 'reasonably': 734,\n",
              " 'bayes': 735,\n",
              " 'dynamically': 736,\n",
              " 'determine': 737,\n",
              " 'signatures': 738,\n",
              " 'with': 739,\n",
              " 'ratnaparkhi': 740,\n",
              " 'finitestate': 741,\n",
              " 'hyponymy': 742,\n",
              " 'creating': 743,\n",
              " 'similarword': 744,\n",
              " 'jaccard': 745,\n",
              " 'gildea': 746,\n",
              " 'considered': 747,\n",
              " 'from': 748,\n",
              " 'jiang': 749,\n",
              " 'conrath': 750,\n",
              " 'lavie': 751,\n",
              " 'string': 752,\n",
              " 'leverage': 753,\n",
              " 'critical': 754,\n",
              " 'sterling': 755,\n",
              " 'classifier': 756,\n",
              " 'dredze': 757,\n",
              " 'fscore': 758,\n",
              " 'collocation': 759,\n",
              " 'pasca': 760,\n",
              " 'crosslingual': 761,\n",
              " 'vossen': 762,\n",
              " 'efficiently': 763,\n",
              " 'pure': 764,\n",
              " 'standard': 765,\n",
              " 'popular': 766,\n",
              " 'neglected': 767,\n",
              " 'notable': 768,\n",
              " 'kupiec': 769,\n",
              " 'linguistics': 770,\n",
              " 'palmers': 771,\n",
              " 'palmer': 772,\n",
              " 'resniks': 773,\n",
              " 'input': 774,\n",
              " 'roark': 775,\n",
              " 'computational': 776,\n",
              " 'complementary': 777,\n",
              " 'centrality': 778,\n",
              " 'choueka': 779,\n",
              " 'germantoenglish': 780,\n",
              " 'candidates': 781,\n",
              " 'provide': 782,\n",
              " 'art': 783,\n",
              " 'expensive': 784,\n",
              " 'frenchenglish': 785,\n",
              " 'liang': 786,\n",
              " 'insights': 787,\n",
              " 'linguisticallymotivated': 788,\n",
              " 'syntaxbased': 789,\n",
              " 'jansche': 790,\n",
              " 'significantly': 791,\n",
              " 'performing': 792,\n",
              " 'extensively': 793,\n",
              " 'minimizing': 794,\n",
              " 'capture': 795,\n",
              " 'possibilities': 796,\n",
              " 'goldberg': 797,\n",
              " 'keller': 798,\n",
              " 'dice': 799,\n",
              " 'none': 800,\n",
              " 'extraction': 801,\n",
              " 'weighting': 802,\n",
              " 'chklovski': 803,\n",
              " 'instances': 804,\n",
              " 'naive': 805,\n",
              " 'let': 806,\n",
              " 'mercer': 807,\n",
              " 'beginnings': 808,\n",
              " 'currently': 809,\n",
              " 'trillions': 810,\n",
              " 'explained': 811,\n",
              " 'leads': 812,\n",
              " 'trees': 813,\n",
              " 'expectations': 814,\n",
              " 'inverse': 815,\n",
              " 'halteren': 816,\n",
              " 'schneider': 817,\n",
              " 'aproaches': 818,\n",
              " 'seen': 819,\n",
              " 'tricky': 820,\n",
              " 'evident': 821,\n",
              " 'conflicting': 822,\n",
              " 'conclusions': 823,\n",
              " 'an': 824,\n",
              " 'marcus': 825,\n",
              " 'discriminative': 826,\n",
              " 'candidate': 827,\n",
              " 'retrainable': 828,\n",
              " 'datasets': 829,\n",
              " 'fundamental': 830,\n",
              " 'ir': 831,\n",
              " 'viewed': 832,\n",
              " 'ambiguous': 833,\n",
              " 'assigned': 834,\n",
              " 'predefined': 835,\n",
              " 'suggests': 836,\n",
              " 'hints': 837,\n",
              " 'doddington': 838,\n",
              " 'banerjee': 839,\n",
              " 'ter': 840,\n",
              " 'snover': 841,\n",
              " 'wer': 842,\n",
              " 'max': 843,\n",
              " 'schemes': 844,\n",
              " 'inchmm': 845,\n",
              " 'maxbleu': 846,\n",
              " 'incorporating': 847,\n",
              " 'gale': 848,\n",
              " 'map': 849,\n",
              " 'sparse': 850,\n",
              " 'lms': 851,\n",
              " 'google': 852,\n",
              " 'pruning': 853,\n",
              " 'emami': 854,\n",
              " 'skepticism': 855,\n",
              " 'actually': 856,\n",
              " 'clough': 857,\n",
              " 'stevenson': 858,\n",
              " 'hope': 859,\n",
              " 'questionanswering': 860,\n",
              " 'simplication': 861,\n",
              " 'summarisation': 862,\n",
              " 'benet': 863,\n",
              " 'coverage': 864,\n",
              " 'strengths': 865,\n",
              " 'detail': 866,\n",
              " 'was': 867,\n",
              " 'lacatusu': 868,\n",
              " 'leading': 869,\n",
              " 'multiple': 870,\n",
              " 'merialdo': 871,\n",
              " 'spans': 872,\n",
              " 'munoz': 873,\n",
              " 'fluency': 874,\n",
              " 'hold': 875,\n",
              " 'actual': 876,\n",
              " 'zwarts': 877,\n",
              " 'dras': 878,\n",
              " 'adding': 879,\n",
              " 'advantage': 880,\n",
              " 'parameters': 881,\n",
              " 'online': 882,\n",
              " 'eigenvector': 883,\n",
              " 'inventories': 884,\n",
              " 'gimenez': 885,\n",
              " 'marquez': 886,\n",
              " 'here': 887,\n",
              " 'stupid': 888,\n",
              " 'kneserney': 889,\n",
              " 'counts': 890,\n",
              " 'matrixtree': 891,\n",
              " 'satta': 892,\n",
              " 'probabilistic': 893,\n",
              " 'risk': 894,\n",
              " 'interested': 895,\n",
              " 'grouped': 896,\n",
              " 'dickinson': 897,\n",
              " 'tseng': 898,\n",
              " 'very': 899,\n",
              " 'languageprocessingnlpapplications': 900,\n",
              " 'suchasprepositional': 901,\n",
              " 'attachment': 902,\n",
              " 'otheranaphora': 903,\n",
              " 'spellingcorrection': 904,\n",
              " 'confusablewordsetdisambiguation': 905,\n",
              " 'modjeska': 906,\n",
              " 'atterer': 907,\n",
              " 'daume': 908,\n",
              " 'sequence': 909,\n",
              " 'probably': 910,\n",
              " 'dened': 911,\n",
              " 'assumed': 912,\n",
              " 'correspond': 913,\n",
              " 'strong': 914,\n",
              " 'analogy': 915,\n",
              " 'anotherstateoftheartwsdengineacombination': 916,\n",
              " 'boosting': 917,\n",
              " 'kernel': 918,\n",
              " 'pca': 919,\n",
              " 'consideration': 920,\n",
              " 'adapt': 921,\n",
              " 'conclude': 922,\n",
              " 'noting': 923,\n",
              " 'automated': 924,\n",
              " 'letter': 925,\n",
              " 'parameterspipikof': 926,\n",
              " 'observationsy': 927,\n",
              " 'psypipik': 928,\n",
              " 'tproductdisplay': 929,\n",
              " 'pststpytst': 930,\n",
              " 'clearly': 931,\n",
              " 'poor': 932,\n",
              " 'kirchoff': 933,\n",
              " 'three': 934,\n",
              " 'dermatas': 935,\n",
              " 'independence': 936,\n",
              " 'accuracies': 937,\n",
              " 'zitouni': 938,\n",
              " 'equation': 939,\n",
              " 'lemmas': 940,\n",
              " 'synonymy': 941,\n",
              " 'hypernymy': 942,\n",
              " 'antonymy': 943,\n",
              " 'widelyused': 944,\n",
              " 'nombank': 945,\n",
              " 'infers': 946,\n",
              " 'constructedis': 947,\n",
              " 'manli': 948,\n",
              " 'examples': 949,\n",
              " 'greinstette': 950,\n",
              " 'summationtext': 951,\n",
              " 'wikipediabased': 952,\n",
              " 'learner': 953,\n",
              " 'relies': 954,\n",
              " 'allow': 955,\n",
              " 'larger': 956,\n",
              " 'stolcke': 957,\n",
              " 'golomb': 958,\n",
              " 'coding': 959,\n",
              " 'areas': 960,\n",
              " 'andparsinggoodman': 961,\n",
              " 'titovandhenderson': 962,\n",
              " 'dunning': 963,\n",
              " 'parser': 964,\n",
              " 'leverages': 965,\n",
              " 'approachesdescribed': 966,\n",
              " 'dedicated': 967,\n",
              " 'efforts': 968,\n",
              " 'mann': 969,\n",
              " 'ponzetto': 970,\n",
              " 'rather': 971,\n",
              " 'cosine': 972,\n",
              " 'decoder': 973,\n",
              " 'application': 974,\n",
              " 'albeit': 975,\n",
              " 'proven': 976,\n",
              " 'small': 977,\n",
              " 'retain': 978,\n",
              " 'v': 979,\n",
              " 'prior': 980,\n",
              " 'addition': 981,\n",
              " 'interrupted': 982,\n",
              " 'uninterrupted': 983,\n",
              " 'extensive': 984,\n",
              " 'mword': 985,\n",
              " 'human': 986,\n",
              " 'scores': 987,\n",
              " 'wordsense': 988,\n",
              " 'deploy': 989,\n",
              " 'contextdependent': 990,\n",
              " 'nonprojective': 991,\n",
              " 'coreference': 992,\n",
              " 'part': 993,\n",
              " 'patrick': 994,\n",
              " 'gauch': 995,\n",
              " 'rachakonda': 996,\n",
              " 'vegnaduzzo': 997,\n",
              " 'pwf': 998,\n",
              " 'pwpf': 999,\n",
              " 'nrels': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function to read GloCe Vector file\n",
        "def read_glove_vector(glove_vec):\n",
        "  with open(glove_vec, 'r', encoding='UTF-8') as f:\n",
        "    words = set()\n",
        "    word_to_vec_map = {}\n",
        "    for line in f:\n",
        "      w_line = line.split()\n",
        "      curr_word = w_line[0]\n",
        "      word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n",
        "\n",
        "\n",
        "\n",
        "  return word_to_vec_map"
      ],
      "metadata": {
        "id": "_aaj6FllILNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read in GloVe vector from Google Drive (premade mapping of words to be used for sentiment analysis)\n",
        "word_to_vec_map = read_glove_vector('/content/drive/MyDrive/UMD - senior year/spring 2022/439D/project/glove.6B.50d.txt')"
      ],
      "metadata": {
        "id": "K3OJwRicILND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "\n",
        "# create embedding matrix (all words in GloVe vector assigned to correct value matrix, all others assigend to 0 vector)\n",
        "vocab_len = len(words_to_index) + 1\n",
        "embed_vector_len = word_to_vec_map['moon'].shape[0]\n",
        "\n",
        "emb_matrix = np.zeros((vocab_len, embed_vector_len))\n",
        "\n",
        "\n",
        "#get glove coordinates of words that are in BOTH glove list and in training data sentences \n",
        "for word, index in words_to_index.items():\n",
        "  embedding_vector = word_to_vec_map.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    print(word)\n",
        "    emb_matrix[index, :] = embedding_vector\n",
        "\n",
        "embedding_layer = Embedding(input_dim=vocab_len, output_dim=embed_vector_len, input_length=maxLen, weights = [emb_matrix], trainable=False)"
      ],
      "metadata": {
        "id": "SXC7zvfSILND",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1819fe43-dc2a-4e0a-a3f2-fbb50144579d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "al\n",
            "et\n",
            "church\n",
            "used\n",
            "information\n",
            "method\n",
            "not\n",
            "models\n",
            "hanks\n",
            "model\n",
            "tagging\n",
            "can\n",
            "word\n",
            "performance\n",
            "approach\n",
            "the\n",
            "cutting\n",
            "translation\n",
            "data\n",
            "words\n",
            "a\n",
            "based\n",
            "mutual\n",
            "wsd\n",
            "similarity\n",
            "smith\n",
            "language\n",
            "approaches\n",
            "this\n",
            "semantic\n",
            "lin\n",
            "results\n",
            "brown\n",
            "proposed\n",
            "similar\n",
            "using\n",
            "shown\n",
            "statistical\n",
            "dependency\n",
            "methods\n",
            "we\n",
            "also\n",
            "measure\n",
            "collocations\n",
            "wu\n",
            "pos\n",
            "markov\n",
            "turney\n",
            "for\n",
            "work\n",
            "in\n",
            "machine\n",
            "mi\n",
            "including\n",
            "one\n",
            "unsupervised\n",
            "two\n",
            "reported\n",
            "algorithm\n",
            "johnson\n",
            "large\n",
            "wikipedia\n",
            "kazama\n",
            "training\n",
            "applications\n",
            "available\n",
            "hidden\n",
            "collins\n",
            "eg\n",
            "thus\n",
            "well\n",
            "since\n",
            "use\n",
            "lexical\n",
            "mckeown\n",
            "jing\n",
            "systems\n",
            "corpora\n",
            "sentence\n",
            "developed\n",
            "patterns\n",
            "gazetteer\n",
            "distributional\n",
            "no\n",
            "syntactic\n",
            "many\n",
            "english\n",
            "system\n",
            "outperform\n",
            "lins\n",
            "text\n",
            "features\n",
            "modeling\n",
            "pointwise\n",
            "unlike\n",
            "b\n",
            "difficult\n",
            "hmm\n",
            "dagan\n",
            "taggers\n",
            "resources\n",
            "version\n",
            "better\n",
            "processing\n",
            "baseline\n",
            "improve\n",
            "although\n",
            "networks\n",
            "problem\n",
            "score\n",
            "found\n",
            "corpus\n",
            "number\n",
            "chan\n",
            "disambiguation\n",
            "good\n",
            "several\n",
            "mihalcea\n",
            "relations\n",
            "uses\n",
            "widely\n",
            "applied\n",
            "sense\n",
            "however\n",
            "wordnet\n",
            "smt\n",
            "identify\n",
            "automatically\n",
            "brill\n",
            "techniques\n",
            "our\n",
            "learning\n",
            "classes\n",
            "koehn\n",
            "zhang\n",
            "recently\n",
            "class\n",
            "due\n",
            "hindle\n",
            "pairs\n",
            "different\n",
            "lee\n",
            "ramage\n",
            "trained\n",
            "mcdonald\n",
            "related\n",
            "quality\n",
            "recent\n",
            "jelinek\n",
            "hughes\n",
            "important\n",
            "much\n",
            "success\n",
            "introduction\n",
            "etc\n",
            "lexicon\n",
            "rapp\n",
            "provides\n",
            "past\n",
            "byrne\n",
            "weeds\n",
            "possible\n",
            "smadja\n",
            "measures\n",
            "achieved\n",
            "may\n",
            "ner\n",
            "equivalence\n",
            "notoriously\n",
            "schutze\n",
            "like\n",
            "task\n",
            "expected\n",
            "distributions\n",
            "structure\n",
            "tasks\n",
            "wellknown\n",
            "context\n",
            "terms\n",
            "set\n",
            "result\n",
            "first\n",
            "annotated\n",
            "s\n",
            "tags\n",
            "successfully\n",
            "best\n",
            "marcu\n",
            "without\n",
            "hoang\n",
            "require\n",
            "test\n",
            "edge\n",
            "phrases\n",
            "note\n",
            "feature\n",
            "introduced\n",
            "generated\n",
            "derose\n",
            "clusters\n",
            "comparison\n",
            "alignment\n",
            "morphological\n",
            "promise\n",
            "as\n",
            "abney\n",
            "f\n",
            "extracted\n",
            "combination\n",
            "ambiguity\n",
            "among\n",
            "requiring\n",
            "representations\n",
            "property\n",
            "events\n",
            "general\n",
            "interesting\n",
            "association\n",
            "natural\n",
            "weights\n",
            "even\n",
            "representation\n",
            "outperformed\n",
            "potential\n",
            "obtaining\n",
            "weir\n",
            "web\n",
            "pedersen\n",
            "straightforward\n",
            "estimation\n",
            "pereira\n",
            "various\n",
            "observed\n",
            "caveat\n",
            "reliance\n",
            "studies\n",
            "want\n",
            "outperforms\n",
            "discover\n",
            "contrast\n",
            "koo\n",
            "papers\n",
            "looked\n",
            "simpler\n",
            "procedure\n",
            "respectively\n",
            "existing\n",
            "smaller\n",
            "automatic\n",
            "suzuki\n",
            "amount\n",
            "lexicography\n",
            "detailed\n",
            "chains\n",
            "smoothing\n",
            "phrase\n",
            "others\n",
            "japanese\n",
            "interest\n",
            "publicly\n",
            "examined\n",
            "formalized\n",
            "lund\n",
            "burgess\n",
            "edmonds\n",
            "lemaire\n",
            "senses\n",
            "increases\n",
            "preferred\n",
            "richer\n",
            "largest\n",
            "though\n",
            "statistically\n",
            "frequently\n",
            "slightly\n",
            "summarization\n",
            "marginal\n",
            "either\n",
            "supervised\n",
            "optimal\n",
            "approximately\n",
            "embeds\n",
            "edges\n",
            "cross\n",
            "offering\n",
            "entries\n",
            "see\n",
            "despite\n",
            "kumar\n",
            "unfortunately\n",
            "generalization\n",
            "frequency\n",
            "all\n",
            "problems\n",
            "parameter\n",
            "dropped\n",
            "freely\n",
            "following\n",
            "tested\n",
            "significance\n",
            "rare\n",
            "modest\n",
            "always\n",
            "understood\n",
            "showing\n",
            "modify\n",
            "manually\n",
            "unable\n",
            "carry\n",
            "mt\n",
            "accuracy\n",
            "length\n",
            "function\n",
            "hybrid\n",
            "research\n",
            "rules\n",
            "monotonic\n",
            "demonstrate\n",
            "unacceptable\n",
            "beyond\n",
            "cyclic\n",
            "listing\n",
            "surprisingly\n",
            "example\n",
            "potentially\n",
            "status\n",
            "it\n",
            "configurations\n",
            "purely\n",
            "inspect\n",
            "recognition\n",
            "together\n",
            "limitation\n",
            "points\n",
            "higher\n",
            "simple\n",
            "sentences\n",
            "t\n",
            "sophisticated\n",
            "arbitrary\n",
            "researchers\n",
            "goes\n",
            "deals\n",
            "on\n",
            "source\n",
            "distribution\n",
            "statistics\n",
            "external\n",
            "there\n",
            "weight\n",
            "high\n",
            "decoding\n",
            "parsing\n",
            "previous\n",
            "tree\n",
            "space\n",
            "joint\n",
            "probabilities\n",
            "retrieval\n",
            "search\n",
            "given\n",
            "m\n",
            "p\n",
            "time\n",
            "explored\n",
            "years\n",
            "described\n",
            "liu\n",
            "gram\n",
            "whether\n",
            "xerox\n",
            "xy\n",
            "efficient\n",
            "selection\n",
            "additional\n",
            "acquisition\n",
            "useful\n",
            "table\n",
            "lead\n",
            "clustering\n",
            "knowledge\n",
            "functions\n",
            "pair\n",
            "effective\n",
            "spaces\n",
            "resnik\n",
            "and\n",
            "dolan\n",
            "noun\n",
            "huang\n",
            "speech\n",
            "include\n",
            "contextual\n",
            "ability\n",
            "reordering\n",
            "advances\n",
            "improvement\n",
            "integrating\n",
            "bleu\n",
            "improvements\n",
            "parse\n",
            "component\n",
            "ways\n",
            "entropy\n",
            "starting\n",
            "metric\n",
            "editing\n",
            "factored\n",
            "process\n",
            "sets\n",
            "unlabeled\n",
            "show\n",
            "luk\n",
            "parsers\n",
            "analysis\n",
            "improved\n",
            "five\n",
            "maximum\n",
            "perform\n",
            "y\n",
            "constructed\n",
            "mccarthy\n",
            "pmi\n",
            "negative\n",
            "kurland\n",
            "instance\n",
            "category\n",
            "ravichandran\n",
            "usually\n",
            "values\n",
            "nlp\n",
            "powerful\n",
            "readily\n",
            "tagger\n",
            "tag\n",
            "commonly\n",
            "demonstrated\n",
            "particular\n",
            "makes\n",
            "variety\n",
            "d\n",
            "distributed\n",
            "framework\n",
            "eisner\n",
            "volk\n",
            "x\n",
            "wang\n",
            "size\n",
            "compute\n",
            "experiments\n",
            "goldwater\n",
            "will\n",
            "helps\n",
            "curran\n",
            "goel\n",
            "fraser\n",
            "describe\n",
            "ratio\n",
            "promising\n",
            "compared\n",
            "intuitively\n",
            "appealing\n",
            "per\n",
            "might\n",
            "dependencies\n",
            "dictionaries\n",
            "simplicity\n",
            "thesaurus\n",
            "log\n",
            "lopez\n",
            "literature\n",
            "known\n",
            "entity\n",
            "grammars\n",
            "algorithms\n",
            "chen\n",
            "evans\n",
            "knight\n",
            "name\n",
            "accurate\n",
            "states\n",
            "griffiths\n",
            "associations\n",
            "considerable\n",
            "pattern\n",
            "em\n",
            "according\n",
            "relatedness\n",
            "compare\n",
            "articles\n",
            "scale\n",
            "output\n",
            "hmms\n",
            "erkan\n",
            "radev\n",
            "linguistic\n",
            "previously\n",
            "directly\n",
            "scheme\n",
            "relation\n",
            "concepts\n",
            "section\n",
            "showed\n",
            "named\n",
            "contexts\n",
            "resolution\n",
            "k\n",
            "typically\n",
            "classification\n",
            "computed\n",
            "done\n",
            "issue\n",
            "mani\n",
            "main\n",
            "reduction\n",
            "less\n",
            "types\n",
            "page\n",
            "while\n",
            "c\n",
            "range\n",
            "prediction\n",
            "great\n",
            "grammar\n",
            "obtained\n",
            "target\n",
            "instead\n",
            "grouping\n",
            "to\n",
            "nouns\n",
            "w\n",
            "side\n",
            "claim\n",
            "extremely\n",
            "whereas\n",
            "give\n",
            "now\n",
            "stochastic\n",
            "scoring\n",
            "latent\n",
            "moses\n",
            "evaluation\n",
            "article\n",
            "extract\n",
            "compression\n",
            "gates\n",
            "independent\n",
            "j\n",
            "positive\n",
            "n\n",
            "verbs\n",
            "partial\n",
            "wide\n",
            "then\n",
            "state\n",
            "study\n",
            "galley\n",
            "definition\n",
            "calculate\n",
            "employing\n",
            "ruge\n",
            "goodman\n",
            "relevant\n",
            "employed\n",
            "took\n",
            "groups\n",
            "able\n",
            "second\n",
            "by\n",
            "paper\n",
            "made\n",
            "matrix\n",
            "cases\n",
            "schmid\n",
            "hand\n",
            "label\n",
            "inventory\n",
            "furthermore\n",
            "probability\n",
            "moore\n",
            "build\n",
            "but\n",
            "fact\n",
            "works\n",
            "gazetteers\n",
            "written\n",
            "robust\n",
            "frequencies\n",
            "distance\n",
            "produce\n",
            "bayesian\n",
            "brin\n",
            "kleinberg\n",
            "purpose\n",
            "shows\n",
            "annotation\n",
            "involve\n",
            "identification\n",
            "create\n",
            "competitive\n",
            "train\n",
            "henderson\n",
            "pp\n",
            "toral\n",
            "coefficient\n",
            "except\n",
            "verb\n",
            "topic\n",
            "surpass\n",
            "pad\n",
            "categorisation\n",
            "equally\n",
            "effect\n",
            "comparisons\n",
            "van\n",
            "yield\n",
            "integration\n",
            "alternative\n",
            "erk\n",
            "estimated\n",
            "form\n",
            "report\n",
            "metrics\n",
            "nist\n",
            "meteor\n",
            "news\n",
            "novel\n",
            "och\n",
            "refined\n",
            "quirk\n",
            "published\n",
            "domain\n",
            "bilingual\n",
            "reason\n",
            "motivation\n",
            "strube\n",
            "exceptions\n",
            "solve\n",
            "exploiting\n",
            "handbuilt\n",
            "incorporate\n",
            "structures\n",
            "solution\n",
            "ie\n",
            "hyperlinked\n",
            "pages\n",
            "complete\n",
            "sequences\n",
            "realized\n",
            "baroni\n",
            "window\n",
            "arguably\n",
            "highly\n",
            "macken\n",
            "defined\n",
            "choose\n",
            "titov\n",
            "utilized\n",
            "luo\n",
            "iii\n",
            "significant\n",
            "inspiration\n",
            "pioneering\n",
            "fundamentally\n",
            "reasonably\n",
            "bayes\n",
            "dynamically\n",
            "determine\n",
            "signatures\n",
            "with\n",
            "creating\n",
            "jaccard\n",
            "gildea\n",
            "considered\n",
            "from\n",
            "jiang\n",
            "conrath\n",
            "lavie\n",
            "string\n",
            "leverage\n",
            "critical\n",
            "sterling\n",
            "classifier\n",
            "collocation\n",
            "pasca\n",
            "vossen\n",
            "efficiently\n",
            "pure\n",
            "standard\n",
            "popular\n",
            "neglected\n",
            "notable\n",
            "linguistics\n",
            "palmers\n",
            "palmer\n",
            "input\n",
            "roark\n",
            "computational\n",
            "complementary\n",
            "centrality\n",
            "candidates\n",
            "provide\n",
            "art\n",
            "expensive\n",
            "liang\n",
            "insights\n",
            "significantly\n",
            "performing\n",
            "extensively\n",
            "minimizing\n",
            "capture\n",
            "possibilities\n",
            "goldberg\n",
            "keller\n",
            "dice\n",
            "none\n",
            "extraction\n",
            "weighting\n",
            "instances\n",
            "naive\n",
            "let\n",
            "mercer\n",
            "beginnings\n",
            "currently\n",
            "trillions\n",
            "explained\n",
            "leads\n",
            "trees\n",
            "expectations\n",
            "inverse\n",
            "schneider\n",
            "seen\n",
            "tricky\n",
            "evident\n",
            "conflicting\n",
            "conclusions\n",
            "an\n",
            "marcus\n",
            "discriminative\n",
            "candidate\n",
            "datasets\n",
            "fundamental\n",
            "ir\n",
            "viewed\n",
            "ambiguous\n",
            "assigned\n",
            "predefined\n",
            "suggests\n",
            "hints\n",
            "doddington\n",
            "banerjee\n",
            "ter\n",
            "wer\n",
            "max\n",
            "schemes\n",
            "incorporating\n",
            "gale\n",
            "map\n",
            "sparse\n",
            "lms\n",
            "google\n",
            "pruning\n",
            "emami\n",
            "skepticism\n",
            "actually\n",
            "clough\n",
            "stevenson\n",
            "hope\n",
            "benet\n",
            "coverage\n",
            "strengths\n",
            "detail\n",
            "was\n",
            "leading\n",
            "multiple\n",
            "spans\n",
            "munoz\n",
            "fluency\n",
            "hold\n",
            "actual\n",
            "dras\n",
            "adding\n",
            "advantage\n",
            "parameters\n",
            "online\n",
            "eigenvector\n",
            "inventories\n",
            "gimenez\n",
            "marquez\n",
            "here\n",
            "stupid\n",
            "counts\n",
            "satta\n",
            "probabilistic\n",
            "risk\n",
            "interested\n",
            "grouped\n",
            "dickinson\n",
            "tseng\n",
            "very\n",
            "attachment\n",
            "modjeska\n",
            "daume\n",
            "sequence\n",
            "probably\n",
            "assumed\n",
            "correspond\n",
            "strong\n",
            "analogy\n",
            "boosting\n",
            "kernel\n",
            "pca\n",
            "consideration\n",
            "adapt\n",
            "conclude\n",
            "noting\n",
            "automated\n",
            "letter\n",
            "clearly\n",
            "poor\n",
            "kirchoff\n",
            "three\n",
            "independence\n",
            "accuracies\n",
            "zitouni\n",
            "equation\n",
            "lemmas\n",
            "synonymy\n",
            "infers\n",
            "manli\n",
            "examples\n",
            "learner\n",
            "relies\n",
            "allow\n",
            "larger\n",
            "golomb\n",
            "coding\n",
            "areas\n",
            "dunning\n",
            "parser\n",
            "leverages\n",
            "dedicated\n",
            "efforts\n",
            "mann\n",
            "rather\n",
            "cosine\n",
            "decoder\n",
            "application\n",
            "albeit\n",
            "proven\n",
            "small\n",
            "retain\n",
            "v\n",
            "prior\n",
            "addition\n",
            "interrupted\n",
            "uninterrupted\n",
            "extensive\n",
            "human\n",
            "scores\n",
            "deploy\n",
            "part\n",
            "patrick\n",
            "gauch\n",
            "pwf\n",
            "wf\n",
            "adaptation\n",
            "veale\n",
            "sat\n",
            "martin\n",
            "common\n",
            "make\n",
            "chiang\n",
            "projects\n",
            "progress\n",
            "shen\n",
            "derived\n",
            "relative\n",
            "likelihood\n",
            "just\n",
            "later\n",
            "etzioni\n",
            "precise\n",
            "completed\n",
            "author\n",
            "inc\n",
            "suffix\n",
            "arrays\n",
            "surface\n",
            "klein\n",
            "benefits\n",
            "module\n",
            "often\n",
            "aspect\n",
            "narrow\n",
            "distinguishing\n",
            "people\n",
            "focusing\n",
            "person\n",
            "rooth\n",
            "graph\n",
            "construction\n",
            "thea\n",
            "suitable\n",
            "value\n",
            "indeed\n",
            "handle\n",
            "presents\n",
            "labeled\n",
            "initial\n",
            "way\n",
            "reduced\n",
            "expert\n",
            "mechanical\n",
            "turk\n",
            "snow\n",
            "solutions\n",
            "ranging\n",
            "type\n",
            "order\n",
            "attention\n",
            "formula\n",
            "theorem\n",
            "structured\n",
            "questions\n",
            "achieving\n",
            "seminal\n",
            "unless\n",
            "case\n",
            "they\n",
            "generate\n",
            "some\n",
            "estimates\n",
            "i\n",
            "estimate\n",
            "focus\n",
            "conditional\n",
            "pietra\n",
            "dictionary\n",
            "find\n",
            "texts\n",
            "figure\n",
            "list\n",
            "unseen\n",
            "term\n",
            "minimum\n",
            "katz\n",
            "community\n",
            "modified\n",
            "precision\n",
            "relational\n",
            "aaa\n",
            "phi\n",
            "schuetze\n",
            "srikant\n",
            "rule\n",
            "garside\n",
            "leech\n",
            "parsed\n",
            "vectors\n",
            "propose\n",
            "strategy\n",
            "gigantic\n",
            "key\n",
            "decoders\n",
            "consequence\n",
            "asymptotic\n",
            "running\n",
            "compelling\n",
            "grammatical\n",
            "properties\n",
            "need\n",
            "certainly\n",
            "daunting\n",
            "annotator\n",
            "newly\n",
            "access\n",
            "pool\n",
            "inexpensive\n",
            "amazon\n",
            "embedding\n",
            "game\n",
            "played\n",
            "volunteers\n",
            "von\n",
            "ahn\n",
            "manning\n",
            "finkel\n",
            "calzolari\n",
            "bindi\n",
            "optimization\n",
            "technique\n",
            "priors\n",
            "associated\n",
            "yang\n",
            "includes\n",
            "answer\n",
            "attains\n",
            "wiebe\n",
            "thesauri\n",
            "idea\n",
            "maximization\n",
            "lemma\n",
            "studied\n",
            "is\n",
            "categories\n",
            "he\n",
            "another\n",
            "generation\n",
            "della\n",
            "bahl\n",
            "vector\n",
            "local\n",
            "measured\n",
            "recall\n",
            "these\n",
            "reduce\n",
            "within\n",
            "extend\n",
            "similarities\n",
            "clarke\n",
            "occur\n",
            "e\n",
            "wat\n",
            "di\n",
            "consider\n",
            "estimating\n",
            "presented\n",
            "compound\n",
            "received\n",
            "gao\n",
            "iterations\n",
            "settings\n",
            "global\n",
            "specific\n",
            "heuristic\n",
            "low\n",
            "along\n",
            "take\n",
            "other\n",
            "linear\n",
            "kim\n",
            "mooney\n",
            "account\n",
            "unknown\n",
            "logistic\n",
            "computing\n",
            "more\n",
            "mining\n",
            "extracting\n",
            "full\n",
            "malt\n",
            "calculated\n",
            "meaning\n",
            "step\n",
            "parallel\n",
            "bootstrapping\n",
            "taxonomy\n",
            "ney\n",
            "pagerank\n",
            "definitions\n",
            "obtain\n",
            "independently\n",
            "ij\n",
            "g\n",
            "inspired\n",
            "factors\n",
            "clark\n",
            "direct\n",
            "every\n",
            "matsuzaki\n",
            "times\n",
            "describes\n",
            "document\n",
            "current\n",
            "conventional\n",
            "takes\n",
            "norms\n",
            "evidence\n",
            "induction\n",
            "leaf\n",
            "likely\n",
            "forms\n",
            "ibm\n",
            "rst\n",
            "dat\n",
            "nok\n",
            "labels\n",
            "bias\n",
            "rosenfeld\n",
            "authors\n",
            "constraints\n",
            "division\n",
            "mrd\n",
            "discussed\n",
            "content\n",
            "nonlocal\n",
            "evaluations\n",
            "interpretation\n",
            "nonparametric\n",
            "requires\n",
            "variant\n",
            "baker\n",
            "putting\n",
            "languages\n",
            "shared\n",
            "error\n",
            "differences\n",
            "errors\n",
            "present\n",
            "li\n",
            "domains\n",
            "synchronous\n",
            "added\n",
            "next\n",
            "neighbors\n",
            "bruce\n",
            "expressions\n",
            "rank\n",
            "question\n",
            "viterbi\n",
            "pado\n",
            "su\n",
            "combine\n",
            "adjectives\n",
            "sentiment\n",
            "determined\n",
            "maximize\n",
            "cohesion\n",
            "evaluated\n",
            "florian\n",
            "leacock\n",
            "lrm\n",
            "total\n",
            "hierarchical\n",
            "learn\n",
            "ff\n",
            "tests\n",
            "opinion\n",
            "differs\n",
            "induced\n",
            "university\n",
            "ranking\n",
            "kind\n",
            "deal\n",
            "exponential\n",
            "correlation\n",
            "choice\n",
            "rely\n",
            "translations\n",
            "select\n",
            "top\n",
            "random\n",
            "cherry\n",
            "layer\n",
            "field\n",
            "longer\n",
            "root\n",
            "discussion\n",
            "frequent\n",
            "span\n",
            "specialist\n",
            "roth\n",
            "roughly\n",
            "ng\n",
            "singer\n",
            "are\n",
            "character\n",
            "barzilay\n",
            "mohammad\n",
            "average\n",
            "extension\n",
            "transition\n",
            "direction\n",
            "inference\n",
            "basis\n",
            "sample\n",
            "depends\n",
            "variants\n",
            "voutilainen\n",
            "upper\n",
            "base\n",
            "parses\n",
            "synonyms\n",
            "means\n",
            "lm\n",
            "decision\n",
            "aligned\n",
            "new\n",
            "variable\n",
            "fixed\n",
            "experimental\n",
            "adj\n",
            "generative\n",
            "pisa\n",
            "voorhees\n",
            "building\n",
            "ti\n",
            "allows\n",
            "setting\n",
            "calculation\n",
            "divided\n",
            "early\n",
            "cited\n",
            "tend\n",
            "observe\n",
            "ando\n",
            "correlate\n",
            "summary\n",
            "semantics\n",
            "overview\n",
            "projective\n",
            "achieve\n",
            "finally\n",
            "major\n",
            "boundaries\n",
            "examine\n",
            "included\n",
            "implementation\n",
            "taking\n",
            "decreases\n",
            "applying\n",
            "iterative\n",
            "czech\n",
            "steps\n",
            "bnc\n",
            "wilks\n",
            "closely\n",
            "reports\n",
            "performs\n",
            "moens\n",
            "wong\n",
            "hiero\n",
            "dataset\n",
            "evaluating\n",
            "lesk\n",
            "benefit\n",
            "drawn\n",
            "recognizer\n",
            "hypotheses\n",
            "relationships\n",
            "entities\n",
            "required\n",
            "trie\n",
            "byte\n",
            "bits\n",
            "emphasize\n",
            "liddy\n",
            "close\n",
            "mst\n",
            "units\n",
            "follow\n",
            "primarily\n",
            "database\n",
            "hirst\n",
            "reasoning\n",
            "opinions\n",
            "taken\n",
            "numbers\n",
            "guthrie\n",
            "handcrafted\n",
            "national\n",
            "variables\n",
            "sampler\n",
            "samplers\n",
            "via\n",
            "explicitly\n",
            "tries\n",
            "sources\n",
            "strongly\n",
            "priori\n",
            "mapping\n",
            "true\n",
            "crf\n",
            "chunking\n",
            "fano\n",
            "above\n",
            "run\n",
            "integrated\n",
            "considering\n",
            "bangalore\n",
            "joshi\n",
            "baldwin\n",
            "vb\n",
            "amounts\n",
            "provided\n",
            "ln\n",
            "svm\n",
            "notion\n",
            "degree\n",
            "built\n",
            "lafferty\n",
            "reducing\n",
            "count\n",
            "hasan\n",
            "classifiers\n",
            "mccallum\n",
            "baldridge\n",
            "divergence\n",
            "referred\n",
            "almost\n",
            "estimator\n",
            "least\n",
            "concept\n",
            "network\n",
            "improving\n",
            "school\n",
            "approximations\n",
            "which\n",
            "specificity\n",
            "contain\n",
            "extra\n",
            "aim\n",
            "explores\n",
            "single\n",
            "extent\n",
            "resolving\n",
            "query\n",
            "hearst\n",
            "sparseness\n",
            "overlap\n",
            "fast\n",
            "implemented\n",
            "gold\n",
            "necessarily\n",
            "clustered\n",
            "correct\n",
            "ones\n",
            "bound\n",
            "corresponding\n",
            "infinite\n",
            "mainly\n",
            "px\n",
            "occurrences\n",
            "cpu\n",
            "node\n",
            "back\n",
            "generally\n",
            "resource\n",
            "exhaustive\n",
            "documents\n",
            "strategies\n",
            "line\n",
            "forward\n",
            "yet\n",
            "generates\n",
            "prune\n",
            "therefore\n",
            "parts\n",
            "watanabe\n",
            "summaries\n",
            "suggested\n",
            "asymptotically\n",
            "determinant\n",
            "itai\n",
            "level\n",
            "acquired\n",
            "neural\n",
            "reasons\n",
            "that\n",
            "introduce\n",
            "croft\n",
            "assign\n",
            "tagged\n",
            "material\n",
            "spanning\n",
            "apple\n",
            "tools\n",
            "setup\n",
            "monolingual\n",
            "difference\n",
            "vat\n",
            "quat\n",
            "british\n",
            "ranked\n",
            "constraint\n",
            "constituent\n",
            "zesch\n",
            "fragments\n",
            "sampson\n",
            "de\n",
            "gb\n",
            "collected\n",
            "empirical\n",
            "concise\n",
            "symbols\n",
            "future\n",
            "experiment\n",
            "distinct\n",
            "numerous\n",
            "survey\n",
            "near\n",
            "phenomena\n",
            "preferences\n",
            "strongest\n",
            "construct\n",
            "segmentation\n",
            "constrained\n",
            "reach\n",
            "paskin\n",
            "increased\n",
            "derivations\n",
            "both\n",
            "complex\n",
            "discourse\n",
            "inter\n",
            "alia\n",
            "limitations\n",
            "exploited\n",
            "intonation\n",
            "annotations\n",
            "indices\n",
            "attempts\n",
            "constructions\n",
            "carroll\n",
            "briscoe\n",
            "kubler\n",
            "ams\n",
            "otherwise\n",
            "further\n",
            "distances\n",
            "german\n",
            "rich\n",
            "bell\n",
            "chodorow\n",
            "assumption\n",
            "traditional\n",
            "ldv\n",
            "consulted\n",
            "roget\n",
            "attributional\n",
            "answering\n",
            "path\n",
            "of\n",
            "kurihara\n",
            "sato\n",
            "election\n",
            "ge\n",
            "lu\n",
            "where\n",
            "extensions\n",
            "style\n",
            "lappin\n",
            "kennedy\n",
            "designed\n",
            "bennett\n",
            "employs\n",
            "impact\n",
            "upon\n",
            "review\n",
            "uniform\n",
            "believe\n",
            "als\n",
            "must\n",
            "ergodic\n",
            "kuhn\n",
            "el\n",
            "specifically\n",
            "separately\n",
            "ca\n",
            "penn\n",
            "encode\n",
            "ldc\n",
            "block\n",
            "encoding\n",
            "randomized\n",
            "equivalent\n",
            "magnitude\n",
            "enhanced\n",
            "strings\n",
            "paik\n",
            "observations\n",
            "pronouns\n",
            "fj\n",
            "underlying\n",
            "predicted\n",
            "billion\n",
            "newswire\n",
            "matching\n",
            "brent\n",
            "dorr\n",
            "drawing\n",
            "regularization\n",
            "distinction\n",
            "semantically\n",
            "predictions\n",
            "choosing\n",
            "making\n",
            "tradeoff\n",
            "crafted\n",
            "illustrative\n",
            "lai\n",
            "kneser\n",
            "computer\n",
            "science\n",
            "taiwan\n",
            "predict\n",
            "salton\n",
            "contains\n",
            "mistakes\n",
            "huge\n",
            "involved\n",
            "beam\n",
            "equations\n",
            "left\n",
            "deleted\n",
            "interpolation\n",
            "refinement\n",
            "broad\n",
            "view\n",
            "aspects\n",
            "encyclopedias\n",
            "representative\n",
            "programs\n",
            "items\n",
            "sensitive\n",
            "l\n",
            "leave\n",
            "factor\n",
            "links\n",
            "strength\n",
            "analyze\n",
            "matsumoto\n",
            "reuters\n",
            "observing\n",
            "approximated\n",
            "z\n",
            "zy\n",
            "guiliano\n",
            "symmetric\n",
            "noted\n",
            "fields\n",
            "tj\n",
            "yielded\n",
            "reference\n",
            "apply\n",
            "extended\n",
            "normally\n",
            "subject\n",
            "head\n",
            "shorter\n",
            "modifying\n",
            "represent\n",
            "modifiers\n",
            "moving\n",
            "wn\n",
            "variance\n",
            "match\n",
            "vo\n",
            "ap\n",
            "frameworks\n",
            "summing\n",
            "marginals\n",
            "enforcing\n",
            "nonterminal\n",
            "threshold\n",
            "partly\n",
            "addressed\n",
            "appropriate\n",
            "details\n",
            "perceptron\n",
            "option\n",
            "mcclosky\n",
            "inducing\n",
            "denis\n",
            "remain\n",
            "biased\n",
            "indicated\n",
            "unit\n",
            "checked\n",
            "lookup\n",
            "rao\n",
            "prime\n",
            "ran\n",
            "estimators\n",
            "converge\n",
            "subjects\n",
            "exact\n",
            "mapped\n",
            "forming\n",
            "templates\n",
            "informatics\n",
            "edinburgh\n",
            "composed\n",
            "call\n",
            "positives\n",
            "false\n",
            "pasting\n",
            "perplexity\n",
            "adda\n",
            "conjunction\n",
            "shepherd\n",
            "adaptations\n",
            "mima\n",
            "matches\n",
            "hat\n",
            "listed\n",
            "plus\n",
            "morphemes\n",
            "petrov\n",
            "inherent\n",
            "explains\n",
            "or\n",
            "removing\n",
            "sekine\n",
            "chang\n",
            "prep\n",
            "acquiring\n",
            "elworthy\n",
            "expansion\n",
            "pang\n",
            "index\n",
            "ccg\n",
            "carried\n",
            "triple\n",
            "transducers\n",
            "million\n",
            "exist\n",
            "regression\n",
            "discovery\n",
            "parc\n",
            "press\n",
            "convergence\n",
            "allowing\n",
            "sampling\n",
            "buckshot\n",
            "looking\n",
            "simmons\n",
            "sometimes\n",
            "get\n",
            "tends\n",
            "correctly\n",
            "extracts\n",
            "selects\n",
            "versions\n",
            "pyx\n",
            "py\n",
            "days\n",
            "iteration\n",
            "pc\n",
            "deletion\n",
            "after\n",
            "giza\n",
            "again\n",
            "graphs\n",
            "increasing\n",
            "propagation\n",
            "deterministic\n",
            "short\n",
            "artificial\n",
            "product\n",
            "earlier\n",
            "adopted\n",
            "decisions\n",
            "driven\n",
            "looks\n",
            "atwell\n",
            "such\n",
            "lexicons\n",
            "isolation\n",
            "hits\n",
            "filtering\n",
            "adopt\n",
            "trials\n",
            "finding\n",
            "paradigms\n",
            "paradigmatic\n",
            "cannot\n",
            "original\n",
            "aggregate\n",
            "cluster\n",
            "seems\n",
            "rejoin\n",
            "belong\n",
            "returned\n",
            "say\n",
            "weighted\n",
            "tens\n",
            "millions\n",
            "fewer\n",
            "load\n",
            "comparable\n",
            "situation\n",
            "become\n",
            "sch\n",
            "revision\n",
            "revising\n",
            "inside\n",
            "dominated\n",
            "engineering\n",
            "graphical\n",
            "originally\n",
            "discuss\n",
            "typical\n",
            "participants\n",
            "decomposition\n",
            "evaluate\n",
            "cautious\n",
            "regardless\n",
            "sequential\n",
            "adapted\n",
            "discrimination\n",
            "mdi\n",
            "ne\n",
            "kempe\n",
            "itg\n",
            "easily\n",
            "modification\n",
            "surprising\n",
            "somewhat\n",
            "opposed\n",
            "body\n",
            "comes\n",
            "ending\n",
            "towell\n",
            "generalized\n",
            "necessary\n",
            "teacher\n",
            "teachers\n",
            "earliest\n",
            "nevertheless\n",
            "tokens\n",
            "henceforth\n",
            "gibbs\n",
            "alignments\n",
            "black\n",
            "focused\n",
            "refer\n",
            "distinctions\n",
            "practical\n",
            "comparative\n",
            "especially\n",
            "tuning\n",
            "operations\n",
            "combining\n",
            "substituting\n",
            "limit\n",
            "long\n",
            "bagga\n",
            "appears\n",
            "subset\n",
            "substitutes\n",
            "expectation\n",
            "occurrence\n",
            "illustrated\n",
            "bichat\n",
            "brok\n",
            "jok\n",
            "lat\n",
            "stok\n",
            "cat\n",
            "mok\n",
            "gat\n",
            "mat\n",
            "incorporated\n",
            "collection\n",
            "relatively\n",
            "chinese\n",
            "towards\n",
            "practically\n",
            "r\n",
            "bene\n",
            "gj\n",
            "chose\n",
            "contribution\n",
            "center\n",
            "ax\n",
            "cz\n",
            "below\n",
            "you\n",
            "needs\n",
            "accounted\n",
            "pointed\n",
            "calls\n",
            "windows\n",
            "out\n",
            "firth\n",
            "know\n",
            "restrictions\n",
            "noise\n",
            "predominant\n",
            "object\n",
            "eat\n",
            "raw\n",
            "arcs\n",
            "bod\n",
            "postprocessing\n",
            "performed\n",
            "final\n",
            "predictive\n",
            "merging\n",
            "null\n",
            "regarded\n",
            "mr\n",
            "mmi\n",
            "jv\n",
            "informatica\n",
            "universita\n",
            "italy\n",
            "prepositional\n",
            "marciniak\n",
            "depend\n",
            "employ\n",
            "ot\n",
            "minimal\n",
            "vary\n",
            "paraphrasing\n",
            "acquire\n",
            "chunk\n",
            "th\n",
            "rough\n",
            "thumb\n",
            "rose\n",
            "schwartz\n",
            "neighboring\n",
            "characteristics\n",
            "bai\n",
            "abstracting\n",
            "dirt\n",
            "titi\n",
            "witi\n",
            "us\n",
            "toward\n",
            "sparser\n",
            "dirichlet\n",
            "plan\n",
            "amaya\n",
            "cost\n",
            "embedded\n",
            "pms\n",
            "remaining\n",
            "jones\n",
            "ned\n",
            "any\n",
            "enforce\n",
            "ultimately\n",
            "free\n",
            "prepositions\n",
            "proves\n",
            "reassuring\n",
            "isozaki\n",
            "krishnan\n",
            "overall\n",
            "serve\n",
            "predictors\n",
            "detected\n",
            "compress\n",
            "interpreting\n",
            "compounds\n",
            "lauer\n",
            "nal\n",
            "gaining\n",
            "popularity\n",
            "morpheme\n",
            "teh\n",
            "whose\n",
            "peak\n",
            "wallstreet\n",
            "journal\n",
            "contingency\n",
            "buchholz\n",
            "marsi\n",
            "reveals\n",
            "move\n",
            "attempt\n",
            "phenomenon\n",
            "standards\n",
            "tobi\n",
            "tones\n",
            "break\n",
            "silverman\n",
            "targeting\n",
            "effects\n",
            "chance\n",
            "population\n",
            "petrovic\n",
            "cover\n",
            "denoted\n",
            "incomplete\n",
            "working\n",
            "presentation\n",
            "translating\n",
            "spanish\n",
            "morphology\n",
            "morphologically\n",
            "closer\n",
            "look\n",
            "sketch\n",
            "engine\n",
            "steier\n",
            "belew\n",
            "expressed\n",
            "retrieved\n",
            "rasp\n",
            "wilson\n",
            "subjective\n",
            "omit\n",
            "rise\n",
            "invalid\n",
            "unrealistic\n",
            "placeholders\n",
            "unique\n",
            "wacholder\n",
            "respect\n",
            "resembles\n",
            "restricted\n",
            "strict\n",
            "subclass\n",
            "cells\n",
            "summarize\n",
            "wiki\n",
            "surrounding\n",
            "lowe\n",
            "relevance\n",
            "saul\n",
            "targets\n",
            "resolve\n",
            "spoken\n",
            "venugopal\n",
            "marton\n",
            "role\n",
            "antecedent\n",
            "governing\n",
            "pronoun\n",
            "guessers\n",
            "endings\n",
            "capitalization\n",
            "approximate\n",
            "varying\n",
            "ideal\n",
            "sr\n",
            "advocated\n",
            "limiting\n",
            "connected\n",
            "observation\n",
            "reliability\n",
            "limited\n",
            "tight\n",
            "morphosyntactic\n",
            "translated\n",
            "combined\n",
            "facet\n",
            "dakka\n",
            "contained\n",
            "wsj\n",
            "bytes\n",
            "files\n",
            "delivered\n",
            "compact\n",
            "clarkson\n",
            "ids\n",
            "evaluates\n",
            "mcmc\n",
            "yielding\n",
            "supported\n",
            "founded\n",
            "xi\n",
            "young\n",
            "filter\n",
            "symmetrical\n",
            "par\n",
            "wsf\n",
            "wtf\n",
            "denote\n",
            "measurement\n",
            "synonym\n",
            "submission\n",
            "lattices\n",
            "ideas\n",
            "encodes\n",
            "iob\n",
            "focuses\n",
            "xia\n",
            "garman\n",
            "weinberg\n",
            "etal\n",
            "confidence\n",
            "renyi\n",
            "argues\n",
            "formulated\n",
            "analogical\n",
            "applies\n",
            "verbal\n",
            "gained\n",
            "issues\n",
            "faced\n",
            "forecasting\n",
            "observes\n",
            "involves\n",
            "qualities\n",
            "everyday\n",
            "cowie\n",
            "department\n",
            "tsing\n",
            "hua\n",
            "hsinchu\n",
            "roc\n",
            "handled\n",
            "efficiency\n",
            "bhattacharya\n",
            "investigating\n",
            "jain\n",
            "neal\n",
            "permutation\n",
            "celebrated\n",
            "citation\n",
            "sebastiani\n",
            "generalizes\n",
            "harvesting\n",
            "realize\n",
            "mentioned\n",
            "noisy\n",
            "plenty\n",
            "variability\n",
            "genres\n",
            "still\n",
            "open\n",
            "suet\n",
            "easy\n",
            "reliably\n",
            "changing\n",
            "sizes\n",
            "yamashina\n",
            "touch\n",
            "feasibility\n",
            "scope\n",
            "chart\n",
            "systematic\n",
            "generalizable\n",
            "meanings\n",
            "accessible\n",
            "readable\n",
            "coupled\n",
            "analytic\n",
            "apparatus\n",
            "needed\n",
            "fruitfully\n",
            "explore\n",
            "cope\n",
            "at\n",
            "massive\n",
            "borders\n",
            "perceptual\n",
            "experience\n",
            "assume\n",
            "resulting\n",
            "differed\n",
            "unclear\n",
            "exploits\n",
            "linked\n",
            "retained\n",
            "under\n",
            "certain\n",
            "conditions\n",
            "u\n",
            "prescribed\n",
            "ptb\n",
            "kudo\n",
            "aug\n",
            "dec\n",
            "eqs\n",
            "pr\n",
            "gf\n",
            "ferreira\n",
            "paca\n",
            "experimented\n",
            "frame\n",
            "similarly\n",
            "particularly\n",
            "lists\n",
            "nielsen\n",
            "leveraging\n",
            "fossum\n",
            "records\n",
            "books\n",
            "canisius\n",
            "sporleder\n",
            "michelson\n",
            "knoblock\n",
            "psd\n",
            "serious\n",
            "practice\n",
            "ji\n",
            "tfi\n",
            "modifier\n",
            "determiners\n",
            "spearman\n",
            "finkelstein\n",
            "odp\n",
            "cast\n",
            "theoretic\n",
            "french\n",
            "only\n",
            "fx\n",
            "fy\n",
            "meanwhile\n",
            "acknowledgments\n",
            "thank\n",
            "fellow\n",
            "organizers\n",
            "johan\n",
            "hall\n",
            "sandra\n",
            "ryan\n",
            "jens\n",
            "nilsson\n",
            "sebastian\n",
            "riedel\n",
            "deniz\n",
            "defect\n",
            "selecting\n",
            "draws\n",
            "medical\n",
            "carreras\n",
            "specify\n",
            "lbj\n",
            "classified\n",
            "relying\n",
            "preexisting\n",
            "taxonomies\n",
            "alvarez\n",
            "lim\n",
            "powers\n",
            "heilman\n",
            "relied\n",
            "soon\n",
            "morton\n",
            "kehler\n",
            "andor\n",
            "covers\n",
            "unanswered\n",
            "created\n",
            "reader\n",
            "characters\n",
            "start\n",
            "capitalized\n",
            "developing\n",
            "exploring\n",
            "decades\n",
            "randomly\n",
            "jittered\n",
            "carefully\n",
            "extractive\n",
            "few\n",
            "antonym\n",
            "dsn\n",
            "course\n",
            "mapreduce\n",
            "psycholinguistics\n",
            "testing\n",
            "approximation\n",
            "electronic\n",
            "textual\n",
            "haghighi\n",
            "insist\n",
            "elaborate\n",
            "program\n",
            "midrange\n",
            "hieu\n",
            "philipp\n",
            "abstract\n",
            "act\n",
            "mozer\n",
            "sensitivity\n",
            "their\n",
            "cr\n",
            "negatives\n",
            "indirectly\n",
            "additionally\n",
            "segments\n",
            "supplied\n",
            "ili\n",
            "pwi\n",
            "alternatively\n",
            "characteristic\n",
            "sibling\n",
            "post\n",
            "theories\n",
            "bug\n",
            "shimizu\n",
            "nakagawa\n",
            "inconclusive\n",
            "effectiveness\n",
            "scl\n",
            "unexplored\n",
            "yf\n",
            "tmi\n",
            "nested\n",
            "pt\n",
            "attested\n",
            "lexile\n",
            "chain\n",
            "duan\n",
            "tsujii\n",
            "highest\n",
            "track\n",
            "turns\n",
            "prevent\n",
            "injection\n",
            "cohen\n",
            "np\n",
            "arc\n",
            "indicates\n",
            "entry\n",
            "microsoft\n",
            "companies\n",
            "nasdaq\n",
            "cloud\n",
            "vendors\n",
            "description\n",
            "lehman\n",
            "merged\n",
            "adjusted\n",
            "pro\n",
            "poses\n",
            "derive\n",
            "rence\n",
            "summarizing\n",
            "carter\n",
            "triples\n",
            "quadruples\n",
            "binary\n",
            "area\n",
            "linguists\n",
            "captured\n",
            "fusion\n",
            "chandrasekar\n",
            "doran\n",
            "cutoff\n",
            "sash\n",
            "point\n",
            "relaxed\n",
            "zc\n",
            "nguyen\n",
            "cao\n",
            "gone\n",
            "measuring\n",
            "attempted\n",
            "pearce\n",
            "evert\n",
            "shift\n",
            "finite\n",
            "formalisms\n",
            "permitting\n",
            "polynomial\n",
            "ambiguities\n",
            "jang\n",
            "seven\n",
            "longman\n",
            "outputted\n",
            "minimising\n",
            "uno\n",
            "moreover\n",
            "aronson\n",
            "extends\n",
            "indicators\n",
            "davidov\n",
            "rappoport\n",
            "references\n",
            "xt\n",
            "doug\n",
            "julian\n",
            "jan\n",
            "penelope\n",
            "manifestation\n",
            "weld\n",
            "ratios\n",
            "wyner\n",
            "nearly\n",
            "formulates\n",
            "factoring\n",
            "neill\n",
            "attracts\n",
            "unassigned\n",
            "predetermined\n",
            "coherent\n",
            "corruption\n",
            "investigator\n",
            "researcher\n",
            "tile\n",
            "debate\n",
            "paid\n",
            "shannons\n",
            "redundant\n",
            "printed\n",
            "deliberately\n",
            "misspelled\n",
            "synthesizer\n",
            "coyote\n",
            "spelled\n",
            "figures\n",
            "confirms\n",
            "subsequence\n",
            "increase\n",
            "dimensionality\n",
            "complexity\n",
            "concerns\n",
            "blitzer\n",
            "scenario\n",
            "them\n",
            "selected\n",
            "constant\n",
            "grow\n",
            "indefinitely\n",
            "arguments\n",
            "purposes\n",
            "growing\n",
            "jacobs\n",
            "event\n",
            "align\n",
            "anonymous\n",
            "link\n",
            "hours\n",
            "realign\n",
            "clearest\n",
            "improves\n",
            "depth\n",
            "opposite\n",
            "trend\n",
            "suffer\n",
            "originating\n",
            "fragmented\n",
            "blended\n",
            "ifw\n",
            "nativist\n",
            "biases\n",
            "innate\n",
            "ellison\n",
            "affinities\n",
            "terra\n",
            "diab\n",
            "globally\n",
            "optimized\n",
            "searching\n",
            "laso\n",
            "heuristics\n",
            "stolz\n",
            "tannenbaum\n",
            "carstensen\n",
            "marshall\n",
            "beale\n",
            "nie\n",
            "schafer\n",
            "projections\n",
            "hwa\n",
            "collectively\n",
            "keyword\n",
            "relationship\n",
            "wider\n",
            "translate\n",
            "stroppa\n",
            "hubs\n",
            "authorities\n",
            "degraded\n",
            "nearest\n",
            "neighbours\n",
            "token\n",
            "constructing\n",
            "analogies\n",
            "put\n",
            "last\n",
            "adopting\n",
            "terminology\n",
            "keeping\n",
            "hereby\n",
            "regard\n",
            "largely\n",
            "classed\n",
            "antonyms\n",
            "invoke\n",
            "said\n",
            "ruger\n",
            "compiled\n",
            "clauses\n",
            "conjoined\n",
            "machines\n",
            "queried\n",
            "requests\n",
            "predicates\n",
            "join\n",
            "quit\n",
            "guide\n",
            "induct\n",
            "launch\n",
            "participate\n",
            "return\n",
            "sign\n",
            "meet\n",
            "desirable\n",
            "servers\n",
            "heavily\n",
            "corresponds\n",
            "rest\n",
            "golding\n",
            "decide\n",
            "wed\n",
            "paradigm\n",
            "extending\n",
            "adaptor\n",
            "adaptive\n",
            "informative\n",
            "optimizes\n",
            "steedman\n",
            "realization\n",
            "white\n",
            "dialog\n",
            "beavers\n",
            "priming\n",
            "reitter\n",
            "undirected\n",
            "baum\n",
            "sadler\n",
            "maarek\n",
            "completeness\n",
            "popularized\n",
            "avoided\n",
            "aligners\n",
            "union\n",
            "conditioned\n",
            "presence\n",
            "ki\n",
            "kx\n",
            "multiplying\n",
            "yields\n",
            "desired\n",
            "fader\n",
            "objective\n",
            "speakers\n",
            "callan\n",
            "nes\n",
            "exploit\n",
            "gather\n",
            "collecting\n",
            "broader\n",
            "symbolic\n",
            "constituents\n",
            "inversion\n",
            "versus\n",
            "supports\n",
            "nice\n",
            "theoretical\n",
            "fq\n",
            "ffu\n",
            "ffv\n",
            "adequately\n",
            "trigram\n",
            "absolute\n",
            "motivate\n",
            "regularized\n",
            "tle\n",
            "psycholinguistic\n",
            "landauer\n",
            "usage\n",
            "digital\n",
            "reflecting\n",
            "organisation\n",
            "mental\n",
            "mean\n",
            "binomial\n",
            "variational\n",
            "implement\n",
            "preserving\n",
            "guarantees\n",
            "snyder\n",
            "multiplication\n",
            "triangular\n",
            "logarithm\n",
            "raise\n",
            "unified\n",
            "findings\n",
            "somehow\n",
            "eventually\n",
            "believed\n",
            "tensor\n",
            "aaaaa\n",
            "methodologies\n",
            "address\n",
            "o\n",
            "returns\n",
            "sco\n",
            "wj\n",
            "cw\n",
            "encouraging\n",
            "rigid\n",
            "syntax\n",
            "combinators\n",
            "telugu\n",
            "bharati\n",
            "ourselves\n",
            "already\n",
            "avoid\n",
            "maximizing\n",
            "marginalizing\n",
            "sort\n",
            "bottleneck\n",
            "contrastive\n",
            "please\n",
            "dependent\n",
            "builtin\n",
            "guessing\n",
            "schwall\n",
            "edelman\n",
            "walker\n",
            "amsler\n",
            "niwa\n",
            "nitta\n",
            "providing\n",
            "burden\n",
            "feedback\n",
            "harman\n",
            "buckley\n",
            "xu\n",
            "robertson\n",
            "interpolated\n",
            "smoothed\n",
            "trigrams\n",
            "backward\n",
            "accomplished\n",
            "former\n",
            "speed\n",
            "cube\n",
            "mode\n",
            "nagata\n",
            "googles\n",
            "traditionally\n",
            "weblink\n",
            "social\n",
            "lewis\n",
            "handbook\n",
            "soldier\n",
            "gun\n",
            "fragment\n",
            "handbooks\n",
            "soldiers\n",
            "guns\n",
            "schools\n",
            "cer\n",
            "coded\n",
            "worse\n",
            "becomes\n",
            "stc\n",
            "interaction\n",
            "proteins\n",
            "bd\n",
            "be\n",
            "quantities\n",
            "quote\n",
            "fails\n",
            "zd\n",
            "cl\n",
            "biomedical\n",
            "wilbur\n",
            "collapsed\n",
            "exploitation\n",
            "advanced\n",
            "strictly\n",
            "contiguous\n",
            "translational\n",
            "equivalences\n",
            "rivaled\n",
            "discounting\n",
            "averaging\n",
            "successful\n",
            "aimed\n",
            "trying\n",
            "overcome\n",
            "essen\n",
            "approached\n",
            "consists\n",
            "elements\n",
            "unweighted\n",
            "ruleset\n",
            "parameterization\n",
            "something\n",
            "biasing\n",
            "most\n",
            "seeking\n",
            "confusion\n",
            "ayan\n",
            "topics\n",
            "sport\n",
            "education\n",
            "cues\n",
            "differentiating\n",
            "mentions\n",
            "michael\n",
            "jordan\n",
            "basketball\n",
            "player\n",
            "professor\n",
            "development\n",
            "unlikely\n",
            "attracted\n",
            "exclusively\n",
            "reproduce\n",
            "analyzed\n",
            "identified\n",
            "six\n",
            "extraneous\n",
            "transformation\n",
            "paraphrases\n",
            "descriptions\n",
            "increasingly\n",
            "consortium\n",
            "disambiguated\n",
            "namely\n",
            "mention\n",
            "detection\n",
            "md\n",
            "consistent\n",
            "stemming\n",
            "reproduced\n",
            "mathematically\n",
            "pfe\n",
            "hi\n",
            "simultaneously\n",
            "directed\n",
            "if\n",
            "conceived\n",
            "asymmetries\n",
            "arise\n",
            "fruit\n",
            "importance\n",
            "judgment\n",
            "paths\n",
            "exception\n",
            "ours\n",
            "political\n",
            "win\n",
            "posted\n",
            "message\n",
            "board\n",
            "quantify\n",
            "stt\n",
            "compares\n",
            "ensures\n",
            "validity\n",
            "walks\n",
            "logical\n",
            "greedy\n",
            "obvious\n",
            "orders\n",
            "biggest\n",
            "smallest\n",
            "slovene\n",
            "scholz\n",
            "yamada\n",
            "lerman\n",
            "assuming\n",
            "twice\n",
            "effort\n",
            "integrate\n",
            "correcting\n",
            "students\n",
            "asked\n",
            "extractor\n",
            "beal\n",
            "ci\n",
            "fv\n",
            "gv\n",
            "ifv\n",
            "ow\n",
            "maynard\n",
            "schwenk\n",
            "tense\n",
            "gender\n",
            "meme\n",
            "sil\n",
            "la\n",
            "pas\n",
            "methodology\n",
            "assocation\n",
            "bed\n",
            "produced\n",
            "vogel\n",
            "define\n",
            "formulas\n",
            "pf\n",
            "pe\n",
            "iat\n",
            "bat\n",
            "forat\n",
            "exercise\n",
            "contemporary\n",
            "ayuso\n",
            "norm\n",
            "inductive\n",
            "fully\n",
            "handwriting\n",
            "atc\n",
            "ccl\n",
            "itri\n",
            "databases\n",
            "franz\n",
            "computation\n",
            "implementations\n",
            "sri\n",
            "toolkit\n",
            "federico\n",
            "portage\n",
            "badr\n",
            "besides\n",
            "balanced\n",
            "tune\n",
            "rpc\n",
            "interpreted\n",
            "conclusion\n",
            "shallow\n",
            "phrasal\n",
            "miller\n",
            "sections\n",
            "distinguishes\n",
            "formal\n",
            "going\n",
            "underpinnings\n",
            "doubly\n",
            "nite\n",
            "informed\n",
            "birch\n",
            "hassan\n",
            "reformulation\n",
            "shed\n",
            "light\n",
            "predictability\n",
            "unix\n",
            "command\n",
            "user\n",
            "enter\n",
            "yoshida\n",
            "davison\n",
            "hirsch\n",
            "keystrokes\n",
            "device\n",
            "pda\n",
            "darragh\n",
            "witten\n",
            "translator\n",
            "foreign\n",
            "ssl\n",
            "combines\n",
            "pdi\n",
            "baron\n",
            "freedman\n",
            "ps\n",
            "talukdar\n",
            "nadeau\n",
            "congress\n",
            "italian\n",
            "intelligence\n",
            "palermo\n",
            "computers\n",
            "watson\n",
            "aquisition\n",
            "frames\n",
            "acl\n",
            "vol\n",
            "influence\n",
            "neighbour\n",
            "represented\n",
            "assessments\n",
            "productions\n",
            "subcategory\n",
            "eight\n",
            "combinations\n",
            "topically\n",
            "caraballo\n",
            "thelen\n",
            "zx\n",
            "central\n",
            "segment\n",
            "underlies\n",
            "expression\n",
            "adds\n",
            "have\n",
            "dealing\n",
            "confronted\n",
            "insufficient\n",
            "robustness\n",
            "adaptable\n",
            "dissimilarity\n",
            "skew\n",
            "tendency\n",
            "wash\n",
            "smear\n",
            "defocus\n",
            "exhibited\n",
            "scales\n",
            "empiricist\n",
            "summarized\n",
            "famous\n",
            "dictum\n",
            "linguist\n",
            "jr\n",
            "shall\n",
            "company\n",
            "keeps\n",
            "identifying\n",
            "schulte\n",
            "im\n",
            "walde\n",
            "abe\n",
            "layered\n",
            "creates\n",
            "reorders\n",
            "pq\n",
            "approaching\n",
            "agree\n",
            "law\n",
            "prominent\n",
            "individual\n",
            "dates\n",
            "berger\n",
            "multiclass\n",
            "sharpen\n",
            "substrings\n",
            "exclude\n",
            "infrequent\n",
            "acquires\n",
            "predominance\n",
            "cellulose\n",
            "publication\n",
            "medium\n",
            "writing\n",
            "scientific\n",
            "publishing\n",
            "firm\n",
            "physical\n",
            "tanaka\n",
            "magnini\n",
            "positions\n",
            "nearby\n",
            "misses\n",
            "sleep\n",
            "sit\n",
            "vast\n",
            "convert\n",
            "scalar\n",
            "kl\n",
            "challenge\n",
            "failed\n",
            "meaningful\n",
            "gains\n",
            "occurring\n",
            "intuitions\n",
            "centering\n",
            "theory\n",
            "grosz\n",
            "morris\n",
            "confirming\n",
            "retrieve\n",
            "mto\n",
            "sa\n",
            "wl\n",
            "iwl\n",
            "fw\n",
            "poorer\n",
            "absence\n",
            "weak\n",
            "supervision\n",
            "encoded\n",
            "specialized\n",
            "implicit\n",
            "because\n",
            "accepted\n",
            "shortening\n",
            "fusing\n",
            "jeopardizes\n",
            "xiong\n",
            "hard\n",
            "simply\n",
            "prohibits\n",
            "violate\n",
            "infrastructure\n",
            "modifications\n",
            "dening\n",
            "ic\n",
            "stable\n",
            "contrasted\n",
            "stories\n",
            "typed\n",
            "splitting\n",
            "planning\n",
            "aggregation\n",
            "dimensional\n",
            "svd\n",
            "group\n",
            "agglomeration\n",
            "discusses\n",
            "introducing\n",
            "nondeterministic\n",
            "mechanism\n",
            "re\n",
            "partitioned\n",
            "ww\n",
            "position\n",
            "objects\n",
            "professionally\n",
            "abstracts\n",
            "operation\n",
            "rooted\n",
            "forest\n",
            "computations\n",
            "partially\n",
            "removed\n",
            "notice\n",
            "bounded\n",
            "mcarthur\n",
            "mei\n",
            "icsi\n",
            "meeting\n",
            "switchboard\n",
            "eval\n",
            "ppl\n",
            "whittaker\n",
            "woodland\n",
            "ts\n",
            "abounds\n",
            "reverse\n",
            "giuseppe\n",
            "felice\n",
            "closeness\n",
            "dasgupta\n",
            "deeper\n",
            "constituency\n",
            "largescale\n",
            "tokunaga\n",
            "emphasis\n",
            "components\n",
            "widespread\n",
            "availability\n",
            "na\n",
            "sup\n",
            "pavel\n",
            "deese\n",
            "conduct\n",
            "analyses\n",
            "measurements\n",
            "convictions\n",
            "evoke\n",
            "reminiscent\n",
            "detect\n",
            "regularities\n",
            "support\n",
            "idioms\n",
            "kick\n",
            "bucket\n",
            "aer\n",
            "karlsson\n",
            "anttila\n",
            "till\n",
            "writers\n",
            "subjectivity\n",
            "jindal\n",
            "ravin\n",
            "structural\n",
            "jenson\n",
            "delimiters\n",
            "trellis\n",
            "soong\n",
            "luang\n",
            "coordination\n",
            "recognizing\n",
            "proposes\n",
            "grosser\n",
            "banko\n",
            "zhou\n",
            "offered\n",
            "schiffman\n",
            "regeneration\n",
            "challange\n",
            "finch\n",
            "rijsbergen\n",
            "willett\n",
            "effectively\n",
            "reasonable\n",
            "fishers\n",
            "seed\n",
            "recognised\n",
            "service\n",
            "reliable\n",
            "trusted\n",
            "classify\n",
            "degrades\n",
            "rapidly\n",
            "investigated\n",
            "before\n",
            "enhancing\n",
            "coherence\n",
            "ontologies\n",
            "ass\n",
            "iverson\n",
            "fass\n",
            "kinds\n",
            "jos\n",
            "ilp\n",
            "remove\n",
            "redundancy\n",
            "recursive\n",
            "dp\n",
            "treated\n",
            "marginalized\n",
            "suggestions\n",
            "lexicographer\n",
            "dundee\n",
            "todman\n",
            "alm\n",
            "volume\n",
            "greetings\n",
            "narratives\n",
            "beneficial\n",
            "aac\n",
            "formally\n",
            "littman\n",
            "orientation\n",
            "tis\n",
            "tjs\n",
            "polar\n",
            "prototypical\n",
            "template\n",
            "recourse\n",
            "identical\n",
            "lesser\n",
            "dutch\n",
            "heylen\n",
            "initialize\n",
            "emission\n",
            "gives\n",
            "indirect\n",
            "affects\n",
            "cf\n",
            "flat\n",
            "motivated\n",
            "nontrivial\n",
            "basic\n",
            "subtree\n",
            "investigate\n",
            "paraphrase\n",
            "lsa\n",
            "lack\n",
            "arabic\n",
            "compatible\n",
            "sakhr\n",
            "tokenization\n",
            "when\n",
            "optimizing\n",
            "criterion\n",
            "lus\n",
            "isa\n",
            "hierarchy\n",
            "sum\n",
            "kirchhoff\n",
            "minus\n",
            "row\n",
            "column\n",
            "relaxation\n",
            "nonstandard\n",
            "nastase\n",
            "sumida\n",
            "implies\n",
            "codes\n",
            "commitment\n",
            "identifies\n",
            "title\n",
            "littered\n",
            "transducer\n",
            "away\n",
            "separate\n",
            "claws\n",
            "importantly\n",
            "facilitates\n",
            "incorporation\n",
            "assigning\n",
            "called\n",
            "inflection\n",
            "folding\n",
            "end\n",
            "sliding\n",
            "sarkar\n",
            "mathematical\n",
            "understanding\n",
            "follows\n",
            "majority\n",
            "belonging\n",
            "decomposes\n",
            "quite\n",
            "helpful\n",
            "styles\n",
            "kuhlmann\n",
            "mohl\n",
            "hindi\n",
            "flexible\n",
            "generalisation\n",
            "palo\n",
            "alto\n",
            "distortion\n",
            "covered\n",
            "ee\n",
            "remedy\n",
            "aggressively\n",
            "fraction\n",
            "reduces\n",
            "predicting\n",
            "aic\n",
            "aicc\n",
            "akaike\n",
            "tsai\n",
            "lebreton\n",
            "mixture\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.layers import LSTM, Activation, Dropout, Dense, Input, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Model\n",
        "import string\n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "9I6GWlwgILND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create model architecture\n",
        "def imdb_rating(input_shape):\n",
        "\n",
        "  X_indices = Input(input_shape)\n",
        "\n",
        "  embeddings = embedding_layer(X_indices)\n",
        "\n",
        "  X = LSTM(128, return_sequences=True)(embeddings)\n",
        "\n",
        "  X = Dropout(0.6)(X)\n",
        "\n",
        "  X = LSTM(128, return_sequences=True)(X)\n",
        "\n",
        "  X = Dropout(0.6)(X)\n",
        "\n",
        "  X = LSTM(128)(X)\n",
        "\n",
        "  #X = Dense(1, activation='sigmoid')(X)\n",
        "\n",
        "  X = Dense(3, activation='softmax')(X)\n",
        "\n",
        "  model = Model(inputs=X_indices, outputs=X)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "JDA-0ehuILND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer converts words in the sentence into its corresponding indexe \n",
        "print(X_train.iloc[-1])  \n",
        "print(tokenizer.texts_to_sequences(X_train)[-1])\n",
        "print('the word since, the first word of the last sentence in the dataset has index: ', words_to_index['regardless'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf67294b-73f9-4bfa-a315-a0faf9c68be2",
        "id": "2D90bAcAILNE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We used pointwise mutual information PMI Church Hanks  obtain distances\n",
            "[41, 4, 99, 23, 5, 467, 3, 9, 1228, 1645]\n",
            "the word since, the first word of the last sentence in the dataset has index:  1976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert training data sentence words into its corresponding indexes\n",
        "X_train_indices = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "#pad sentences of indexes so all same length\n",
        "X_train_indices = pad_sequences(X_train_indices, maxlen=maxLen, padding='post')\n",
        "print('the shape of the new training data is 796 samples, each with 835 words (bc of padding): ', X_train_indices.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c7aa90-f6db-428e-b21b-a3243ef65a0c",
        "id": "HvAfR1D5ILNE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the shape of the new training data is 796 samples, each with 835 words (bc of padding):  (2150, 835)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train the model\n",
        "model = imdb_rating((maxLen,))\n",
        "\n",
        "model.compile(optimizer=\"adam\", \n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.AUC()]) #try out new loss may not even need focal loss\n",
        "\n",
        "model.fit(X_train_indices, Y_train, batch_size=256, epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8783c5e4-255c-45f2-bf30-cb06402966c3",
        "id": "-RBRdqh-ILNE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "9/9 [==============================] - 16s 316ms/step - loss: 0.6732 - accuracy: 0.3442\n",
            "Epoch 2/15\n",
            "9/9 [==============================] - 2s 257ms/step - loss: 0.6425 - accuracy: 0.3288\n",
            "Epoch 3/15\n",
            "9/9 [==============================] - 2s 275ms/step - loss: 0.6394 - accuracy: 0.3033\n",
            "Epoch 4/15\n",
            "9/9 [==============================] - 2s 274ms/step - loss: 0.6372 - accuracy: 0.3437\n",
            "Epoch 5/15\n",
            "9/9 [==============================] - 2s 216ms/step - loss: 0.6366 - accuracy: 0.3433\n",
            "Epoch 6/15\n",
            "9/9 [==============================] - 2s 218ms/step - loss: 0.6368 - accuracy: 0.3177\n",
            "Epoch 7/15\n",
            "9/9 [==============================] - 2s 263ms/step - loss: 0.6363 - accuracy: 0.3428\n",
            "Epoch 8/15\n",
            "9/9 [==============================] - 3s 313ms/step - loss: 0.6365 - accuracy: 0.3419\n",
            "Epoch 9/15\n",
            "9/9 [==============================] - 2s 216ms/step - loss: 0.6367 - accuracy: 0.3437\n",
            "Epoch 10/15\n",
            "9/9 [==============================] - 2s 218ms/step - loss: 0.6369 - accuracy: 0.3442\n",
            "Epoch 11/15\n",
            "9/9 [==============================] - 2s 219ms/step - loss: 0.6364 - accuracy: 0.3442\n",
            "Epoch 12/15\n",
            "9/9 [==============================] - 2s 218ms/step - loss: 0.6365 - accuracy: 0.3423\n",
            "Epoch 13/15\n",
            "9/9 [==============================] - 2s 219ms/step - loss: 0.6366 - accuracy: 0.3442\n",
            "Epoch 14/15\n",
            "9/9 [==============================] - 2s 220ms/step - loss: 0.6365 - accuracy: 0.3423\n",
            "Epoch 15/15\n",
            "9/9 [==============================] - 2s 218ms/step - loss: 0.6365 - accuracy: 0.3502\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd2a0147950>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the testing data sentence words into its corresponding indexes\n",
        "X_test_indices = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "#pad sentences of indexes so all same length\n",
        "X_test_indices = pad_sequences(X_test_indices, maxlen=maxLen, padding='post')\n",
        "print('the shape of the new training data is 200 samples, each with 835 words (bc of padding): ', X_test_indices.shape)\n",
        "\n",
        "\n",
        "model.evaluate(X_test_indices, Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e519379-28f7-4654-9a3b-b2aeeb9cd937",
        "id": "IP_I7Z2xILNF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the shape of the new training data is 200 samples, each with 835 words (bc of padding):  (538, 835)\n",
            "17/17 [==============================] - 2s 54ms/step - loss: 0.6376 - accuracy: 0.2900\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6375555992126465, 0.2899628281593323]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_predictions = model.predict(X_test_indices)"
      ],
      "metadata": {
        "id": "lB2oQULqILNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_predictions[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "486ad5ed-b3db-453b-8d13-86c77ea79df3",
        "id": "7xpVE26aILNF"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.34169406, 0.32020408, 0.33810192],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.3416941 , 0.32020405, 0.3381019 ],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020405, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.3202041 , 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.3202041 , 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.3202041 , 0.33810186],\n",
              "       [0.3416941 , 0.32020405, 0.3381019 ],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810192],\n",
              "       [0.34169406, 0.3202041 , 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.3202041 , 0.33810192],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.3416941 , 0.32020405, 0.3381019 ],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020405, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810192],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020405, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.3416941 , 0.32020405, 0.3381019 ],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020405, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.3202041 , 0.33810186],\n",
              "       [0.34169406, 0.3202041 , 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.3202041 , 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020405, 0.33810186],\n",
              "       [0.34169406, 0.3202041 , 0.33810186],\n",
              "       [0.3416941 , 0.32020405, 0.3381019 ],\n",
              "       [0.34169406, 0.32020405, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.3202041 , 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020405, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020405, 0.33810186],\n",
              "       [0.34169406, 0.3202041 , 0.33810186],\n",
              "       [0.3416941 , 0.32020405, 0.3381019 ],\n",
              "       [0.34169406, 0.32020408, 0.33810192],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020405, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.3416941 , 0.32020405, 0.3381019 ],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020405, 0.33810186],\n",
              "       [0.3416941 , 0.32020405, 0.3381019 ],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.3416941 , 0.32020405, 0.3381019 ],\n",
              "       [0.34169406, 0.3202041 , 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810192],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.3202041 , 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.3202041 , 0.33810192],\n",
              "       [0.3416941 , 0.32020405, 0.3381019 ],\n",
              "       [0.3416941 , 0.32020405, 0.3381019 ],\n",
              "       [0.34169406, 0.32020405, 0.33810186],\n",
              "       [0.3416941 , 0.32020405, 0.3381019 ],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.3416941 , 0.32020405, 0.3381019 ],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.3416941 , 0.32020405, 0.3381019 ],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.3202041 , 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810186],\n",
              "       [0.34169406, 0.32020408, 0.33810192],\n",
              "       [0.3416941 , 0.32020405, 0.3381019 ],\n",
              "       [0.3416941 , 0.32020405, 0.3381019 ],\n",
              "       [0.34169406, 0.32020408, 0.33810186]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#take max of the softmax output\n",
        "my_predictions_final = np.argmax(my_predictions, axis=1)\n",
        "\n",
        "#map values back to (-1,0,1)\n",
        "my_predictions_final - 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx0PvSoZJZYU",
        "outputId": "079917b5-28b6-4c1a-dba3-128ef37ab39d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
              "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(Y_test, axis=1)"
      ],
      "metadata": {
        "id": "Vg15f33LJ6L8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df05359-dd00-4c43-bff2-93d0c5248592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 1, 2, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1, 1, 1, 1, 2, 2,\n",
              "       1, 2, 2, 1, 1, 2, 1, 0, 2, 1, 1, 2, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2,\n",
              "       1, 1, 0, 2, 0, 1, 2, 1, 0, 2, 1, 1, 0, 2, 0, 2, 1, 2, 1, 2, 0, 0,\n",
              "       1, 0, 0, 2, 0, 2, 0, 1, 0, 0, 2, 1, 1, 2, 0, 1, 1, 0, 2, 2, 0, 1,\n",
              "       0, 0, 2, 1, 0, 0, 2, 2, 2, 1, 0, 1, 0, 2, 1, 2, 0, 2, 1, 1, 1, 1,\n",
              "       1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 2, 1, 1, 0, 1, 0, 0, 1, 2, 1, 1, 1,\n",
              "       0, 2, 2, 1, 2, 1, 2, 2, 1, 1, 0, 2, 2, 2, 2, 2, 1, 0, 1, 2, 0, 1,\n",
              "       1, 0, 2, 1, 1, 1, 0, 0, 2, 0, 2, 1, 0, 0, 1, 1, 2, 2, 2, 1, 0, 0,\n",
              "       0, 2, 0, 1, 2, 2, 1, 1, 0, 0, 0, 2, 0, 0, 1, 1, 0, 1, 2, 0, 1, 2,\n",
              "       1, 0, 0, 2, 2, 2, 2, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 2, 2, 1, 0,\n",
              "       2, 0, 0, 1, 1, 0, 1, 1, 0, 0, 2, 2, 1, 0, 0, 1, 1, 2, 2, 2, 2, 1,\n",
              "       1, 2, 1, 1, 2, 2, 2, 2, 1, 0, 2, 1, 1, 0, 1, 1, 1, 0, 2, 1, 2, 0,\n",
              "       2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 0, 2, 2, 0, 2, 2, 1, 0,\n",
              "       1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 2, 2, 2, 2, 1, 0, 2, 0, 1, 1, 0, 1,\n",
              "       1, 2, 0, 0, 2, 1, 0, 1, 0, 1, 0, 1, 2, 2, 2, 0, 1, 1, 2, 2, 1, 1,\n",
              "       0, 2, 0, 2, 2, 0, 1, 1, 2, 0, 1, 1, 2, 0, 2, 0, 1, 1, 2, 0, 0, 1,\n",
              "       0, 2, 1, 1, 0, 0, 0, 2, 1, 2, 1, 1, 2, 1, 1, 0, 2, 1, 0, 0, 1, 0,\n",
              "       1, 1, 2, 2, 0, 0, 0, 1, 2, 2, 1, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0,\n",
              "       2, 0, 2, 1, 0, 2, 1, 0, 0, 1, 1, 2, 0, 2, 2, 2, 2, 1, 1, 1, 2, 1,\n",
              "       2, 1, 1, 2, 1, 0, 2, 0, 2, 2, 2, 2, 0, 2, 0, 2, 1, 1, 2, 1, 0, 0,\n",
              "       0, 2, 2, 0, 1, 2, 0, 0, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2,\n",
              "       0, 2, 0, 1, 2, 0, 2, 0, 2, 2, 1, 2, 0, 2, 1, 2, 2, 2, 0, 2, 2, 1,\n",
              "       1, 1, 0, 1, 1, 1, 1, 2, 2, 1, 2, 0, 2, 2, 2, 1, 2, 2, 0, 0, 2, 0,\n",
              "       1, 0, 0, 1, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 1, 2, 2,\n",
              "       1, 1, 0, 1, 0, 1, 2, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}