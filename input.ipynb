{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h7Lr7k5d1jd"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/ENEE439d-TEXTML/TextML/blob/master/input.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/ENEE439d-TEXTML/TextML/blob/master/input.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPiicLOUd1jj"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \"tensorflow-text==2.8.*\" # A dependency of the preprocessing for BERT inputs\n",
        "!pip install -q tf-models-official==2.7.0 # For adamW\n",
        "!pip install focal-loss # focal loss implmnetion for tf\n",
        "!pip install pdfminer.six #pdf text extratction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52FpqTzrPr-t",
        "outputId": "5e566534-6316-47d2-ac5e-f6dbb84d5d9f"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: focal-loss in /usr/local/lib/python3.7/dist-packages (0.0.7)\n",
            "Requirement already satisfied: tensorflow>=2.2 in /usr/local/lib/python3.7/dist-packages (from focal-loss) (2.8.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (14.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (4.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.14.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.21.6)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.44.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.25.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.5.3)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (57.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2->focal-loss) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.2->focal-loss) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (1.35.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (3.2.0)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (20220506)\n",
            "Requirement already satisfied: cryptography~=36.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (36.0.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (2.0.12)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography~=36.0.0->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography~=36.0.0->pdfminer.six) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "BPnv0Vlcd3KI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd #basic imports\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split # https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization, bert  # to create AdamW optimizer\n",
        "from focal_loss import SparseCategoricalFocalLoss\n",
        "import official.nlp.bert.tokenization"
      ],
      "metadata": {
        "id": "dkxbtcKbP4AU"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfparser import PDFParser"
      ],
      "metadata": {
        "id": "Xx7DXy_KYgAE"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data read in"
      ],
      "metadata": {
        "id": "Q-Dc1EsOuQA0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "465BADPxeNII",
        "outputId": "5d78c9a1-bf44-46b2-d840-18aacd1bef7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Sk1zTIA6eOM5",
        "outputId": "6eb70d12-e2df-4c85-d985-2f033c18b846"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  cited_paper  label                                               text\n",
              "0    A00-2024      0  We analyzed a set of articles and identified s...\n",
              "1    A00-2024      0  Table 3: Example compressions Compression AvgL...\n",
              "2    A00-2024      0  5.3 Related works and discussion Our two-step ...\n",
              "3    A00-2024      0  (1999) proposed a summarization system based o...\n",
              "4    A00-2024      0  We found that the deletion of lead parts did n..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-082aff95-0166-4a04-8c36-7d9df1b7419f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cited_paper</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We analyzed a set of articles and identified s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>Table 3: Example compressions Compression AvgL...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>5.3 Related works and discussion Our two-step ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>(1999) proposed a summarization system based o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We found that the deletion of lead parts did n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-082aff95-0166-4a04-8c36-7d9df1b7419f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-082aff95-0166-4a04-8c36-7d9df1b7419f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-082aff95-0166-4a04-8c36-7d9df1b7419f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ],
      "source": [
        "filepath = '/content/drive/MyDrive/Text-ML/full_sentiment_dataset.csv' #'data.csv'\n",
        "df= pd.read_csv(filepath)\n",
        "df1=df.drop(['no','paper','context_a','context_b'],axis=1)\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImSOYF6Py2J8",
        "outputId": "be2d3b91-6b1b-4710-cfec-d2b0f925f131"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 0    7627\n",
              " 1     829\n",
              "-1     280\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtering by regex"
      ],
      "metadata": {
        "id": "FHIvNaLfuUoe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ytc03_FI-b5L",
        "outputId": "99cf582a-5207-4cc7-c6fd-53e17a4a2666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\[*\\]|\\(\\d{4}\\)|\\(?((\\(?((\\w+, )*(\\w+ )+)((and|&) ((\\w+ *))?)?,? \\(?\\d{4}\\)?|((\\w+, )*(\\w+ )+)et al\\. ?,? \\(?\\d{4}\\)?\\)?)((; )|( (and|&) ))*)+\n"
          ]
        }
      ],
      "source": [
        "context=df1['text']\n",
        "\n",
        "re1= \"\\(((([A-Za-z]+ *)+(, \\d+))+(; )*)+\\)\" # matches author and author, year\n",
        "re_year=\",? \\(?\\d{4}\\)?\" # match , {4 digits} which may be wrapped in () \n",
        "re_and=\"(and|&) \"\n",
        "re_auth=\"((\\w+, )*(\\w+ )+)\"\n",
        "re_et= re_auth+\"et al\\. ?\"+re_year # matches author et al. , year\n",
        "re_2a= re_auth+\"(\"+re_and+\"((\\w+ *))?)?\"+re_year # matches author and author, year\n",
        "re_sep=\"((; )|( \"+re_and+\"))*\"# match the '; ' gap or ' and ' gap\n",
        "re_para_year=\"\\(\\d{4}\\)\"\n",
        "re_in_brack=\"\\[*\\]\"\n",
        "re_apa =re_in_brack+\"|\"+re_para_year+\"|\"+\"\\(?(\"+\"(\\(?\"+re_2a+\"|\"+re_et+\"\\)?)\"+re_sep+\")+\"\n",
        "print(re_apa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "NEZIAWQtykDg"
      },
      "outputs": [],
      "source": [
        "def remove_matches(text,regex=re_apa):\n",
        "  text1=text\n",
        "  rem_len=0\n",
        "  pattern= re.compile(regex)\n",
        "  while True:\n",
        "    matches=pattern.search(text1)\n",
        "    #print(matches)\n",
        "    if matches == None:\n",
        "      break\n",
        "\n",
        "    spn=matches.span()\n",
        "    text1=text1[0:spn[0]]+text1[spn[1]:-1]\n",
        "    cit_len=spn[1]-spn[0]\n",
        "    rem_len+=cit_len\n",
        "  \n",
        "  if len(text) >0:\n",
        "    percent_removed=rem_len/len(text)\n",
        "  else:\n",
        "    percent_removed=1\n",
        "  return text1,percent_removed \n",
        "\n",
        "# print(context[5])\n",
        "# remove_citation(context[5],regex=re_apa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "TmjjH1gp9A6T"
      },
      "outputs": [],
      "source": [
        "output=df1['text'].apply(lambda x: remove_matches(text=x,regex=re_apa)) #df['col1'] = df.apply(lambda x: complex_function(x['col1']), axis=1)\n",
        "df_o = pd.DataFrame(list(output), columns =['clean','p_rem'])\n",
        "output_1=df_o['clean'].apply(lambda x: remove_matches(text=x,regex='[^\\w_0-9 ]+')) \n",
        "df_o_1 = pd.DataFrame(list(output_1), columns =['clean','p_rem'])\n",
        "#df_o.head()\n",
        "\n",
        "df1['text_clean']=df_o_1['clean']\n",
        "df1['text_clean_len']=df_o_1['clean'].apply(len)\n",
        "df1['p_rem']=df_o['p_rem']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "OVt3NtaJDDrx",
        "outputId": "0c380576-919b-45ec-e535-a9adf96a72bd"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     cited_paper  label                                               text  \\\n",
              "0       A00-2024      0  We analyzed a set of articles and identified s...   \n",
              "1       A00-2024      0  Table 3: Example compressions Compression AvgL...   \n",
              "2       A00-2024      0  5.3 Related works and discussion Our two-step ...   \n",
              "3       A00-2024      0  (1999) proposed a summarization system based o...   \n",
              "4       A00-2024      0  We found that the deletion of lead parts did n...   \n",
              "...          ...    ...                                                ...   \n",
              "8731    W96-0213      1  He has achieved state-of-the art results by ap...   \n",
              "8732    W96-0213      0  B = (Brill and Wu, 1998); M = (Magerman, 1995)...   \n",
              "8733    W96-0213      0  The model we use is similar to that of (Ratnap...   \n",
              "8734    W96-0213      1  Our model exploits the same kind of tag-n-gram...   \n",
              "8735    W96-0213      0  In that table, TBL stands for Brill's transfor...   \n",
              "\n",
              "                                             text_clean  text_clean_len  \\\n",
              "0     We analyzed a set of articles and identified s...             425   \n",
              "1     Table 3 Example compressions Compression AvgLe...             229   \n",
              "2     53 Related works and discussion Our twostep mo...             105   \n",
              "3      proposed a summarization system based on the ...             321   \n",
              "4     We found that the deletion of lead parts did n...              73   \n",
              "...                                                 ...             ...   \n",
              "8731  He has achieved stateofthe art results by appl...             139   \n",
              "8732   B  M  Magerman 1995 O  our data R  Ratnaparkhi 1              48   \n",
              "8733  The model we use is similar to that of Ratnapa...              55   \n",
              "8734  Our model exploits the same kind of tagngram i...             157   \n",
              "8735  In that table TBL stands for Brills transforma...             288   \n",
              "\n",
              "         p_rem  \n",
              "0     0.098765  \n",
              "1     0.260745  \n",
              "2     0.308176  \n",
              "3     0.078804  \n",
              "4     0.408000  \n",
              "...        ...  \n",
              "8731  0.151515  \n",
              "8732  0.421488  \n",
              "8733  0.000000  \n",
              "8734  0.000000  \n",
              "8735  0.000000  \n",
              "\n",
              "[8736 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2b56ce6a-4e8f-4959-bd3c-cdccda157d96\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cited_paper</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>text_clean_len</th>\n",
              "      <th>p_rem</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We analyzed a set of articles and identified s...</td>\n",
              "      <td>We analyzed a set of articles and identified s...</td>\n",
              "      <td>425</td>\n",
              "      <td>0.098765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>Table 3: Example compressions Compression AvgL...</td>\n",
              "      <td>Table 3 Example compressions Compression AvgLe...</td>\n",
              "      <td>229</td>\n",
              "      <td>0.260745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>5.3 Related works and discussion Our two-step ...</td>\n",
              "      <td>53 Related works and discussion Our twostep mo...</td>\n",
              "      <td>105</td>\n",
              "      <td>0.308176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>(1999) proposed a summarization system based o...</td>\n",
              "      <td>proposed a summarization system based on the ...</td>\n",
              "      <td>321</td>\n",
              "      <td>0.078804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We found that the deletion of lead parts did n...</td>\n",
              "      <td>We found that the deletion of lead parts did n...</td>\n",
              "      <td>73</td>\n",
              "      <td>0.408000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8731</th>\n",
              "      <td>W96-0213</td>\n",
              "      <td>1</td>\n",
              "      <td>He has achieved state-of-the art results by ap...</td>\n",
              "      <td>He has achieved stateofthe art results by appl...</td>\n",
              "      <td>139</td>\n",
              "      <td>0.151515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8732</th>\n",
              "      <td>W96-0213</td>\n",
              "      <td>0</td>\n",
              "      <td>B = (Brill and Wu, 1998); M = (Magerman, 1995)...</td>\n",
              "      <td>B  M  Magerman 1995 O  our data R  Ratnaparkhi 1</td>\n",
              "      <td>48</td>\n",
              "      <td>0.421488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8733</th>\n",
              "      <td>W96-0213</td>\n",
              "      <td>0</td>\n",
              "      <td>The model we use is similar to that of (Ratnap...</td>\n",
              "      <td>The model we use is similar to that of Ratnapa...</td>\n",
              "      <td>55</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8734</th>\n",
              "      <td>W96-0213</td>\n",
              "      <td>1</td>\n",
              "      <td>Our model exploits the same kind of tag-n-gram...</td>\n",
              "      <td>Our model exploits the same kind of tagngram i...</td>\n",
              "      <td>157</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8735</th>\n",
              "      <td>W96-0213</td>\n",
              "      <td>0</td>\n",
              "      <td>In that table, TBL stands for Brill's transfor...</td>\n",
              "      <td>In that table TBL stands for Brills transforma...</td>\n",
              "      <td>288</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8736 rows Ã— 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b56ce6a-4e8f-4959-bd3c-cdccda157d96')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2b56ce6a-4e8f-4959-bd3c-cdccda157d96 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2b56ce6a-4e8f-4959-bd3c-cdccda157d96');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove under and over sized samples\n",
        "large samples appear to be poorly written"
      ],
      "metadata": {
        "id": "pkNPWqAHKLxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getMidLen(data,label,labelKey='label',lenKey='text_clean_len',lowMod=1,highMod=1):\n",
        "  df1 =data.loc[data[labelKey] == label]\n",
        "  neu_mean=np.mean(list(df1[lenKey]))\n",
        "  neu_std=np.std(list(df1[lenKey]))\n",
        "  df1_no_high = df1.loc[df1[lenKey] < highMod*(neu_mean +neu_std)]\n",
        "  # print(neu_mean)\n",
        "  # print(neu_std)\n",
        "\n",
        "  while neu_std > neu_mean:\n",
        "    neu_mean=np.mean(list(df1_no_high['text_clean_len']))\n",
        "    neu_std=np.std(list(df1_no_high['text_clean_len']))\n",
        "    # print(neu_mean)\n",
        "    # print(neu_std)\n",
        "    df1_no_high = df1.loc[df1['text_clean_len'] < highMod*(neu_mean +neu_std)]\n",
        "\n",
        "  df1_mid = df1_no_high.loc[df1_no_high['text_clean_len'] > lowMod*(neu_mean -neu_std)]\n",
        "\n",
        "  return df1_mid\n",
        "\n",
        "df2 = df1.loc[df1['p_rem'] < .5] #keep sampels with less than half of it are citation\n",
        "\n",
        "df_neu=getMidLen(df2,0,lowMod=2)\n",
        "df_pos=getMidLen(df2,1,lowMod=1,highMod=2)\n",
        "df_neg=getMidLen(df2,-1,lowMod=1,highMod=2)\n",
        "df3= pd.concat([df_neg,df_neu,df_pos])\n",
        "df3['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq-BsMfleXID",
        "outputId": "6a27635b-e951-4c82-f365-cfa3e65dc898"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 0    2524\n",
              " 1     746\n",
              "-1     246\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def catagorize(data,labelKey='label'):\n",
        "  rows=len(data.index)\n",
        "  onehots=np.zeros((rows,3),dtype=int)\n",
        "  for i,lab in enumerate(data[labelKey]):\n",
        "    onehots[i][lab+1]=1\n",
        "  return onehots\n",
        "\n",
        "hots=catagorize(df3)\n",
        "df3['label_onehot']=list(hots)\n",
        "df3['label_index']=df3['label']+1"
      ],
      "metadata": {
        "id": "VnsKnaB0AU4i"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq= np.array(list(df3['label_index'].value_counts(normalize=True,sort=False)))\n",
        "print(freq)\n",
        "class_ratio= 1/freq\n",
        "class_ratio"
      ],
      "metadata": {
        "id": "Mbn2dX1C2Ixs",
        "outputId": "ff805d42-0f63-4203-fb01-5ee5be951304",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.06996587 0.71786121 0.21217292]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([14.29268293,  1.39302694,  4.71313673])"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Model"
      ],
      "metadata": {
        "id": "QtrXZF4_unOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(list(df3['text_clean']), list(df3['label_index']), test_size=0.2, random_state=42)\n",
        "X_train= [[s] for s in X_train]\n",
        "X_test= [[s] for s in X_test]\n",
        "y_train=[[s] for s in list(y_train)]\n",
        "y_test=[[s] for s in list(y_test)]"
      ],
      "metadata": {
        "id": "kKtJq5Baj1Wl"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose a BERT model to fine-tune (Taken from tutorial)\n",
        "\n",
        "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "metadata": {
        "id": "9WK-J5dQpprm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc687c56-83eb-4e7f-9eb2-68c69642414b"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## check model passes"
      ],
      "metadata": {
        "id": "91uobQaIu-iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ],
      "metadata": {
        "id": "YINTv-Uu8HP5"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_test = X_train[1]\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
        "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
        "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGAgJwGg08-j",
        "outputId": "749f90f6-1fc2-413f-c9a4-7000e8e91afe"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys       : ['input_mask', 'input_word_ids', 'input_type_ids']\n",
            "Shape      : (1, 128)\n",
            "Word Ids   : [  101  1999  5688 11416  6024  4275  2024  4738  2000 25845  1996  4101]\n",
            "Input Mask : [1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "metadata": {
        "id": "U2o1JW9b9MHu"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
        "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
        "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
        "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
        "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW0iEt6Z9PKt",
        "outputId": "e49abc80-9db0-463c-c2bb-2569c37fde1b"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Pooled Outputs Shape:(1, 512)\n",
            "Pooled Outputs Values:[ 0.99835694 -0.7678578  -0.21300124  0.08411987 -0.08593949  0.98550844\n",
            "  0.9732349  -0.8306031  -0.55687106 -0.95725054 -0.3960305  -0.94115573]\n",
            "Sequence Outputs Shape:(1, 128, 512)\n",
            "Sequence Outputs Values:[[ 3.8915187e-01  2.3267061e-01  6.1780311e-02 ... -9.2866874e-01\n",
            "   4.3027106e-01  8.3279318e-01]\n",
            " [ 4.5310837e-01  7.2308904e-01 -3.2866317e-01 ... -1.1163950e-04\n",
            "  -3.3482751e-01  5.7274055e-01]\n",
            " [-2.5876865e-01  1.4199525e+00 -4.2525381e-01 ...  4.9038833e-01\n",
            "   1.4491324e-01  5.7048750e-01]\n",
            " ...\n",
            " [ 2.6529512e-01 -2.3668993e-01  8.4921330e-02 ...  2.0211086e-02\n",
            "   3.9103544e-01  8.9449620e-01]\n",
            " [ 2.9499257e-01  5.8424294e-01 -6.7344594e-01 ... -1.6148384e+00\n",
            "   1.3211843e+00 -6.0517174e-01]\n",
            " [-1.8697177e-01  5.2527428e-01  9.2377967e-01 ... -5.6088662e-01\n",
            "   1.0293359e+00 -7.8856331e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## full model setup"
      ],
      "metadata": {
        "id": "GSyS6XhXvGXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(3, activation='softmax', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "metadata": {
        "id": "XWtmKUBu_3kJ"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model = build_classifier_model()"
      ],
      "metadata": {
        "id": "Z17Cu4Awc3jA"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## check loss function"
      ],
      "metadata": {
        "id": "KgOUUHifvD3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_raw_result = classifier_model(tf.constant(text_test))\n",
        "print(bert_raw_result)\n",
        "\n",
        "l =  SparseCategoricalFocalLoss(gamma=2,class_weight=class_ratio)\n",
        "test =tf.convert_to_tensor([1.0])\n",
        "l(test,bert_raw_result)"
      ],
      "metadata": {
        "id": "jHqpunBDTKym",
        "outputId": "4117fdc7-ece9-4c13-ccf8-099c3c53c804",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[0.09427555 0.03744136 0.8682831 ]], shape=(1, 3), dtype=float32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=4.2398114>"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Save and Log"
      ],
      "metadata": {
        "id": "hTEGlj9RvLg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "steps_per_epoch = 200 #tf.data.experimental.cardinality(X_train).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 2e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "# def auc_wrapper(y_true,y_pred):\n",
        "#   print(y_true,y_pred)\n",
        "\n",
        "#   y_true=tf.reshape(y_true,[1])\n",
        "#   print(y_true)\n",
        "#   y_true= tf.cast(y_true, tf.int32)\n",
        "#   print(y_true)\n",
        "#   y_true=tf.one_hot(y_true,depth=3)\n",
        "#   print(y_true)\n",
        "#   return tf.keras.metrics.AUC(multi_label=True)(y_true,y_pred)\n",
        "\n",
        "\n",
        "loss =  SparseCategoricalFocalLoss(gamma=2,class_weight=class_ratio) #tf.keras.losses.MeanSquaredError()\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]#, auc_wrapper]#, tf.keras.metrics.AUC(multi_label=True)]\n",
        "\n",
        "\n",
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"citation_BERT_{epoch}\",\n",
        "        save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
        "        monitor=\"val_sparse_categorical_accuracy\",\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.TensorBoard('./logs', update_freq=1)\n",
        "]\n"
      ],
      "metadata": {
        "id": "MB9Af5RMCuqP"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f'Training model with {tfhub_handle_encoder}')\n",
        "# history = classifier_model.fit(x=X_train,y=y_train, validation_data=(X_test,y_test),epochs=epochs,callbacks= callbacks, verbose=True)"
      ],
      "metadata": {
        "id": "89v_3XqEE3HN"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classifier_model.save_weights(''/content/drive/MyDrive/Text-ML/checkpoint1')"
      ],
      "metadata": {
        "id": "Pox0n2xZjdtX"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect model"
      ],
      "metadata": {
        "id": "Z_fseNCGvY4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "00eGEQhDdMUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "998b6294-e6ec-4622-97ca-2a153e1d2214"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%tensorboard --logdir=logs"
      ],
      "metadata": {
        "id": "Gszh9ZNBbQXe"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.load_weights('/content/drive/MyDrive/Text-ML/checkpoint')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMRXTmWvmBNn",
        "outputId": "90c7a6a1-5d05-466d-c3d7-be10509c2f89"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f82acc3f0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preds=classifier_model.predict(X_train,verbose=1)"
      ],
      "metadata": {
        "id": "vCgZJCYsvdMe"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preds_t=classifier_model.predict(X_test,verbose=1)"
      ],
      "metadata": {
        "id": "Wu5a35I1wlOi"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns; sns.set_theme()"
      ],
      "metadata": {
        "id": "Q-NxIjYJx8S1"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# c_mat=tf.math.confusion_matrix(np.argmax(preds,-1),y_train)\n",
        "# ax = sns.heatmap(c_mat,annot=True,linewidths=.5)"
      ],
      "metadata": {
        "id": "_5R4bXyzxoql"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# c_mat=tf.math.confusion_matrix(np.argmax(preds_t,-1),y_test)\n",
        "# ax = sns.heatmap(c_mat,annot=True,linewidths=.5)"
      ],
      "metadata": {
        "id": "RPkpGRdDyCz6"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get attention colorings"
      ],
      "metadata": {
        "id": "ZRca_y_Kvq86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1\n"
      ],
      "metadata": {
        "id": "aSNvnkyoJdqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.layers"
      ],
      "metadata": {
        "id": "PR_gDYEQofck",
        "outputId": "2ef9ab73-d810-4fea-e530-b487141a9ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.engine.input_layer.InputLayer at 0x7f829e584dd0>,\n",
              " <tensorflow_hub.keras_layer.KerasLayer at 0x7f82aa8eee50>,\n",
              " <tensorflow_hub.keras_layer.KerasLayer at 0x7f82aa8ee350>,\n",
              " <keras.layers.core.dropout.Dropout at 0x7f829cafd510>,\n",
              " <keras.layers.core.dense.Dense at 0x7f829cb98b90>]"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from official.nlp import bert \n",
        "import official.nlp.bert.tokenization"
      ],
      "metadata": {
        "id": "bC2uW7zrx7VR"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = bert.tokenization.FullTokenizer(vocab_file='/content/drive/MyDrive/Text-ML/vocab.txt')\n",
        "preprocesser_model = keras.Model(inputs=classifier_model.input,outputs=classifier_model.get_layer('preprocessing').output)\n",
        "encoder_model = keras.Model(inputs=classifier_model.input,outputs=classifier_model.get_layer('BERT_encoder').output)"
      ],
      "metadata": {
        "id": "Lnb_ie7zxpbc"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocab size:\", len(tokenizer.vocab))"
      ],
      "metadata": {
        "id": "mjgJ0WAwRjYb",
        "outputId": "0853e3b5-9817-44ee-83ec-495b8576a281",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 30522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attnetion token mapping"
      ],
      "metadata": {
        "id": "z7DPKDmPI9yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attn(context,prep,encoder): # assume stirng array input\n",
        "  t_context=tf.convert_to_tensor(context)\n",
        "\n",
        "  p_out=prep(t_context)\n",
        "  #print(p_out)\n",
        "  stop_index=0\n",
        "  while(stop_index< p_out[\"input_mask\"].shape[1] and p_out[\"input_mask\"][0][stop_index] == 1):\n",
        "    stop_index+=1\n",
        "  \n",
        "  if stop_index >= 128:\n",
        "    stop_index=127\n",
        "\n",
        "  output = encoder(t_context)\n",
        "  #print(output[\"sequence_output\"].shape)\n",
        "  valid_entries=output[\"sequence_output\"][:,1:stop_index-1,:]\n",
        "  a=tf.math.reduce_mean(valid_entries,-1)\n",
        "  mean=tf.math.reduce_mean(a,-1,keepdims=True)\n",
        "  std=tf.math.reduce_std(a,-1,keepdims=True)\n",
        "  a1=(a-mean)/std\n",
        "\n",
        "  return a1"
      ],
      "metadata": {
        "id": "NC_Cyexu9TQI"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attn_for_words(context,tokenizer,prep,encoder):\n",
        "  attn = get_attn(context,prep,encoder).numpy()\n",
        "  tokens = tokenizer.tokenize(context[0]) \n",
        "\n",
        "  indicies=np.ones((len(tokens)),dtype=int)\n",
        "  for i,tok in enumerate(tokens):\n",
        "    if '##' in tok:\n",
        "      indicies[i]=0\n",
        "\n",
        "  full_words=tokens.copy()\n",
        "  ix=-1\n",
        "  for i,tok in enumerate(tokens):\n",
        "    if not indicies[i]:\n",
        "      attn[0][ix]+=attn[0][i]\n",
        "      full_words[ix]+=tok[2:]\n",
        "    else:\n",
        "      ix=i\n",
        "\n",
        "  t_f=tf.convert_to_tensor(full_words) #stores as byte string...\n",
        "  masked_f=tf.boolean_mask(t_f,indicies)\n",
        "  t_a=tf.convert_to_tensor(attn)[0]\n",
        "  masked_a=tf.boolean_mask(t_a[:len(indicies)],indicies)\n",
        "  \n",
        "\n",
        "  return masked_f.numpy(),masked_a.numpy()\n",
        "\n",
        "#words, at=get_attn_for_words(processed,tokenizer,preprocesser_model,encoder_model)"
      ],
      "metadata": {
        "id": "iyhKVIiY121W"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## converters and annotation"
      ],
      "metadata": {
        "id": "kj_6JUFQIvG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_for_input(raw_context):\n",
        "  c1,_ =remove_matches(text=raw_context,regex=re_apa)\n",
        "  c2,_ =remove_matches(text=c1,regex='[^\\w_\\-0-9 ]+')\n",
        "  return [c2]\n",
        "\n",
        "#process_for_input(example)"
      ],
      "metadata": {
        "id": "PTVxz5uFJZyI"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_bytes_strs(words):\n",
        "  return [w.decode('UTF-8') for w in list(words)]"
      ],
      "metadata": {
        "id": "uFUPhtLDH5SS"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_to_color(attn): #blue pos red neg\n",
        "  rgbs = np.zeros((len(attn),3),dtype=int)\n",
        "  for i,score in enumerate(attn):\n",
        "    if score < 0:\n",
        "      rgbs[i][0]=-255*score//2\n",
        "    else:\n",
        "      rgbs[i][2]=255*score//2\n",
        "  \n",
        "  return rgbs"
      ],
      "metadata": {
        "id": "yDhvZkOFMFJ3"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def coloring(text,fore=None,back=None):\n",
        "    txt=text\n",
        "    if fore != None and fore[0] != -1:\n",
        "      txt = \"\\033[38;2;{};{};{}m\".format(fore[0], fore[1], fore[2])+txt\n",
        "    if back != None and back[0] != -1:\n",
        "      txt = \"\\033[48;2;{};{};{}m\".format(back[0], back[1], back[2])+txt\n",
        "    return txt\n",
        "\n",
        "#print(coloring('Hello',back=[500,0,0]) + coloring('Hello', back=(0,0,255)))"
      ],
      "metadata": {
        "id": "X8GvqhLvOR8H"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## full pipeline"
      ],
      "metadata": {
        "id": "spUdAeqoI0Ve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def color_by_attn(text,toker,preper,encoder):\n",
        "  all_words_original=text.split()\n",
        "  all_words=text.lower().split()\n",
        "  processed= process_for_input(text)\n",
        "\n",
        "  words, at=get_attn_for_words(processed,tokenizer,preprocesser_model,encoder_model)\n",
        "\n",
        "  ws=conv_bytes_strs(words)\n",
        "  conv=conv_to_color(at)\n",
        "  mapping=dict(zip(ws,conv))\n",
        "  orig_mapping=dict(zip(all_words,all_words_original))\n",
        "\n",
        "  for i,w in enumerate(all_words):\n",
        "    if w not in mapping:\n",
        "      mapping[w]=[0,0,0] #make black\n",
        "    else:\n",
        "      mapping[w]=list(mapping[w])\n",
        "\n",
        "  colored=[coloring(orig_mapping[word],fore=[255,255,255],back=mapping[word]) for word in all_words]\n",
        "  printed=' '.join(colored)\n",
        "  return printed\n",
        "\n",
        "example=list(df3['text'])[0]\n",
        "p=color_by_attn(example,tokenizer,preprocesser_model,encoder_model)\n",
        "print(p)\n",
        "p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hossOHKtZ1Wn",
        "outputId": "935958f4-9884-435d-e32d-6a7c7cbb9013"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[48;2;12;0;0m\u001b[38;2;255;255;255mMany \u001b[48;2;0;0;108m\u001b[38;2;255;255;255mapproaches \u001b[48;2;0;0;102m\u001b[38;2;255;255;255mfor \u001b[48;2;140;0;0m\u001b[38;2;255;255;255mPOS \u001b[48;2;103;0;0m\u001b[38;2;255;255;255mtagging \u001b[48;2;0;0;60m\u001b[38;2;255;255;255mhave \u001b[48;2;22;0;0m\u001b[38;2;255;255;255mbeen \u001b[48;2;188;0;0m\u001b[38;2;255;255;255mdeveloped \u001b[48;2;0;0;46m\u001b[38;2;255;255;255min \u001b[48;2;29;0;0m\u001b[38;2;255;255;255mthe \u001b[48;2;0;0;0m\u001b[38;2;255;255;255mpast, \u001b[48;2;0;0;281m\u001b[38;2;255;255;255mincluding \u001b[48;2;0;0;0m\u001b[38;2;255;255;255mrule-based \u001b[48;2;103;0;0m\u001b[38;2;255;255;255mtagging \u001b[48;2;0;0;0m\u001b[38;2;255;255;255m(Brill, \u001b[48;2;0;0;0m\u001b[38;2;255;255;255m1995), \u001b[48;2;9;0;0m\u001b[38;2;255;255;255mHMM \u001b[48;2;316;0;0m\u001b[38;2;255;255;255mtaggers \u001b[48;2;0;0;0m\u001b[38;2;255;255;255m(Brants, \u001b[48;2;0;0;0m\u001b[38;2;255;255;255m2000; \u001b[48;2;0;0;0m\u001b[38;2;255;255;255mCutting \u001b[48;2;0;0;0m\u001b[38;2;255;255;255mand \u001b[48;2;0;0;0m\u001b[38;2;255;255;255mothers, \u001b[48;2;0;0;0m\u001b[38;2;255;255;255m1992), \u001b[48;2;0;0;0m\u001b[38;2;255;255;255mmaximum-entropy \u001b[48;2;0;0;41m\u001b[38;2;255;255;255mmodels \u001b[48;2;0;0;0m\u001b[38;2;255;255;255m(Rathnaparki, \u001b[48;2;0;0;0m\u001b[38;2;255;255;255m1996), \u001b[48;2;0;0;97m\u001b[38;2;255;255;255mcyclic \u001b[48;2;61;0;0m\u001b[38;2;255;255;255mdependency \u001b[48;2;0;0;45m\u001b[38;2;255;255;255mnetworks \u001b[48;2;0;0;0m\u001b[38;2;255;255;255m(Toutanova \u001b[48;2;0;0;0m\u001b[38;2;255;255;255met \u001b[48;2;0;0;0m\u001b[38;2;255;255;255mal. \u001b[48;2;0;0;0m\u001b[38;2;255;255;255m, \u001b[48;2;0;0;0m\u001b[38;2;255;255;255m2003), \u001b[48;2;0;0;0m\u001b[38;2;255;255;255mmemory-based \u001b[48;2;0;0;84m\u001b[38;2;255;255;255mlearning \u001b[48;2;0;0;0m\u001b[38;2;255;255;255m(Daelemans \u001b[48;2;0;0;0m\u001b[38;2;255;255;255met \u001b[48;2;0;0;0m\u001b[38;2;255;255;255mal. \u001b[48;2;0;0;0m\u001b[38;2;255;255;255m, \u001b[48;2;0;0;0m\u001b[38;2;255;255;255m1996), \u001b[48;2;0;0;0m\u001b[38;2;255;255;255metc. \u001b[48;2;0;0;15m\u001b[38;2;255;255;255mall \u001b[48;2;0;0;258m\u001b[38;2;255;255;255mof \u001b[48;2;0;0;186m\u001b[38;2;255;255;255mthese \u001b[48;2;0;0;108m\u001b[38;2;255;255;255mapproaches \u001b[48;2;0;0;110m\u001b[38;2;255;255;255mrequire \u001b[48;2;215;0;0m\u001b[38;2;255;255;255meither \u001b[48;2;0;0;217m\u001b[38;2;255;255;255ma \u001b[48;2;207;0;0m\u001b[38;2;255;255;255mlarge \u001b[48;2;0;0;46m\u001b[38;2;255;255;255mamount \u001b[48;2;0;0;258m\u001b[38;2;255;255;255mof \u001b[48;2;70;0;0m\u001b[38;2;255;255;255mannotated \u001b[48;2;77;0;0m\u001b[38;2;255;255;255mtraining \u001b[48;2;166;0;0m\u001b[38;2;255;255;255mdata \u001b[48;2;0;0;0m\u001b[38;2;255;255;255m(for \u001b[48;2;50;0;0m\u001b[38;2;255;255;255msupervised \u001b[48;2;0;0;0m\u001b[38;2;255;255;255mtagging) \u001b[48;2;0;0;93m\u001b[38;2;255;255;255mor \u001b[48;2;0;0;217m\u001b[38;2;255;255;255ma \u001b[48;2;467;0;0m\u001b[38;2;255;255;255mlexicon \u001b[48;2;0;0;9m\u001b[38;2;255;255;255mlisting \u001b[48;2;0;0;15m\u001b[38;2;255;255;255mall \u001b[48;2;0;0;81m\u001b[38;2;255;255;255mpossible \u001b[48;2;45;0;0m\u001b[38;2;255;255;255mtags \u001b[48;2;0;0;102m\u001b[38;2;255;255;255mfor \u001b[48;2;0;0;114m\u001b[38;2;255;255;255meach \u001b[48;2;117;0;0m\u001b[38;2;255;255;255mword \u001b[48;2;0;0;0m\u001b[38;2;255;255;255m(for \u001b[48;2;0;0;0m\u001b[38;2;255;255;255munsupervised \u001b[48;2;0;0;0m\u001b[38;2;255;255;255mtagging).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\x1b[48;2;12;0;0m\\x1b[38;2;255;255;255mMany \\x1b[48;2;0;0;108m\\x1b[38;2;255;255;255mapproaches \\x1b[48;2;0;0;102m\\x1b[38;2;255;255;255mfor \\x1b[48;2;140;0;0m\\x1b[38;2;255;255;255mPOS \\x1b[48;2;103;0;0m\\x1b[38;2;255;255;255mtagging \\x1b[48;2;0;0;60m\\x1b[38;2;255;255;255mhave \\x1b[48;2;22;0;0m\\x1b[38;2;255;255;255mbeen \\x1b[48;2;188;0;0m\\x1b[38;2;255;255;255mdeveloped \\x1b[48;2;0;0;46m\\x1b[38;2;255;255;255min \\x1b[48;2;29;0;0m\\x1b[38;2;255;255;255mthe \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255mpast, \\x1b[48;2;0;0;281m\\x1b[38;2;255;255;255mincluding \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255mrule-based \\x1b[48;2;103;0;0m\\x1b[38;2;255;255;255mtagging \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255m(Brill, \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255m1995), \\x1b[48;2;9;0;0m\\x1b[38;2;255;255;255mHMM \\x1b[48;2;316;0;0m\\x1b[38;2;255;255;255mtaggers \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255m(Brants, \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255m2000; \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255mCutting \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255mand \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255mothers, \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255m1992), \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255mmaximum-entropy \\x1b[48;2;0;0;41m\\x1b[38;2;255;255;255mmodels \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255m(Rathnaparki, \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255m1996), \\x1b[48;2;0;0;97m\\x1b[38;2;255;255;255mcyclic \\x1b[48;2;61;0;0m\\x1b[38;2;255;255;255mdependency \\x1b[48;2;0;0;45m\\x1b[38;2;255;255;255mnetworks \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255m(Toutanova \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255met \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255mal. \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255m, \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255m2003), \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255mmemory-based \\x1b[48;2;0;0;84m\\x1b[38;2;255;255;255mlearning \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255m(Daelemans \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255met \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255mal. \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255m, \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255m1996), \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255metc. \\x1b[48;2;0;0;15m\\x1b[38;2;255;255;255mall \\x1b[48;2;0;0;258m\\x1b[38;2;255;255;255mof \\x1b[48;2;0;0;186m\\x1b[38;2;255;255;255mthese \\x1b[48;2;0;0;108m\\x1b[38;2;255;255;255mapproaches \\x1b[48;2;0;0;110m\\x1b[38;2;255;255;255mrequire \\x1b[48;2;215;0;0m\\x1b[38;2;255;255;255meither \\x1b[48;2;0;0;217m\\x1b[38;2;255;255;255ma \\x1b[48;2;207;0;0m\\x1b[38;2;255;255;255mlarge \\x1b[48;2;0;0;46m\\x1b[38;2;255;255;255mamount \\x1b[48;2;0;0;258m\\x1b[38;2;255;255;255mof \\x1b[48;2;70;0;0m\\x1b[38;2;255;255;255mannotated \\x1b[48;2;77;0;0m\\x1b[38;2;255;255;255mtraining \\x1b[48;2;166;0;0m\\x1b[38;2;255;255;255mdata \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255m(for \\x1b[48;2;50;0;0m\\x1b[38;2;255;255;255msupervised \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255mtagging) \\x1b[48;2;0;0;93m\\x1b[38;2;255;255;255mor \\x1b[48;2;0;0;217m\\x1b[38;2;255;255;255ma \\x1b[48;2;467;0;0m\\x1b[38;2;255;255;255mlexicon \\x1b[48;2;0;0;9m\\x1b[38;2;255;255;255mlisting \\x1b[48;2;0;0;15m\\x1b[38;2;255;255;255mall \\x1b[48;2;0;0;81m\\x1b[38;2;255;255;255mpossible \\x1b[48;2;45;0;0m\\x1b[38;2;255;255;255mtags \\x1b[48;2;0;0;102m\\x1b[38;2;255;255;255mfor \\x1b[48;2;0;0;114m\\x1b[38;2;255;255;255meach \\x1b[48;2;117;0;0m\\x1b[38;2;255;255;255mword \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255m(for \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255munsupervised \\x1b[48;2;0;0;0m\\x1b[38;2;255;255;255mtagging).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('test')"
      ],
      "metadata": {
        "id": "r3D3FipbOOxP",
        "outputId": "5219b975-d5f8-4f54-f452-087293d2d57d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF text extratction\n"
      ],
      "metadata": {
        "id": "TD_FQ6ioGFGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_pdf_to_string(file_path):\n",
        "\toutput_string = StringIO()\n",
        "\twith open(file_path, 'rb') as in_file:\n",
        "\t    parser = PDFParser(in_file)\n",
        "\t    doc = PDFDocument(parser)\n",
        "\t    rsrcmgr = PDFResourceManager()\n",
        "\t    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
        "\t    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "\t    for page in PDFPage.create_pages(doc):\n",
        "\t        interpreter.process_page(page)\n",
        "\n",
        "\treturn(output_string.getvalue())\n",
        " \n",
        "def sent_extract(sample):\n",
        "\tdelims=re.findall('\\. [A-Z]',sample)\n",
        "\tsents=re.split('\\. [A-Z]',sample)\n",
        "\n",
        "\tsents[0]=sents[0]+'.'\n",
        "\tfor i,s in enumerate(sents[1:]):\n",
        "\t\tsents[i+1]=delims[i][2]+s+'.'\n",
        "\treturn sents\n",
        "\n",
        "def pdf_text_extract(path):\n",
        "  text=convert_pdf_to_string(path)\n",
        "  text1 = text.replace('\\x0c','')\n",
        "  text2 = text1.split('.\\n\\n')\n",
        "  refine=[t.replace('\\n',' ') for t in text2]\n",
        "  r=[]\n",
        "  for t in refine:\n",
        "    r+=sent_extract(t)\n",
        "  return r\n",
        "\n",
        "path='/content/drive/MyDrive/Text-ML/phocus.pdf'\n",
        "text=pdf_text_extract(path)\n"
      ],
      "metadata": {
        "id": "HkUe38wlJSov"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_end=0\n",
        "ref_start=len(text)-1\n",
        "while 'Â©' not in text[title_end]:\n",
        "  title_end+=1\n",
        "while 'REFERENCES' not in text[ref_start]:\n",
        "  ref_start-=1\n",
        "content=text[title_end+3:ref_start]"
      ],
      "metadata": {
        "id": "O9lsV2ij-2vn"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_class(text,model,d=4):\n",
        "  codes=tf.constant([[-1.0,0.0,1.0]])\n",
        "  pred=model.predict(text)[0]\n",
        "  i=tf.argmax(pred)\n",
        "  res=codes*pred\n",
        "  def roundDown(n, d):\n",
        "    d = int('1' + ('0' * d))\n",
        "    return np.floor(np.array(n) * d) / d\n",
        "  pred=roundDown(pred)\n",
        "  score=tf.math.reduce_mean(res,-1)[0].numpy()\n",
        "  classification=codes[0,i].numpy()\n",
        "  max_confidence=pred[i]\n",
        "  return classification,max_confidence,score,list(pred)"
      ],
      "metadata": {
        "id": "Uc5Sg-uKYigY"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.floor(np.array(raw)*100) /100"
      ],
      "metadata": {
        "id": "R4jbdykFqJwJ",
        "outputId": "ef0d33a7-2fc1-405b-915b-cc6f1df98005",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.05, 0.46, 0.48], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coloring of a PDF"
      ],
      "metadata": {
        "id": "_Td1mswaCOFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "errs=[]\n",
        "dfs=[]\n",
        "cols=['sentence','class','confidence','net score','neg','neu','pos','text','colored']\n",
        "for i,t in enumerate(content):\n",
        "  try:\n",
        "    c,conf,sc,raw=text_class([t],classifier_model)\n",
        "    colored=color_by_attn(t,tokenizer,preprocesser_model,encoder_model)\n",
        "    df_t = pd.DataFrame([[i,c,conf,sc,raw[0],raw[1],raw[2],t,colored]],columns=cols)\n",
        "    print(i,c,conf,sc,colored)\n",
        "    dfs.append(df_t)\n",
        "    \n",
        "  except Exception:\n",
        "    errs.append(i)"
      ],
      "metadata": {
        "id": "AcFhtVjoCRTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_phocus=pd.concat(dfs)"
      ],
      "metadata": {
        "id": "fcWMHFQ4iMeP"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_paper_sentiment=np.mean(df_phocus['net score'])\n",
        "overall_paper_sentiment"
      ],
      "metadata": {
        "id": "e786wcE1nYsZ",
        "outputId": "21975d8e-3656-4e47-cf89-beaa56949bda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07737980037927628"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_phocus.sort_values(by=['net score'])"
      ],
      "metadata": {
        "id": "mPGTjJnCnr1k",
        "outputId": "356d867f-6fe6-475f-96de-628c0dedbb31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    sentence  class  confidence  net score       neg       neu       pos  \\\n",
              "0         23   -1.0    0.924179  -0.304831  0.924179  0.066136  0.009686   \n",
              "0        118   -1.0    0.917094  -0.296697  0.917094  0.055904  0.027002   \n",
              "0          2   -1.0    0.764693  -0.225062  0.764693  0.145799  0.089508   \n",
              "0         70   -1.0    0.597995  -0.191424  0.597995  0.378284  0.023721   \n",
              "0        140   -1.0    0.597995  -0.191424  0.597995  0.378284  0.023721   \n",
              "..       ...    ...         ...        ...       ...       ...       ...   \n",
              "0          5    1.0    0.875303   0.290539  0.003685  0.121012  0.875303   \n",
              "0        110    1.0    0.907548   0.299300  0.009649  0.082802  0.907548   \n",
              "0        141    1.0    0.934530   0.303610  0.023699  0.041771  0.934530   \n",
              "0        120    1.0    0.915919   0.303936  0.004112  0.079969  0.915919   \n",
              "0         72    1.0    0.960823   0.311771  0.025509  0.013667  0.960823   \n",
              "\n",
              "                                                 text  \\\n",
              "0   However, this method generates the overall sen...   \n",
              "0   However, we cannot complete this job yet out o...   \n",
              "0   Quantitative metrics could not evaluate the re...   \n",
              "0   Those metrics are derived from citations and d...   \n",
              "0   Those metrics are derived from citations and d...   \n",
              "..                                                ...   \n",
              "0   We propose Phocus, a novel evaluation mechanis...   \n",
              "0                 The main idea is shown in Figure 3.   \n",
              "0   Semantic Scholar makes the first step towards ...   \n",
              "0   Besides, we also compare our modules to other ...   \n",
              "0   Espe- cially, Semantic Scholar makes the first...   \n",
              "\n",
              "                                              colored  \n",
              "0   \u001b[48;2;0;0;0m\u001b[38;2;255;255;255mHowever, \u001b[48;...  \n",
              "0   \u001b[48;2;0;0;0m\u001b[38;2;255;255;255mHowever, \u001b[48;...  \n",
              "0   \u001b[48;2;0;0;89m\u001b[38;2;255;255;255mQuantitative ...  \n",
              "0   \u001b[48;2;0;0;5m\u001b[38;2;255;255;255mThose \u001b[48;2;4...  \n",
              "0   \u001b[48;2;0;0;5m\u001b[38;2;255;255;255mThose \u001b[48;2;4...  \n",
              "..                                                ...  \n",
              "0   \u001b[48;2;253;0;0m\u001b[38;2;255;255;255mWe \u001b[48;2;0;...  \n",
              "0   \u001b[48;2;0;0;85m\u001b[38;2;255;255;255mThe \u001b[48;2;11...  \n",
              "0   \u001b[48;2;72;0;0m\u001b[38;2;255;255;255mSemantic \u001b[48...  \n",
              "0   \u001b[48;2;0;0;0m\u001b[38;2;255;255;255mBesides, \u001b[48;...  \n",
              "0   \u001b[48;2;0;0;0m\u001b[38;2;255;255;255mEspe- \u001b[48;2;0...  \n",
              "\n",
              "[174 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3713ddb5-2145-4d7a-824c-a398ff6e7afa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>class</th>\n",
              "      <th>confidence</th>\n",
              "      <th>net score</th>\n",
              "      <th>neg</th>\n",
              "      <th>neu</th>\n",
              "      <th>pos</th>\n",
              "      <th>text</th>\n",
              "      <th>colored</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.924179</td>\n",
              "      <td>-0.304831</td>\n",
              "      <td>0.924179</td>\n",
              "      <td>0.066136</td>\n",
              "      <td>0.009686</td>\n",
              "      <td>However, this method generates the overall sen...</td>\n",
              "      <td>\u001b[48;2;0;0;0m\u001b[38;2;255;255;255mHowever, \u001b[48;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>118</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.917094</td>\n",
              "      <td>-0.296697</td>\n",
              "      <td>0.917094</td>\n",
              "      <td>0.055904</td>\n",
              "      <td>0.027002</td>\n",
              "      <td>However, we cannot complete this job yet out o...</td>\n",
              "      <td>\u001b[48;2;0;0;0m\u001b[38;2;255;255;255mHowever, \u001b[48;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.764693</td>\n",
              "      <td>-0.225062</td>\n",
              "      <td>0.764693</td>\n",
              "      <td>0.145799</td>\n",
              "      <td>0.089508</td>\n",
              "      <td>Quantitative metrics could not evaluate the re...</td>\n",
              "      <td>\u001b[48;2;0;0;89m\u001b[38;2;255;255;255mQuantitative ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>70</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.597995</td>\n",
              "      <td>-0.191424</td>\n",
              "      <td>0.597995</td>\n",
              "      <td>0.378284</td>\n",
              "      <td>0.023721</td>\n",
              "      <td>Those metrics are derived from citations and d...</td>\n",
              "      <td>\u001b[48;2;0;0;5m\u001b[38;2;255;255;255mThose \u001b[48;2;4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>140</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.597995</td>\n",
              "      <td>-0.191424</td>\n",
              "      <td>0.597995</td>\n",
              "      <td>0.378284</td>\n",
              "      <td>0.023721</td>\n",
              "      <td>Those metrics are derived from citations and d...</td>\n",
              "      <td>\u001b[48;2;0;0;5m\u001b[38;2;255;255;255mThose \u001b[48;2;4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.875303</td>\n",
              "      <td>0.290539</td>\n",
              "      <td>0.003685</td>\n",
              "      <td>0.121012</td>\n",
              "      <td>0.875303</td>\n",
              "      <td>We propose Phocus, a novel evaluation mechanis...</td>\n",
              "      <td>\u001b[48;2;253;0;0m\u001b[38;2;255;255;255mWe \u001b[48;2;0;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>110</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.907548</td>\n",
              "      <td>0.299300</td>\n",
              "      <td>0.009649</td>\n",
              "      <td>0.082802</td>\n",
              "      <td>0.907548</td>\n",
              "      <td>The main idea is shown in Figure 3.</td>\n",
              "      <td>\u001b[48;2;0;0;85m\u001b[38;2;255;255;255mThe \u001b[48;2;11...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>141</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.934530</td>\n",
              "      <td>0.303610</td>\n",
              "      <td>0.023699</td>\n",
              "      <td>0.041771</td>\n",
              "      <td>0.934530</td>\n",
              "      <td>Semantic Scholar makes the first step towards ...</td>\n",
              "      <td>\u001b[48;2;72;0;0m\u001b[38;2;255;255;255mSemantic \u001b[48...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>120</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.915919</td>\n",
              "      <td>0.303936</td>\n",
              "      <td>0.004112</td>\n",
              "      <td>0.079969</td>\n",
              "      <td>0.915919</td>\n",
              "      <td>Besides, we also compare our modules to other ...</td>\n",
              "      <td>\u001b[48;2;0;0;0m\u001b[38;2;255;255;255mBesides, \u001b[48;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>72</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.960823</td>\n",
              "      <td>0.311771</td>\n",
              "      <td>0.025509</td>\n",
              "      <td>0.013667</td>\n",
              "      <td>0.960823</td>\n",
              "      <td>Espe- cially, Semantic Scholar makes the first...</td>\n",
              "      <td>\u001b[48;2;0;0;0m\u001b[38;2;255;255;255mEspe- \u001b[48;2;0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>174 rows Ã— 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3713ddb5-2145-4d7a-824c-a398ff6e7afa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3713ddb5-2145-4d7a-824c-a398ff6e7afa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3713ddb5-2145-4d7a-824c-a398ff6e7afa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "errs"
      ],
      "metadata": {
        "id": "sfr8lMzYMJNq",
        "outputId": "15f2441c-98fb-42fc-f82e-1a7b99df1fce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 89, 137]"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(color_by_attn(content[86],tokenizer,preprocesser_model,encoder_model))"
      ],
      "metadata": {
        "id": "5W-uSOK4MKgk",
        "outputId": "47509aa2-4ec2-40a6-d5b2-d187fd5e516d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-9a638c6f8473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_by_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m86\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreprocesser_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-44-0f6592054b79>\u001b[0m in \u001b[0;36mcolor_by_attn\u001b[0;34m(text, toker, preper, encoder)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprocessed\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mprocess_for_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_attn_for_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreprocesser_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv_bytes_strs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-3ba09b0bc540>\u001b[0m in \u001b[0;36mget_attn_for_words\u001b[0;34m(context, tokenizer, prep, encoder)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindicies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0mattn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m       \u001b[0mfull_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 131 is out of bounds for axis 0 with size 125"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content[134]"
      ],
      "metadata": {
        "id": "igoZJhU3MWaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.split('\\d{1,}https:',content[134]) #remove subscripts"
      ],
      "metadata": {
        "id": "_BQ6mWooOfj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.split('Table \\d{1,}:',content[134]) #remove tables"
      ],
      "metadata": {
        "id": "mwEOZ1WSPiAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re.split('Figure \\d{1,}:',content[134]) #remove tables"
      ],
      "metadata": {
        "id": "YtulqK88P_wX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "input.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}