{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h7Lr7k5d1jd"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/ENEE439d-TEXTML/TextML/blob/master/input.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/ENEE439d-TEXTML/TextML/blob/master/input.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPiicLOUd1jj"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \"tensorflow-text==2.8.*\" # A dependency of the preprocessing for BERT inputs\n",
        "!pip install -q tf-models-official==2.7.0 # For adamW\n",
        "!pip install focal-loss # focal loss implmnetion for tf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52FpqTzrPr-t",
        "outputId": "759c72fc-f90b-44c0-e5b1-a2ed9e3747a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.9 MB 6.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 60.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 11.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 41.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 8.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 54.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47.8 MB 1.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 237 kB 57.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 90 kB 8.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 60.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting focal-loss\n",
            "  Downloading focal_loss-0.0.7-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tensorflow>=2.2 in /usr/local/lib/python3.7/dist-packages (from focal-loss) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.25.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (14.0.1)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.44.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (4.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.0.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.14.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (57.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (3.3.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.8.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (3.17.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2->focal-loss) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.2->focal-loss) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (3.3.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (3.2.0)\n",
            "Installing collected packages: focal-loss\n",
            "Successfully installed focal-loss-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BPnv0Vlcd3KI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd #basic imports\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split # https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization, bert  # to create AdamW optimizer\n",
        "from focal_loss import SparseCategoricalFocalLoss\n",
        "import official.nlp.bert.tokenization"
      ],
      "metadata": {
        "id": "dkxbtcKbP4AU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data read in"
      ],
      "metadata": {
        "id": "Q-Dc1EsOuQA0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "465BADPxeNII",
        "outputId": "241f8a72-5d37-4cb6-8ef6-994a7d755b12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Sk1zTIA6eOM5",
        "outputId": "8479483c-0152-494c-cad0-bd84aa5ee2db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  cited_paper  label                                               text\n",
              "0    A00-2024      0  We analyzed a set of articles and identified s...\n",
              "1    A00-2024      0  Table 3: Example compressions Compression AvgL...\n",
              "2    A00-2024      0  5.3 Related works and discussion Our two-step ...\n",
              "3    A00-2024      0  (1999) proposed a summarization system based o...\n",
              "4    A00-2024      0  We found that the deletion of lead parts did n..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-98eb8afe-54aa-4dde-a4ab-9103b2ed15ee\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cited_paper</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We analyzed a set of articles and identified s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>Table 3: Example compressions Compression AvgL...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>5.3 Related works and discussion Our two-step ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>(1999) proposed a summarization system based o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We found that the deletion of lead parts did n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-98eb8afe-54aa-4dde-a4ab-9103b2ed15ee')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-98eb8afe-54aa-4dde-a4ab-9103b2ed15ee button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-98eb8afe-54aa-4dde-a4ab-9103b2ed15ee');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "filepath = '/content/drive/MyDrive/Text-ML/full_sentiment_dataset.csv' #'data.csv'\n",
        "df= pd.read_csv(filepath)\n",
        "df1=df.drop(['no','paper','context_a','context_b'],axis=1)\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImSOYF6Py2J8",
        "outputId": "7a7f4241-e95c-418e-e8d5-2fd9adcf8baf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 0    7627\n",
              " 1     829\n",
              "-1     280\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtering by regex"
      ],
      "metadata": {
        "id": "FHIvNaLfuUoe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ytc03_FI-b5L",
        "outputId": "8ed4b90b-ee65-4730-ae65-5b1f9e7ef222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\[*\\]|\\(\\d{4}\\)|\\(?((\\(?((\\w+, )*(\\w+ )+)((and|&) ((\\w+ *))?)?,? \\(?\\d{4}\\)?|((\\w+, )*(\\w+ )+)et al\\. ?,? \\(?\\d{4}\\)?\\)?)((; )|( (and|&) ))*)+\n"
          ]
        }
      ],
      "source": [
        "context=df1['text']\n",
        "\n",
        "re1= \"\\(((([A-Za-z]+ *)+(, \\d+))+(; )*)+\\)\" # matches author and author, year\n",
        "re_year=\",? \\(?\\d{4}\\)?\" # match , {4 digits} which may be wrapped in () \n",
        "re_and=\"(and|&) \"\n",
        "re_auth=\"((\\w+, )*(\\w+ )+)\"\n",
        "re_et= re_auth+\"et al\\. ?\"+re_year # matches author et al. , year\n",
        "re_2a= re_auth+\"(\"+re_and+\"((\\w+ *))?)?\"+re_year # matches author and author, year\n",
        "re_sep=\"((; )|( \"+re_and+\"))*\"# match the '; ' gap or ' and ' gap\n",
        "re_para_year=\"\\(\\d{4}\\)\"\n",
        "re_in_brack=\"\\[*\\]\"\n",
        "re_apa =re_in_brack+\"|\"+re_para_year+\"|\"+\"\\(?(\"+\"(\\(?\"+re_2a+\"|\"+re_et+\"\\)?)\"+re_sep+\")+\"\n",
        "print(re_apa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NEZIAWQtykDg"
      },
      "outputs": [],
      "source": [
        "def remove_matches(text,regex=re_apa):\n",
        "  text1=text\n",
        "  rem_len=0\n",
        "  pattern= re.compile(regex)\n",
        "  while True:\n",
        "    matches=pattern.search(text1)\n",
        "    #print(matches)\n",
        "    if matches == None:\n",
        "      break\n",
        "\n",
        "    spn=matches.span()\n",
        "    text1=text1[0:spn[0]]+text1[spn[1]:-1]\n",
        "    cit_len=spn[1]-spn[0]\n",
        "    rem_len+=cit_len\n",
        "  \n",
        "  if len(text) >0:\n",
        "    percent_removed=rem_len/len(text)\n",
        "  else:\n",
        "    percent_removed=1\n",
        "  return text1,percent_removed \n",
        "\n",
        "# print(context[5])\n",
        "# remove_citation(context[5],regex=re_apa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TmjjH1gp9A6T"
      },
      "outputs": [],
      "source": [
        "output=df1['text'].apply(lambda x: remove_matches(text=x,regex=re_apa)) #df['col1'] = df.apply(lambda x: complex_function(x['col1']), axis=1)\n",
        "df_o = pd.DataFrame(list(output), columns =['clean','p_rem'])\n",
        "output_1=df_o['clean'].apply(lambda x: remove_matches(text=x,regex='[^\\w_0-9 ]+')) \n",
        "df_o_1 = pd.DataFrame(list(output_1), columns =['clean','p_rem'])\n",
        "#df_o.head()\n",
        "\n",
        "df1['text_clean']=df_o_1['clean']\n",
        "df1['text_clean_len']=df_o_1['clean'].apply(len)\n",
        "df1['p_rem']=df_o['p_rem']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "OVt3NtaJDDrx",
        "outputId": "ae82e824-9a1f-43e5-d989-977d7dfcdbe8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     cited_paper  label                                               text  \\\n",
              "0       A00-2024      0  We analyzed a set of articles and identified s...   \n",
              "1       A00-2024      0  Table 3: Example compressions Compression AvgL...   \n",
              "2       A00-2024      0  5.3 Related works and discussion Our two-step ...   \n",
              "3       A00-2024      0  (1999) proposed a summarization system based o...   \n",
              "4       A00-2024      0  We found that the deletion of lead parts did n...   \n",
              "...          ...    ...                                                ...   \n",
              "8731    W96-0213      1  He has achieved state-of-the art results by ap...   \n",
              "8732    W96-0213      0  B = (Brill and Wu, 1998); M = (Magerman, 1995)...   \n",
              "8733    W96-0213      0  The model we use is similar to that of (Ratnap...   \n",
              "8734    W96-0213      1  Our model exploits the same kind of tag-n-gram...   \n",
              "8735    W96-0213      0  In that table, TBL stands for Brill's transfor...   \n",
              "\n",
              "                                             text_clean  text_clean_len  \\\n",
              "0     We analyzed a set of articles and identified s...             425   \n",
              "1     Table 3 Example compressions Compression AvgLe...             229   \n",
              "2     53 Related works and discussion Our twostep mo...             105   \n",
              "3      proposed a summarization system based on the ...             321   \n",
              "4     We found that the deletion of lead parts did n...              73   \n",
              "...                                                 ...             ...   \n",
              "8731  He has achieved stateofthe art results by appl...             139   \n",
              "8732   B  M  Magerman 1995 O  our data R  Ratnaparkhi 1              48   \n",
              "8733  The model we use is similar to that of Ratnapa...              55   \n",
              "8734  Our model exploits the same kind of tagngram i...             157   \n",
              "8735  In that table TBL stands for Brills transforma...             288   \n",
              "\n",
              "         p_rem  \n",
              "0     0.098765  \n",
              "1     0.260745  \n",
              "2     0.308176  \n",
              "3     0.078804  \n",
              "4     0.408000  \n",
              "...        ...  \n",
              "8731  0.151515  \n",
              "8732  0.421488  \n",
              "8733  0.000000  \n",
              "8734  0.000000  \n",
              "8735  0.000000  \n",
              "\n",
              "[8736 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-55423898-3b57-4f9c-a3e0-488d3753e7f2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cited_paper</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>text_clean_len</th>\n",
              "      <th>p_rem</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We analyzed a set of articles and identified s...</td>\n",
              "      <td>We analyzed a set of articles and identified s...</td>\n",
              "      <td>425</td>\n",
              "      <td>0.098765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>Table 3: Example compressions Compression AvgL...</td>\n",
              "      <td>Table 3 Example compressions Compression AvgLe...</td>\n",
              "      <td>229</td>\n",
              "      <td>0.260745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>5.3 Related works and discussion Our two-step ...</td>\n",
              "      <td>53 Related works and discussion Our twostep mo...</td>\n",
              "      <td>105</td>\n",
              "      <td>0.308176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>(1999) proposed a summarization system based o...</td>\n",
              "      <td>proposed a summarization system based on the ...</td>\n",
              "      <td>321</td>\n",
              "      <td>0.078804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We found that the deletion of lead parts did n...</td>\n",
              "      <td>We found that the deletion of lead parts did n...</td>\n",
              "      <td>73</td>\n",
              "      <td>0.408000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8731</th>\n",
              "      <td>W96-0213</td>\n",
              "      <td>1</td>\n",
              "      <td>He has achieved state-of-the art results by ap...</td>\n",
              "      <td>He has achieved stateofthe art results by appl...</td>\n",
              "      <td>139</td>\n",
              "      <td>0.151515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8732</th>\n",
              "      <td>W96-0213</td>\n",
              "      <td>0</td>\n",
              "      <td>B = (Brill and Wu, 1998); M = (Magerman, 1995)...</td>\n",
              "      <td>B  M  Magerman 1995 O  our data R  Ratnaparkhi 1</td>\n",
              "      <td>48</td>\n",
              "      <td>0.421488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8733</th>\n",
              "      <td>W96-0213</td>\n",
              "      <td>0</td>\n",
              "      <td>The model we use is similar to that of (Ratnap...</td>\n",
              "      <td>The model we use is similar to that of Ratnapa...</td>\n",
              "      <td>55</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8734</th>\n",
              "      <td>W96-0213</td>\n",
              "      <td>1</td>\n",
              "      <td>Our model exploits the same kind of tag-n-gram...</td>\n",
              "      <td>Our model exploits the same kind of tagngram i...</td>\n",
              "      <td>157</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8735</th>\n",
              "      <td>W96-0213</td>\n",
              "      <td>0</td>\n",
              "      <td>In that table, TBL stands for Brill's transfor...</td>\n",
              "      <td>In that table TBL stands for Brills transforma...</td>\n",
              "      <td>288</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8736 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55423898-3b57-4f9c-a3e0-488d3753e7f2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-55423898-3b57-4f9c-a3e0-488d3753e7f2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-55423898-3b57-4f9c-a3e0-488d3753e7f2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove under and over sized samples\n",
        "large samples appear to be poorly written"
      ],
      "metadata": {
        "id": "pkNPWqAHKLxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getMidLen(data,label,labelKey='label',lenKey='text_clean_len',lowMod=1,highMod=1):\n",
        "  df1 =data.loc[data[labelKey] == label]\n",
        "  neu_mean=np.mean(list(df1[lenKey]))\n",
        "  neu_std=np.std(list(df1[lenKey]))\n",
        "  df1_no_high = df1.loc[df1[lenKey] < highMod*(neu_mean +neu_std)]\n",
        "  # print(neu_mean)\n",
        "  # print(neu_std)\n",
        "\n",
        "  while neu_std > neu_mean:\n",
        "    neu_mean=np.mean(list(df1_no_high['text_clean_len']))\n",
        "    neu_std=np.std(list(df1_no_high['text_clean_len']))\n",
        "    # print(neu_mean)\n",
        "    # print(neu_std)\n",
        "    df1_no_high = df1.loc[df1['text_clean_len'] < highMod*(neu_mean +neu_std)]\n",
        "\n",
        "  df1_mid = df1_no_high.loc[df1_no_high['text_clean_len'] > lowMod*(neu_mean -neu_std)]\n",
        "\n",
        "  return df1_mid\n",
        "\n",
        "df2 = df1.loc[df1['p_rem'] < .5] #keep sampels with less than half of it are citation\n",
        "\n",
        "df_neu=getMidLen(df2,0,lowMod=2)\n",
        "df_pos=getMidLen(df2,1,lowMod=1,highMod=2)\n",
        "df_neg=getMidLen(df2,-1,lowMod=1,highMod=2)\n",
        "df3= pd.concat([df_neg,df_neu,df_pos])\n",
        "df3['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq-BsMfleXID",
        "outputId": "e3b8cf5d-1767-4daf-ebe4-74436b4ddc86"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 0    2524\n",
              " 1     746\n",
              "-1     246\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def catagorize(data,labelKey='label'):\n",
        "  rows=len(data.index)\n",
        "  onehots=np.zeros((rows,3),dtype=int)\n",
        "  for i,lab in enumerate(data[labelKey]):\n",
        "    onehots[i][lab+1]=1\n",
        "  return onehots\n",
        "\n",
        "hots=catagorize(df3)\n",
        "df3['label_onehot']=list(hots)\n",
        "df3['label_index']=df3['label']+1"
      ],
      "metadata": {
        "id": "VnsKnaB0AU4i"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq= np.array(list(df3['label_index'].value_counts(normalize=True,sort=False)))\n",
        "print(freq)\n",
        "class_ratio= 1/freq\n",
        "class_ratio"
      ],
      "metadata": {
        "id": "Mbn2dX1C2Ixs",
        "outputId": "d003d74b-e34d-47f0-b69f-e514aa08b4b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.06996587 0.71786121 0.21217292]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([14.29268293,  1.39302694,  4.71313673])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Model"
      ],
      "metadata": {
        "id": "QtrXZF4_unOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(list(df3['text_clean']), list(df3['label_index']), test_size=0.2, random_state=42)\n",
        "X_train= [[s] for s in X_train]\n",
        "X_test= [[s] for s in X_test]\n",
        "y_train=[[s] for s in list(y_train)]\n",
        "y_test=[[s] for s in list(y_test)]"
      ],
      "metadata": {
        "id": "kKtJq5Baj1Wl"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose a BERT model to fine-tune (Taken from tutorial)\n",
        "\n",
        "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "metadata": {
        "id": "9WK-J5dQpprm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9835c3c-9b3b-4a8f-fb88-3d1fb7885afd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## check model passes"
      ],
      "metadata": {
        "id": "91uobQaIu-iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ],
      "metadata": {
        "id": "YINTv-Uu8HP5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_test = X_train[1]\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
        "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
        "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGAgJwGg08-j",
        "outputId": "6c2fcacc-f59c-4f44-e068-6434f4b4c836"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys       : ['input_word_ids', 'input_type_ids', 'input_mask']\n",
            "Shape      : (1, 128)\n",
            "Word Ids   : [  101  1999  5688 11416  6024  4275  2024  4738  2000 25845  1996  4101]\n",
            "Input Mask : [1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "metadata": {
        "id": "U2o1JW9b9MHu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
        "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
        "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
        "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
        "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW0iEt6Z9PKt",
        "outputId": "ceaa502a-7bd0-4264-a275-bf55bfdfa3c4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Pooled Outputs Shape:(1, 512)\n",
            "Pooled Outputs Values:[ 0.99835694 -0.7678578  -0.21300124  0.08411987 -0.08593949  0.98550844\n",
            "  0.9732349  -0.8306031  -0.55687106 -0.95725054 -0.3960305  -0.94115573]\n",
            "Sequence Outputs Shape:(1, 128, 512)\n",
            "Sequence Outputs Values:[[ 3.8915187e-01  2.3267061e-01  6.1780311e-02 ... -9.2866874e-01\n",
            "   4.3027106e-01  8.3279318e-01]\n",
            " [ 4.5310837e-01  7.2308904e-01 -3.2866317e-01 ... -1.1163950e-04\n",
            "  -3.3482751e-01  5.7274055e-01]\n",
            " [-2.5876865e-01  1.4199525e+00 -4.2525381e-01 ...  4.9038833e-01\n",
            "   1.4491324e-01  5.7048750e-01]\n",
            " ...\n",
            " [ 2.6529512e-01 -2.3668993e-01  8.4921330e-02 ...  2.0211086e-02\n",
            "   3.9103544e-01  8.9449620e-01]\n",
            " [ 2.9499257e-01  5.8424294e-01 -6.7344594e-01 ... -1.6148384e+00\n",
            "   1.3211843e+00 -6.0517174e-01]\n",
            " [-1.8697177e-01  5.2527428e-01  9.2377967e-01 ... -5.6088662e-01\n",
            "   1.0293359e+00 -7.8856331e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## full model setup"
      ],
      "metadata": {
        "id": "GSyS6XhXvGXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(3, activation='softmax', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "metadata": {
        "id": "XWtmKUBu_3kJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model = build_classifier_model()"
      ],
      "metadata": {
        "id": "Z17Cu4Awc3jA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## check loss function"
      ],
      "metadata": {
        "id": "KgOUUHifvD3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_raw_result = classifier_model(tf.constant(text_test))\n",
        "print(bert_raw_result)\n",
        "\n",
        "l =  SparseCategoricalFocalLoss(gamma=2,class_weight=class_ratio)\n",
        "test =tf.convert_to_tensor([1.0])\n",
        "l(test,bert_raw_result)"
      ],
      "metadata": {
        "id": "jHqpunBDTKym",
        "outputId": "9f0026ad-6a60-4800-dd67-b21d797f400e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[0.07656216 0.749452   0.1739859 ]], shape=(1, 3), dtype=float32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.02522065>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Save and Log"
      ],
      "metadata": {
        "id": "hTEGlj9RvLg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "steps_per_epoch = 200 #tf.data.experimental.cardinality(X_train).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 2e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "# def auc_wrapper(y_true,y_pred):\n",
        "#   print(y_true,y_pred)\n",
        "\n",
        "#   y_true=tf.reshape(y_true,[1])\n",
        "#   print(y_true)\n",
        "#   y_true= tf.cast(y_true, tf.int32)\n",
        "#   print(y_true)\n",
        "#   y_true=tf.one_hot(y_true,depth=3)\n",
        "#   print(y_true)\n",
        "#   return tf.keras.metrics.AUC(multi_label=True)(y_true,y_pred)\n",
        "\n",
        "\n",
        "loss =  SparseCategoricalFocalLoss(gamma=2,class_weight=class_ratio) #tf.keras.losses.MeanSquaredError()\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]#, auc_wrapper]#, tf.keras.metrics.AUC(multi_label=True)]\n",
        "\n",
        "\n",
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"citation_BERT_{epoch}\",\n",
        "        save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
        "        monitor=\"val_sparse_categorical_accuracy\",\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.TensorBoard('./logs', update_freq=1)\n",
        "]\n"
      ],
      "metadata": {
        "id": "MB9Af5RMCuqP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f'Training model with {tfhub_handle_encoder}')\n",
        "# history = classifier_model.fit(x=X_train,y=y_train, validation_data=(X_test,y_test),epochs=epochs,callbacks= callbacks, verbose=True)"
      ],
      "metadata": {
        "id": "89v_3XqEE3HN"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classifier_model.save_weights(''/content/drive/MyDrive/Text-ML/checkpoint1')"
      ],
      "metadata": {
        "id": "Pox0n2xZjdtX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect model"
      ],
      "metadata": {
        "id": "Z_fseNCGvY4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "00eGEQhDdMUK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%tensorboard --logdir=logs"
      ],
      "metadata": {
        "id": "Gszh9ZNBbQXe"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.load_weights('/content/drive/MyDrive/Text-ML/checkpoint')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMRXTmWvmBNn",
        "outputId": "3f084ea8-8e1f-44fa-aa41-9a742d9ff606"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fad54818f90>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds=classifier_model.predict(X_train,verbose=1)"
      ],
      "metadata": {
        "id": "vCgZJCYsvdMe",
        "outputId": "b066afa4-87bb-45cb-c5a3-dcf1083aea65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88/88 [==============================] - 197s 2s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds_t=classifier_model.predict(X_test,verbose=1)"
      ],
      "metadata": {
        "id": "Wu5a35I1wlOi",
        "outputId": "b96d2fba-58d8-4ed7-ecb7-f273f0fd3410",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 49s 2s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns; sns.set_theme()"
      ],
      "metadata": {
        "id": "Q-NxIjYJx8S1"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_mat=tf.math.confusion_matrix(np.argmax(preds,-1),y_train)\n",
        "ax = sns.heatmap(c_mat,annot=True,linewidths=.5)"
      ],
      "metadata": {
        "id": "_5R4bXyzxoql",
        "outputId": "8124b216-f286-49b3-ea83-bcb09c6a2421",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c_mat=tf.math.confusion_matrix(np.argmax(preds_t,-1),y_test)\n",
        "ax = sns.heatmap(c_mat,annot=True,linewidths=.5)"
      ],
      "metadata": {
        "id": "RPkpGRdDyCz6",
        "outputId": "f4e79de6-c346-4063-925b-b34922941b2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get attention colorings"
      ],
      "metadata": {
        "id": "ZRca_y_Kvq86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1\n"
      ],
      "metadata": {
        "id": "aSNvnkyoJdqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.layers"
      ],
      "metadata": {
        "id": "PR_gDYEQofck",
        "outputId": "1a3c7f44-a31a-4764-f516-0b7901863e7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.engine.input_layer.InputLayer at 0x7fad57ceb3d0>,\n",
              " <tensorflow_hub.keras_layer.KerasLayer at 0x7fad57ca6710>,\n",
              " <tensorflow_hub.keras_layer.KerasLayer at 0x7fad57c0ab10>,\n",
              " <keras.layers.core.dropout.Dropout at 0x7fad569ae390>,\n",
              " <keras.layers.core.dense.Dense at 0x7fad56a6e650>]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from official.nlp import bert \n",
        "import official.nlp.bert.tokenization"
      ],
      "metadata": {
        "id": "bC2uW7zrx7VR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = bert.tokenization.FullTokenizer(vocab_file='/content/drive/MyDrive/Text-ML/vocab.txt')\n",
        "preprocesser_model = keras.Model(inputs=classifier_model.input,outputs=classifier_model.get_layer('preprocessing').output)\n",
        "encoder_model = keras.Model(inputs=classifier_model.input,outputs=classifier_model.get_layer('BERT_encoder').output)"
      ],
      "metadata": {
        "id": "Lnb_ie7zxpbc"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocab size:\", len(tokenizer.vocab))"
      ],
      "metadata": {
        "id": "mjgJ0WAwRjYb",
        "outputId": "360e6c09-196d-43b7-a821-3999f910cb98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 30522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_for_input(raw_context):\n",
        "  c1,_ =remove_matches(text=raw_context,regex=re_apa)\n",
        "  c2,_ =remove_matches(text=c1,regex='[^\\w_0-9 ]+')\n",
        "  return [c2]\n",
        "  \n",
        "#process_for_input(example)"
      ],
      "metadata": {
        "id": "PTVxz5uFJZyI"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attn(context,prep,encoder): # assume stirng array input\n",
        "  t_context=tf.convert_to_tensor(context)\n",
        "\n",
        "  p_out=prep(t_context)\n",
        "  stop_index=0\n",
        "  while(p_out[\"input_mask\"][0][stop_index] == 1):\n",
        "    stop_index+=1\n",
        "\n",
        "  output = encoder(t_context)\n",
        "  valid_entries=output[\"sequence_output\"][:,1:stop_index-1,:]\n",
        "  a=tf.math.reduce_mean(valid_entries,-1)\n",
        "  mean=tf.math.reduce_mean(a,-1,keepdims=True)\n",
        "  std=tf.math.reduce_std(a,-1,keepdims=True)\n",
        "  a1=(a-mean)/std\n",
        "\n",
        "  return a1"
      ],
      "metadata": {
        "id": "NC_Cyexu9TQI"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attn_for_words(context,tokenizer,prep,encoder):\n",
        "  attn = get_attn(context,prep,encoder).numpy()\n",
        "  tokens = tokenizer.tokenize(context[0]) \n",
        "\n",
        "  indicies=np.ones((len(tokens)),dtype=int)\n",
        "  for i,tok in enumerate(tokens):\n",
        "    if '##' in tok:\n",
        "      indicies[i]=0\n",
        "\n",
        "  full_words=tokens.copy()\n",
        "  ix=-1\n",
        "  for i,tok in enumerate(tokens):\n",
        "    if not indicies[i]:\n",
        "      attn[0][ix]+=attn[0][i]\n",
        "      full_words[ix]+=tok[2:]\n",
        "    else:\n",
        "      ix=i\n",
        "\n",
        "  t_f=tf.convert_to_tensor(full_words) #stores as byte string...\n",
        "  masked_f=tf.boolean_mask(t_f,indicies)\n",
        "\n",
        "  t_a=tf.convert_to_tensor(attn)[0]\n",
        "  masked_a=tf.boolean_mask(t_a,indicies)\n",
        "\n",
        "  return masked_f.numpy(),masked_a.numpy()\n",
        "\n",
        "#words, at=get_attn_for_words(processed,tokenizer,preprocesser_model,encoder_model)"
      ],
      "metadata": {
        "id": "iyhKVIiY121W"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_bytes_strs(words):\n",
        "  return [w.decode('UTF-8') for w in list(words)]"
      ],
      "metadata": {
        "id": "uFUPhtLDH5SS"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_to_color(attn): #blue pos red neg\n",
        "  rgbs = np.zeros((len(attn),3),dtype=int)\n",
        "  for i,score in enumerate(attn):\n",
        "    if score > 0:\n",
        "      rgbs[i][0]=255*score//2\n",
        "    else:\n",
        "      rgbs[i][2]=-255*score//2\n",
        "  \n",
        "  return rgbs"
      ],
      "metadata": {
        "id": "yDhvZkOFMFJ3"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def coloring(text,fore=None,back=None):\n",
        "    txt=text\n",
        "    if fore != None:\n",
        "      txt = \"\\033[38;2;{};{};{}m\".format(fore[0], fore[1], fore[2])+txt\n",
        "    if back != None:\n",
        "      txt = \"\\033[48;2;{};{};{}m\".format(back[0], back[1], back[2])+txt\n",
        "    return txt\n",
        "\n",
        "#print(coloring('Hello',back=[500,0,0]) + coloring('Hello', back=(0,0,255)))"
      ],
      "metadata": {
        "id": "X8GvqhLvOR8H"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def color_by_attn(text,toker,preper,encoder):\n",
        "  all_words_original=text.split()\n",
        "  all_words=text.lower().split()\n",
        "  processed= process_for_input(text)\n",
        "  words, at=get_attn_for_words(processed,tokenizer,preprocesser_model,encoder_model)\n",
        "\n",
        "  ws=conv_bytes_strs(words)\n",
        "  conv=conv_to_color(at)\n",
        "  mapping=dict(zip(ws,conv))\n",
        "  orig_mapping=dict(zip(all_words,all_words_original))\n",
        "\n",
        "  for i,w in enumerate(all_words):\n",
        "    if w not in mapping:\n",
        "      mapping[w]=[0,0,0]\n",
        "    else:\n",
        "      mapping[w]=list(mapping[w])\n",
        "\n",
        "  colored=[coloring(orig_mapping[word],back=mapping[word]) for word in all_words]\n",
        "  printed=' '.join(colored)\n",
        "  return printed\n",
        "\n",
        "example=list(df3['text'])[0]\n",
        "print(color_by_attn(example,tokenizer,preprocesser_model,encoder_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hossOHKtZ1Wn",
        "outputId": "8f16ae82-1032-421d-cc17-39809dbfd17a"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[48;2;0;0;14mMany \u001b[48;2;94;0;0mapproaches \u001b[48;2;30;0;0mfor \u001b[48;2;0;0;127mPOS \u001b[48;2;0;0;104mtagging \u001b[48;2;62;0;0mhave \u001b[48;2;0;0;13mbeen \u001b[48;2;0;0;201mdeveloped \u001b[48;2;46;0;0min \u001b[48;2;0;0;15mthe \u001b[48;2;0;0;0mpast, \u001b[48;2;262;0;0mincluding \u001b[48;2;0;0;0mrule-based \u001b[48;2;0;0;104mtagging \u001b[48;2;0;0;0m(Brill, \u001b[48;2;0;0;0m1995), \u001b[48;2;32;0;0mHMM \u001b[48;2;0;0;306mtaggers \u001b[48;2;0;0;0m(Brants, \u001b[48;2;0;0;0m2000; \u001b[48;2;0;0;0mCutting \u001b[48;2;0;0;0mand \u001b[48;2;0;0;0mothers, \u001b[48;2;0;0;0m1992), \u001b[48;2;0;0;0mmaximum-entropy \u001b[48;2;0;0;80mmodels \u001b[48;2;0;0;0m(Rathnaparki, \u001b[48;2;0;0;0m1996), \u001b[48;2;107;0;0mcyclic \u001b[48;2;0;0;69mdependency \u001b[48;2;38;0;0mnetworks \u001b[48;2;0;0;0m(Toutanova \u001b[48;2;0;0;0met \u001b[48;2;0;0;0mal. \u001b[48;2;0;0;0m, \u001b[48;2;0;0;0m2003), \u001b[48;2;0;0;0mmemory-based \u001b[48;2;73;0;0mlearning \u001b[48;2;0;0;0m(Daelemans \u001b[48;2;0;0;0met \u001b[48;2;0;0;0mal. \u001b[48;2;0;0;0m, \u001b[48;2;0;0;0m1996), \u001b[48;2;0;0;0metc. \u001b[48;2;26;0;0mall \u001b[48;2;254;0;0mof \u001b[48;2;172;0;0mthese \u001b[48;2;94;0;0mapproaches \u001b[48;2;80;0;0mrequire \u001b[48;2;0;0;212meither \u001b[48;2;211;0;0ma \u001b[48;2;0;0;209mlarge \u001b[48;2;28;0;0mamount \u001b[48;2;254;0;0mof \u001b[48;2;0;0;55mannotated \u001b[48;2;0;0;66mtraining \u001b[48;2;0;0;167mdata \u001b[48;2;0;0;0m(for \u001b[48;2;0;0;69msupervised \u001b[48;2;0;0;0mtagging) \u001b[48;2;91;0;0mor \u001b[48;2;211;0;0ma \u001b[48;2;0;0;505mlexicon \u001b[48;2;13;0;0mlisting \u001b[48;2;26;0;0mall \u001b[48;2;84;0;0mpossible \u001b[48;2;0;0;45mtags \u001b[48;2;30;0;0mfor \u001b[48;2;72;0;0meach \u001b[48;2;0;0;192mword \u001b[48;2;0;0;0m(for \u001b[48;2;0;0;0munsupervised \u001b[48;2;0;0;0mtagging).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF text extratction\n"
      ],
      "metadata": {
        "id": "TD_FQ6ioGFGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pdfminer.six"
      ],
      "metadata": {
        "id": "zPD706w-I9zR",
        "outputId": "22300c57-7c48-4a49-f48c-3e0e4b2e87be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20220319-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 9.0 MB/s \n",
            "\u001b[?25hCollecting cryptography\n",
            "  Downloading cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 35.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.21)\n",
            "Installing collected packages: cryptography, pdfminer.six\n",
            "Successfully installed cryptography-37.0.2 pdfminer.six-20220319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfparser import PDFParser"
      ],
      "metadata": {
        "id": "hKrdAGGRJPJB"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_pdf_to_string(file_path):\n",
        "\toutput_string = StringIO()\n",
        "\twith open(file_path, 'rb') as in_file:\n",
        "\t    parser = PDFParser(in_file)\n",
        "\t    doc = PDFDocument(parser)\n",
        "\t    rsrcmgr = PDFResourceManager()\n",
        "\t    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
        "\t    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "\t    for page in PDFPage.create_pages(doc):\n",
        "\t        interpreter.process_page(page)\n",
        "\n",
        "\treturn(output_string.getvalue())\n",
        " \n",
        "def sent_extract(sample):\n",
        "\tdelims=re.findall('\\. [A-Z]',sample)\n",
        "\tsents=re.split('\\. [A-Z]',sample)\n",
        "\n",
        "\tsents[0]=sents[0]+'.'\n",
        "\tfor i,s in enumerate(sents[1:]):\n",
        "\t\tsents[i+1]=delims[i][2]+s+'.'\n",
        "\treturn sents\n",
        "\n",
        "def pdf_text_extract(path):\n",
        "  text=convert_pdf_to_string(path)\n",
        "  text1 = text.replace('\\x0c','')\n",
        "  text2 = text1.split('.\\n\\n')\n",
        "  refine=[t.replace('\\n',' ') for t in text2]\n",
        "  r=[]\n",
        "  for t in refine:\n",
        "    r+=sent_extract(t)\n",
        "  return r\n",
        "\n",
        "path='/content/drive/MyDrive/Text-ML/phocus.pdf'\n",
        "pdf_text_extract(path)\n"
      ],
      "metadata": {
        "id": "HkUe38wlJSov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdcabd79-3424-46cc-b9d4-2f339fe95611"
      },
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2 2 0 2  n a J  4 1  ] L D . s c [  2 v 5 1 9 2 0 . 1 0 2 2 : v i X r a  Phocus: Picking Valuable Research from a Sea of Citations  Xinrong Zhang zxr19@mails.tsinghua.edu.cn Tsinghua University Beijing, China  Zihou Ren∗ rzh20@mails.tsinghua.edu.cn Tsinghua University Beijing, China  Shuqi Liu∗ liu-sq19@mails.tsinghua.edu.cn Tsinghua University Beijing, China  Yunlong Deng∗ dengyl20@mails.tsinghua.edu.cn Tsinghua University Beijing, China  Xi Li∗ lixi19@mails.tsinghua.edu.cn Tsinghua University Beijing, China  Yadi Xiao∗ xyd18@mails.tsinghua.edu.cn Tsinghua University Beijing, China  Yuxing Han∗ yuxinghan@tsinghua-sz.org Tsinghua Shenzhen International Graduate School Shenzhen, China  Jiangtao Wen† jtwen@tsinghua.edu.cn Tsinghua University Beijing, China  ABSTRACT The deluge of new papers has significantly blocked the develop- ment of academics, which is mainly caused by author-level and publication-level evaluation metrics that only focus on quantity.',\n",
              " 'Those metrics have resulted in several severe problems that trou- ble scholars focusing on the important research direction for a long time and even promote an impetuous academic atmosphere.',\n",
              " 'To solve those problems, we propose Phocus, a novel academic evaluation mechanism for authors and papers.',\n",
              " 'Phocus analyzes the sentence containing a citation and its contexts to predict the sentiment towards the corresponding reference.',\n",
              " 'Combining others factors, Phocus classifies citations coarsely, ranks all references within a paper, and utilizes the results of the classifier and the rank- ing model to get the local influential factor of a reference to the citing paper.',\n",
              " 'The global influential factor of the reference to the citing paper is the product of the local influential factor and the total influential factor of the citing paper.',\n",
              " 'Consequently, an author’s academic influential factor is the sum of one’s contributions to each paper one co-authors.',\n",
              " 'KEYWORDS citation classification, sentiment analysis, academic influential fac- tor, data mining  Figure 1: the number of new publications on IEEE Xplore each year from 2000 to 2021.',\n",
              " '1 INTRODUCTION The number of papers published each year has grown greatly.',\n",
              " 'For example, as shown in Figure 1, the number of new papers on IEEE Xplore1 increases sharply over the decade.',\n",
              " 'Paper boom in academic fields results in many severe problems.',\n",
              " 'Cortes et al. [10] examine 2014 NeurIPS and find that it is not able to pick out excellent researches, and could identify terrible papers.',\n",
              " 'Chu et al. [8] reveal that too many papers published each year in a field hinder its development.',\n",
              " 'They state this opinion in two aspects.',\n",
              " 'First, researchers are busy coping with a lot of papers, but don’t have enough time to fully learn novel ideas; Second, the focused attention on a promising idea might be broken up by the deluge of new ideas.',\n",
              " '∗Authors contributed equally to this research. †Corresponding author 1https://ieeexplore.ieee.org/Xplore/home.jsp  Reference Format: Xinrong Zhang, Zihou Ren, Xi Li, Shuqi Liu, Yunlong Deng, Yadi Xiao, Yuxing Han, and Jiangtao Wen. 2022.',\n",
              " 'Phocus: Picking Valuable Research from a Sea of Citations.',\n",
              " 'In Proceedings of . , 7 pages.',\n",
              " ', , © 2022 .',\n",
              " '            , ,  Zhang and Wen et al.',\n",
              " 'The reason for the sharp increase in papers is that evaluation metrics for researchers and scholars focus on the number of papers.',\n",
              " 'From the scientific output, research funding, to the evaluation of professional rank, papers play a very important role, and the more papers, the better.',\n",
              " 'However, It is time to make changes.',\n",
              " 'Quantitative metrics could not evaluate the real academic impact of a scholar or a paper.',\n",
              " 'They ignore the essential differences between citations, which is a fatal error.',\n",
              " 'Seglen expresses strong opposition to im- pact factors that measure the academic influence of journals for committees seldom have the specialist’ insights to assess primary researches[31].',\n",
              " 'We propose Phocus, a novel evaluation mechanism for scholars and publications.',\n",
              " 'Phocus analyzes the sentence containing a citation and its contexts to predict the sentiment polarity towards the corre- sponding reference.',\n",
              " 'Besides, Phocus also considers the total number of citations, the number of citations per sentence, author overlap, and the number of references, similar to [35].',\n",
              " 'Given those factors above, Phocus uses Naive Bayesian Classifier to divide citations coarsely into 4 categories and utilizes the LambdaMART model to sort all references within a paper.',\n",
              " 'Combining the categories and the ranking results, every reference gets its local influential factor within [−1, 1], related to the citing paper.',\n",
              " 'The global influential factor of the reference to the citing paper is the product of the local influential factor and the total influential factor of the citing paper.',\n",
              " 'Consequently, an author’s academic influential factor is the sum of his contributions to each paper he co-authors.',\n",
              " '2 RELATED WORK Our work involves citation classification, aspect-based sentiment analysis, ranking model and evaluation metrics for academics, which will be introduced in subsections below respectively.',\n",
              " '2.1 Citation Classification In fact, there are already many kinds of research that have focused on citation classification.',\n",
              " 'For example, Teufel et al. [33] classify citation intents into 12 classes, using simple regular match to ex- tract features.',\n",
              " 'Valenzuela et al. [35] divide citations into 4 classes: highly influential, background, method and results citations, using SVM with an RBF kernel and random forests, taking 13 features into consideration: total number of direct citations, number of direct cita- tions per section, the total number of indirect citations and number of indirect citations per section, author overlap, is considered help- ful, citation appears in table and caption, 1/number of references, number of paper citations/all citations, the similarity between ab- stracts, PageRank[28], number of total citing papers after transitive closure, and field of the cited paper.',\n",
              " 'While Jurgens et al. [20] define 7 classes of citation intents: background, motivation, uses, extension, continuation, comparison or contrast, and future, with a Random Forest classifier trained using 4 types of features: structural fea- tures, lexical, morphological and grammatical features, field, and usage.',\n",
              " 'Cohan et al. [9] propose a multitask model using BiLSTM and attention mechanism to classify citation intents that is the primary task and predict the section where the citation occurs and where a sentence needs a citation that is auxiliary tasks and is used to assist the primary task2.',\n",
              " 'They categorize intents into 3 classes:  background information, method, and result comparison.',\n",
              " 'Besides, Cohan builds a citation intent dataset SciCite.',\n",
              " 'Those works sim- ply classify citations according to intents but ignore the sentiment citing paper towards references, which is vital.',\n",
              " 'Butt et al. [6] utilize Naive-Bayes Classifier to predict the senti- ment polarity of a sentence containing a citation and its contexts.',\n",
              " 'Whereas Liu et al. [23] use averaged word embeddings to represent sentence vectors and to classify sentiment polarities.',\n",
              " 'However, this method generates the overall sentiment of text, rather than the precise sentiment towards the cited paper, which is unable to apply directly.',\n",
              " '2.2 Aspect-based Sentiment Analysis Aspect-based sentiment analysis (ABSA) is proposed to define such a task.',\n",
              " 'Usually, ABSA consists of two stages: locating aspects and analyzing sentiment.',\n",
              " 'Some works solve this problem also in a two- stage way, while some jointly.',\n",
              " 'To detect citation span in Wikipedia, Fetahu et al. [13] propose a sequence classification method using a linear chain CRF to decide which text fragments are covered by a citation at the sub-sentence level.',\n",
              " 'Whereas Kaplan et al. [22] detect non-explicit citing sentences that surround an explicit citing sentence, utilizing relational, entity, lexical, and grammatical coherence between them. [25][39]even try to find the most relative sentences in reference paper with the citing sentences.',\n",
              " 'Qazvinian and Radev [29] proposed a method based on probabilistic inference to extract non-explicit citing sentences by modelling the sentences in an article and their lexical similarities as a Markov Random Field tuned to detect the patterns that context data create and employ a Belief Propagation mechanism to detect likely context sentences.',\n",
              " 'Abu-Jbara and Radev [1] determine the citation block by first segmenting the sentences and then classifying each word in the sentence as being inside or outside the citation block.',\n",
              " 'Finally, they aggregate the labels of all the words contained in a segment to assign a label to the whole segment using three different label aggregation rules(majority label of the words, at least one of the words, or all of them).',\n",
              " 'Kaplan et al. [21] proposed a new method based on coreference-chains for extracting citation blocks from research papers.',\n",
              " 'Given aspects, Sun et al. [32] construct an auxiliary sentence from a aspect, and feed the sentence-pair into BERT-based model.',\n",
              " 'Gao et al. [14] utilize three target-dependent variations of the 𝐵𝐸𝑅𝑇𝑏𝑎𝑠𝑒 model.',\n",
              " 'Bai et al. [2] propose a novel relational graph attention network3, which integrates typed syntactic dependency information.',\n",
              " 'As the errors are cumulated in the pipeline, some researchers explore solutions that detect aspects and classify sentiment jointly.',\n",
              " 'Wang et al. [37] propose a latent aspect rating analysis problem that aims at analyzing reviewers’ latent opinions on an entity from several aspects.',\n",
              " 'For a certain entity, they define a set of keywords of aspects and segment reviews into the aspect level.',\n",
              " 'Given as- pect segmentation results, they use a novel latent rating regression model to calculate aspect ratings and corresponding weights.',\n",
              " 'How- ever, Wang et al. ignore the inter-dependencies between words and sentences, which causes great information loss.',\n",
              " 'This class prob- lem is also called aspect-based sentiment analysis (ABSA).',\n",
              " 'Ruder  2https://github.com/allenai/scicite  3https://github.com/muyeby/RGAT-ABSA  Phocus: Picking Valuable Research from a Sea of Citations  , ,  et al. [30] proposes a hierarchical bidirectional LSTM to model the inter-dependencies of sentences within a review.',\n",
              " 'The aspect is represented by the average of its entity and attribute embeddings.',\n",
              " 'Hoang et al. [18] propose to use a sentence pair classifier model from BERT[11] to solve ABSA at sentence and text levels.',\n",
              " 'Hu et al. [19] propose a span-based extract-then-classify framework based on BERT4.',\n",
              " 'Xu et al. [38] build a dataset, ReviewRC5, and extend BERT with an extra tasking-specific layer to tune each task.',\n",
              " 'Wal- laart et al. [36] propose a two-stage algorithm to solve the ABSA for restaurant reviews: predicting the sentiment with a lexicalized domain ontology, and using a neural network with a rotatory at- tention mechanism (LCR-Rot) as a backup algorithm.',\n",
              " 'The order of rotatory attention mechanism operation is changed and the ro- tatory attention mechanism is iterated multiple times.',\n",
              " 'Trusca et al. extend [36] with deep contextual word embeddings and add an extra attention layer to its high-level representations[34].',\n",
              " 'To ad- dress the imbalance issue and utilize the interaction between aspect terms, Luo et al. [24] propose a gradient harmonized and cascaded labelling model based on BERT.',\n",
              " 'Chen et al. [7] utilize directional graph convolutional networks to perform end-to-end ABSA task.',\n",
              " '2.3 Ranking Model The ranking model is based on LambdaMART, which is the boosted tree version of LambdaRank[5].',\n",
              " 'This algorithm solves the gradients of non-smooth cost functions used in ranking models.',\n",
              " 'Burges et al. [4] give a review on RankNet, LambdaRank, and LambdaMART.',\n",
              " 'To illustrate the ranking network, we use 𝑐𝑖 𝑗 to denote the 𝑗-th ci- tation of the 𝑖-th reference paper.',\n",
              " 'Our ranking network receives an matrix of shape ((cid:205)𝑖 𝑛_𝑐𝑖𝑡𝑖, 4), where 4 stands for the feature quater- nion of (au_overlap, n_cit, cit_word, sen_label).',\n",
              " 'Among which cit_word is calculated as the total number of words in 𝑐𝑜𝑛𝑡𝑒𝑥𝑡_𝑎 + 𝑠𝑒𝑛𝑡𝑒𝑛𝑐𝑒 + 𝑐𝑜𝑛𝑡𝑒𝑥𝑡_𝑏.',\n",
              " 'The network calculate a score 𝑠𝑖 𝑗 on each time of citation 𝑐𝑖 𝑗 individually, averaging on duplicate citations to (cid:205)𝑗 𝑠𝑖 𝑗 .',\n",
              " 'Then 𝑠𝑖 is get the score of each reference paper 𝑠𝑖 = used to rank all the reference paper, outputting 𝑟𝑖 .',\n",
              " '1 𝑛_𝑐𝑖𝑡𝑖  2.4 Evaluation Metrics In the academic field, there are journal-level, author-level and paper- level metrics that measure their impacts.',\n",
              " 'The Impact Factor (IF)[26] and CiteScore6 are used to measure the impact of a journal based on the number of times articles cited during a fixed period published by the journal.',\n",
              " 'Besides, Journal Cita- tion Reports (JCR) give ranking for journals7, Eigenfactor scores[3] measure how likely a journal is to be used, and SCImago Journal Rank (SJR)[15] regards the citations issued by more import jour- nals as more important than those issued by less important ones.',\n",
              " 'Whereas Source Normalized Impact per Paper (SNIP)[27] indicates that a single citation is much more important in subject areas where citations are less, and vice versa.',\n",
              " 'Author-level metrics include h-index, g-index, i10-index and so on.',\n",
              " 'H-index also called index ℎ, is proposed by Jorge E.',\n",
              " 'Hirsch[17], and its definition is the number of papers with citation numbers  4https://github.com/huminghao16/SpanABSA 5https://howardhsu.github.io/dataset/ 6https://service.elsevier.com/app/answers/detail/a_id/14880/supporthub/scopus/ 7https://jcr.clarivate.com/jcr/home  Figure 2: the overview of Phocus.',\n",
              " 'higher or equal to ℎ.',\n",
              " 'The g-index is defined as the largest number such that the top 𝑔 articles received together at least 𝑔2 citations[12].',\n",
              " 'Google Scholar proposes the i10-index that is the number of a publication with at least 10 citations.',\n",
              " 'Those metrics are derived from citations and do not reveal the truth among citations.',\n",
              " 'Paper-level metrics are usually the number of citations.',\n",
              " 'Espe- cially, Semantic Scholar makes the first step towards citation classi- fication.',\n",
              " 'It divided citations into 4 classes: highly influential, back- ground, method and results citations[35], using SVM with an RBF kernel and random forests.',\n",
              " 'The features Semantic Scholar use are the total number of direct citations, number of direct citations per section, the total number of indirect citations and number of indirect citations per section, author overlap, is considered help- ful, citation appears in table and caption, 1/number of references, number of paper citations/all citations, the similarity between ab- stracts, PageRank[28], number of total citing papers after transitive closure, and field of the cited paper.',\n",
              " '3 METHODOLOGY As shown in Figure ??, our algorithm consists of 4 stages: pre- processing, calculating factors, evaluating contribution, and prop- agating influential factors.',\n",
              " 'In pre-processing stage, we clean raw data, and obtain simple factors.',\n",
              " 'Complex factors, like sentiment polarity are calculated in second stage.',\n",
              " 'When get all factors needed, we classify citations into four classes and rank all references, and figure out the local contribution factor of each reference.',\n",
              " 'We ini- tialize all new paper to the database with an academic influential factor 1.0, and propagate its impact on references iteratively.',\n",
              " 'The factors extracted from papers are listed out in Table 1  3.1 Pre-processing Given a paper of string format, a series of steps process the raw data for the next stage: parsing, segmentation, and matching.',\n",
              " 'Paring is aimed at dividing the input text into title, authors, sections, and references.',\n",
              " 'We utilize flari8 to parse the title, authors and publish year of the input paper and its references.',\n",
              " 'We segment the input paper into two-level: section level and sentence level.',\n",
              " 'Section seg- mentation is based on keywords matching and classified into three  8https://pypi.org/project/flair/  Table 1: factor list  Table 2: the classifying standards of Phocus.',\n",
              " 'Definition  Ranges  Label  Description  Zhang and Wen et al.',\n",
              " ', ,  Name  cit_id  cit_title cit_author cit_year  au_overlap  sent_id  reference number of a paper in the reference list title of a reference authors of cit_title publish year of cit_title  overlap between authors of cit_title and citing paper id of a sentence  sec_id  section id of a sentence  n_cit  cit_text  context_a  context_b  sen_label  time of cit_id cited in citing paper  text of the sentence that contains the cit_id related sentences previous to cit_text related sentences behind to cit_text  the sentiment citing paper towards cit_ id  positive integer  string list of authors year  [0, 1]  natural number 0: related work introduction 1: main body 2: conclusion  natural number  string  string  string  -1: negative 0: neutral 1: positive  3 2 1 0  extending the work; highly influenced by the work using the work related work negative sentiment towards the work  Figure 3: the propagation rules of influential factors  categories: 0 representing related work, introduction or other back- ground citation; 1 representing main body including methodology, experiments and so on; 2 representing conclusion and other parts.',\n",
              " 'Sentences are segmented using regular expression matching and are then labelled by their ID according to their appearing order.',\n",
              " 'Reference parsing generates title, authors, publish year and even their citation markers in the paper.',\n",
              " 'Given that information, we locate citations in each sentence and match citation markers with their corresponding reference papers.',\n",
              " 'Then we could easily get the factor n_cit and cit_text.',\n",
              " 'Factor au_overlap is calculated according to the following equation:  𝑎𝑢_𝑜𝑣𝑒𝑟𝑙𝑎𝑝 = 2 × 𝐴∩𝐵 |𝐴 |+|𝐵 |  (1)  where A is the author set of citing paper, and B is the author set of reference paper.',\n",
              " '3.2 Calculating Factors There are still three factors unsolved: context_a, context_b, and sen_label.',\n",
              " 'We obtain context_a, context_b with BERT, and propose a novel aspect-based sentiment analysis algorithm to classify citation sentiment.',\n",
              " 'We fine-tune BERT on a manually annotated dataset contain- ing over 1,000 sentence pairs labelled as \"related\" or \"irrelevant\".',\n",
              " 'Each sentence pair is generated from a single academic paper.',\n",
              " 'We get an accuracy of 94.5% on the evaluation dataset.',\n",
              " 'To ob- tain the context of cit_context, we apply the above classifier it- eratively on sentence pair (𝑆 [𝑠𝑒𝑛𝑡_𝑖𝑑 − 𝑖], 𝑆 [𝑠𝑒𝑛𝑡_𝑖𝑑]) (𝑆 represent- ing the list of all sentences in the paper) where 𝑖 increases from 1.',\n",
              " 'Once an \"irrelevant\" pair is reported, the iteration is aborted and we take 𝑆 [𝑠𝑒𝑛𝑡_𝑖𝑑 − 𝑖 : 𝑠𝑒𝑛𝑡_𝑖𝑑] as context_a.',\n",
              " 'Another stop- ping criterion is that 𝑆 [𝑠𝑒𝑛𝑡_𝑖𝑑 − 𝑖] should always be in the same paragraph with 𝑆 [𝑠𝑒𝑛𝑡_𝑖𝑑].',\n",
              " 'A similar procedure is performed on (𝑆 [𝑠𝑒𝑛𝑡_𝑖𝑑 + 𝑖], 𝑆 [𝑠𝑒𝑛𝑡_𝑖𝑑]) to get context_b.',\n",
              " '3.3 Evaluating Contribution After gathering all needed factors, we train a classifier to categorize citation into 4 classes: very important, important, neutral, and terrible.',\n",
              " 'And we also train a ranking model to predict the related order of references in terms of their contributions to the paper.',\n",
              " 'First, we classify citations into four categories with a Naive Bayesian classifier.',\n",
              " 'The classifying standards are shown in Table 2, and a larger number of labels represents more contributions.',\n",
              " 'The ranking model is based on LambdaMART, which is the boosted tree version of LambdaRank[5].',\n",
              " 'This algorithm solves the gradients of non-smooth cost functions used in ranking models.',\n",
              " 'Burges et al. [4] give a review on RankNet, LambdaRank, and Lamb- daMART.',\n",
              " 'Based on the classes and order of references, we project them  into [0, 1] to get their influential factors.',\n",
              " 'Phocus: Picking Valuable Research from a Sea of Citations  , ,  3.4 Propagating Influential Factors Given a list of references and their influential factors of the citing paper, we design some rules to propagate their influence.',\n",
              " 'The main idea is shown in Figure 3.',\n",
              " '𝐴 denote a citing paper with academic influential factor 𝐴𝐹𝐴 initialized as 1, set 𝑅𝐴, 𝐼 𝐹 𝑙 𝐴 denote all references of 𝐴, and their corresponding local contribution to 𝐴, and 𝐼 𝐹 𝑙 𝐴𝑖 ∈ [−1, 1] is the local contribution of reference i to 𝐴. 𝐶𝐴 is the set of all papers that cite 𝐴, and for 𝑗 ∈ 𝐶𝐴, 𝐼 𝐹 𝑙 𝑗𝐴 ∈ [−1, 1] is A’s local contribution to 𝑗.',\n",
              " 'Then, the academic influential factor of 𝐴 is:  𝐴𝐹𝐴 =  ∑︁  𝐴𝐹 𝑗 𝐼 𝐹 𝑙 𝑗𝐴  (2)  𝑗 ∈𝐶𝐴 For author 𝑎 who publishes a set of papers 𝑃𝑎, and his contribution to paper 𝑖 ∈ 𝑃𝑎 is 𝐶𝑖𝑎 ∈ [0, 1], his academic influential factor is: ∑︁  𝐴𝐹𝑎 =  𝐶𝑖𝑎𝐴𝐹𝑖  (3)  𝑖 ∈𝑃𝑎 For paper 𝐴, and its 𝑁 authors, (cid:205)𝑁 𝑖 𝐶𝐴𝑖 ≡ 1.',\n",
              " 'There are two problems to prove to ensure that our method is logical.',\n",
              " 'The first one is margin effects.',\n",
              " 'And the second one the propagation rules.',\n",
              " '4 EXPERIMENTS We conduct several experiments to demonstrate our new metrics that measure the influential factors of an individual scientist or scholar and the citation impact of the publications.',\n",
              " 'As the influential factor of a paper is the weighted sum of all papers that cite it and its corresponding contribution to them, the final and full network of paper and network should be constructed.',\n",
              " 'However, we cannot complete this job yet out of no access to some databases, not enough time or computational resources.',\n",
              " 'We will select some scholars and their publications as targets, and utilize primary citation and secondary citation relationships.',\n",
              " 'Besides, we also compare our modules to other state-of-art algorithms to show the improvement we achieve.',\n",
              " '4.1 Peer Comparison Scholar and their publications.',\n",
              " 'Let Scholar Y denote some scholar.',\n",
              " 'We will show the difference between Scholar Y and the Turing Award winner Pat.',\n",
              " 'Hanrahan9.',\n",
              " 'As we emphasize, Pat.',\n",
              " 'Hanrahan is much more influential than scholar Y is not only for that he wins Turing Award, but also is based on solid statistics of citations.',\n",
              " 'For example, He et al. [16] take one paper of scholar Y as a base- line that performs only better than one baseline among eleven.',\n",
              " 'Table 4 shows evaluation results of scholar Y and Pat.',\n",
              " 'Hanrahan on Aminer10, Google Scholar11, Semantic Scholar12 and Phocus.',\n",
              " 'Table 3 lists the number of publications and citations of scholar Y and Pat.',\n",
              " 'Hanrahan.',\n",
              " 'It’s obviously that scholar Y is more produc- tive than Pat.',\n",
              " 'Hanrahan.',\n",
              " 'However, those numbers covers up some significant truths that not all papers are equal influential and not all citations mean agreement with the cited ones. where h repre- sents h-index, g represents g-index, i10 means i10-index, and HIC  9https://scholar.google.com/citations?hl=zh-CN&user=RzEnQmgAAAAJ 10https://www.aminer.cn/ 11https://scholar.google.com/ 12https://www.semanticscholar.org/  Table 3: statistics of Y and Hanrahan  Scholar  Aminer  Google Scholar  publications citations citations  Y  1146  77903  78663  Hanrahan 381  52214  50568  Semantic Scholar publications citations  771  315  59679  56383  Table 4: evaluation results from several platforms  Scholar  Aminer  Google Scholar  Semantic Scholar  h  131  Y  Hanrahan 97  g  258  228  h  123  93  i10  723  200  h  HIC  119  5843  88  3741  Phocus (Primary)  0.40  0.52  Figure 4: for paper A, paper B and C cite it directly, called primary citations, D to A is secondary and E to A is tertiary.',\n",
              " 'is the number of highly influential citations.',\n",
              " 'H-index, also called index ℎ, is proposed by Jorge E.',\n",
              " 'Hirsch[17], and its definition is the number of papers with citation number higher or equal to ℎ .',\n",
              " 'The g-index is defined as the largest number such that the top 𝑔 articles received together at least 𝑔2 citations[12].',\n",
              " 'Google Scholar proposes i10-index that is the number of a publication with at least 10 cita- tions.',\n",
              " 'Those metrics are derived from citations and do not reveal the truth among citations.',\n",
              " 'Semantic Scholar makes the first step towards citation classification.',\n",
              " 'It divided citations into 4 classes: highly influential, background, method and results citations[35], us- ing SVM with a RBF kernel and random forests.',\n",
              " 'The features Seman- tic Scholar use are total number of direct citations, number of direct citations per section, total number of indirect citations and number of indirect citations per section, author overlap, is considered help- ful, citation appears in table and caption, 1/number of references, number of paper citations/all citations, similarity between abstracts, PageRank[28], number of total citing papers after transitive closure, and field of the cited paper.',\n",
              " 'We collect XX papers that cite scholar Y from 78663, and XX papers that cite Patrick Hanrahan from 56383.',\n",
              " 'Only utilizing primary citations, we get the global academic influ- ential factors of scholar Y and Patrick Hanrahan is 0.40 and 0.52 respectively.',\n",
              " 'Figure 4  4.2 Mathematical Invariance To verify the model, we conduct a series of experiments to prove it’s reasonable.',\n",
              " 'First, given a set of references within a paper, removing anyone reference from the set won’t change the related order of left refer- ences.',\n",
              " 'And when removing a reference at a time, the left references also keep related orders.',\n",
              " ', ,  Zhang and Wen et al.',\n",
              " 'Table 5: features used for citation span  Feature  Description  distance  position  segment  The distance (in words) between the word and the target citaion.',\n",
              " 'This feature takes the value 1 if the word comes before the target citation, and 0 otherwise.',\n",
              " 'After splitting the sentence into segments by punctuation and coordination conjunctions, this feature takes the value 1 if the word occurs in the same segment with the target reference, and 0 otherwise.',\n",
              " 'pos_tag  The part of speech tag of the word, the word before, and the word after.',\n",
              " 'dTreeDistance  Length of the shortest dependency path (in the dependency parse tree) that connects the word to the target reference or its representative.',\n",
              " 'lca  The type of the node in the dependency parse tree that is the least common ancestor of the word and the target reference.',\n",
              " 'Table 6: results for three different models for citation span  Model Precision Recall  F1  SVM LR CRF  0.78 0.68 0.65  0.56 0.67 0.64  0.65 0.67 0.64  Also, the final score should be stable and insensitive to propa- gating order under a certain paper pool.',\n",
              " 'Our strategy starts from a default influential factor 1.0, traversing through each paper and updating the influential factor successively.',\n",
              " 'It is proven through experiments that regardless of the updating order, the final score of each paper remains the same.',\n",
              " '4.3 Citation Span We conduct some experiments guided by [1] as our baseline.',\n",
              " 'We annotate the citation span for about 345 citing sentences as our data set to train and test the baseline model.',\n",
              " 'First, we use the tokenizer tool that SpaCy13 provides to segment the text of each citing sentence into tokens, and use tagger and parser tool to assign part-of-speech-tags and dependency labels to each token.',\n",
              " 'Then, we extract features listed in Table 5. as the input of the baseline model.',\n",
              " 'The training is performed using SVM, Logistic Regression, and CRF, respectively.',\n",
              " 'We use 10-fold cross-validation for training and testing.',\n",
              " 'Table 6 lists the precision, recall, and F1 for the three model.',\n",
              " '13https://spacy.io/  5 RESULTS As shown in Table 4, Phocus figures out that the global academic influential factors of scholar Y and Patrick Hanrahan are 0.40 and 0.52 respectively, and Patrick Hanrahan is 30% higher than scholar Y.',\n",
              " 'It’s the results that only utilize primary citation data.',\n",
              " 'While the evaluation results from Aminer, Google Scholar and even Semantic Scholar shows that scholar Y is more productive and influential than Patrick Hanrahan.',\n",
              " '6 CONCLUSION In this paper, we come up with Phocus, a novel set of academic evaluation metrics for authors and publications based on citation judgements that utilize aspect-based sentiment analysis.',\n",
              " 'To verify our evaluation mechanism, peer comparison and ablation studies have been conducted.',\n",
              " 'The results show that our metrics are able to identify the truly worthiness of a paper or a scholar, which is difficult to citation times based metrics, like h-index, g-index and others.',\n",
              " 'Phocus still need improvements.',\n",
              " 'As shown in Section Exper- iments, we only use primary citation data, which is not enough to fully prove the reliability of Phocus.',\n",
              " 'Besides, using more data such as secondary and tertiary citations could further reflect the gaps between scholars and between metrics.',\n",
              " 'There are still many problems unsolved, such as “citation circles” (groups of researchers who cite one another’s work), and self-citation.',\n",
              " 'REFERENCES [1] Amjad Abu-Jbara and Dragomir Radev. 2012.',\n",
              " 'Reference scope identification in citing sentences.',\n",
              " 'In Proceedings of the 2012 Conference of the North Ameri- can Chapter of the Association for Computational Linguistics: Human Language Technologies. 80–90.',\n",
              " '[2] Xuefeng Bai, Pengbo Liu, and Yue Zhang. 2021.',\n",
              " 'Investigating Typed Syntac- tic Dependencies for Targeted Sentiment Classification Using Graph Attention Neural Network.',\n",
              " 'IEEE/ACM Trans.',\n",
              " 'Audio, Speech and Lang.',\n",
              " 'Proc. 29 (jan 2021), 503–514. https://doi.org/10.1109/TASLP.2020.3042009  [3] Carl T.',\n",
              " 'Bergstrom. 2007.',\n",
              " 'Eigenfactor Measuring the value and prestige of scholarly  journals.',\n",
              " 'College & Research Libraries News 68 (2007), 314–316.',\n",
              " '[4] Christopher J.',\n",
              " 'C.',\n",
              " 'Burges. 2010.',\n",
              " 'From RankNet to LambdaRank to LambdaMART:  An Overview.',\n",
              " '[5] Christopher J.',\n",
              " 'C.',\n",
              " 'Burges, Robert J.',\n",
              " 'Ragno, and Quoc V.',\n",
              " 'Le. 2006.',\n",
              " 'Learning to  Rank with Nonsmooth Cost Functions.',\n",
              " 'In NIPS.',\n",
              " '[6] Bilal Hayat Butt, Muhammad Rafi, Arsal Jamal, Raja Sami Ur Rehman, Syed Muhammad Zubair Alam, and Muhammad Bilal Alam. 2015.',\n",
              " 'Classification of Research Citations (CRC).',\n",
              " 'In CLBib@ISSI.',\n",
              " '[7] Guimin Chen, Yuanhe Tian, and Yan Song. 2020.',\n",
              " 'Joint Aspect Extraction and Sen- timent Analysis with Directional Graph Convolutional Networks.',\n",
              " 'In Proceedings of the 28th International Conference on Computational Linguistics.',\n",
              " 'International Committee on Computational Linguistics, Barcelona, Spain (Online), 272–279. https://doi.org/10.18653/v1/2020.coling-main.24  [8] Johan S.',\n",
              " 'G.',\n",
              " 'Chu and James A.',\n",
              " 'Evans. 2021.',\n",
              " 'Slowed canonical progress in large fields of science.',\n",
              " 'Proceedings of the National Academy of Sciences of the United States of America 118 (2021).',\n",
              " '[9] Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady. 2019.',\n",
              " 'Structural Scaffolds for Citation Intent Classification in Scientific Publications.',\n",
              " 'ArXiv abs/1904.01608 (2019).',\n",
              " '[10] Corinna Cortes and Neil Lawrence. 2021.',\n",
              " 'Inconsistency in Conference Peer Review: Revisiting the 2014 NeurIPS Experiment.',\n",
              " 'ArXiv abs/2109.09774 (2021). [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.',\n",
              " 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.',\n",
              " 'ArXiv abs/1810.04805 (2019).',\n",
              " '[12] Leo Egghe. 2006.',\n",
              " 'Theory and practise of the g-index.',\n",
              " 'Scientometrics 69 (2006),  131–152.',\n",
              " '[13] Besnik Fetahu, Katja Markert, and Avishek Anand. 2017.',\n",
              " 'Fine Grained Citation Span for References in Wikipedia.',\n",
              " 'In Proceedings of the 2017 Conference on Em- pirical Methods in Natural Language Processing.',\n",
              " 'Association for Computational Linguistics, Copenhagen, Denmark, 1990–1999. https://doi.org/10.18653/v1/D17- 1212  Phocus: Picking Valuable Research from a Sea of Citations  , ,  [14] Zhengjie Gao, Ao Feng, Xinyu Song, and Xi Wu. 2019.',\n",
              " 'Target-Dependent Senti-  Processing and Management 16 (1980).',\n",
              " 'ment Classification With BERT.',\n",
              " 'IEEE Access 7 (2019), 154290–154299.',\n",
              " '[15] Borja Gonzalez-Pereira, Vicente Guerrero-Bote, and Felix Moya-Anegon. 2009.',\n",
              " 'The SJR indicator: A new indicator of journals’ scientific prestige. arXiv:0912.4141 [cs.DL]  [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015.',\n",
              " 'Deep Residual  Learning for Image Recognition. arXiv:1512.03385 [cs.CV]  [17] Jorge E.',\n",
              " 'Hirsch. 2005.',\n",
              " 'An index to quantify an individual’s scientific research  output.',\n",
              " 'Proc.',\n",
              " 'Natl.',\n",
              " 'Acad.',\n",
              " 'Sci.',\n",
              " 'USA 102 (2005), 16569–16572.',\n",
              " '[18] Mickel Hoang, Oskar Alija Bihorac, and Jacobo Rouces. 2019.',\n",
              " 'Aspect-Based  Sentiment Analysis using BERT.',\n",
              " 'In NODALIDA.',\n",
              " '[19] Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li, and Yiwei Lv. 2019.',\n",
              " 'Open-Domain Targeted Sentiment Analysis via Span-Based Extraction and Clas- sification.',\n",
              " 'In Proceedings of the 57th Annual Meeting of the Association for Compu- tational Linguistics.',\n",
              " 'Association for Computational Linguistics, Florence, Italy, 537–546. https://doi.org/10.18653/v1/P19-1051  [20] David Jurgens, Srijan Kumar, Raine Hoover, Daniel A.',\n",
              " 'McFarland, and Dan Jurafsky. 2016.',\n",
              " 'Citation Classification for Behavioral Analysis of a Scientific Field.',\n",
              " 'ArXiv abs/1609.00435 (2016).',\n",
              " '[21] Dain Kaplan, Ryu Iida, and Takenobu Tokunaga. 2009.',\n",
              " 'Automatic extraction of citation contexts for research paper summarization: A coreference-chain based approach.',\n",
              " 'In Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries (NLPIR4DL). 88–95.',\n",
              " '[22] Dain Kaplan, Takenobu Tokunaga, and Simone Teufel. 2016.',\n",
              " 'Citation Block Determination Using Textual Coherence.',\n",
              " 'Journal of Information Processing 24, 3 (2016), 540–553. https://doi.org/10.2197/ipsjjip.24.540  [23] Haixia Liu. 2017.',\n",
              " 'Sentiment Analysis of Citations Using Word2vec.',\n",
              " 'ArXiv  abs/1704.00177 (2017).',\n",
              " '[24] Huaishao Luo, Lei Ji, Tianrui Li, Daxin Jiang, and Nan Duan. 2020.',\n",
              " 'GRACE: Gradient Harmonized and Cascaded Labeling for Aspect-based Sentiment Anal- ysis.',\n",
              " 'In Findings of the Association for Computational Linguistics: EMNLP 2020.',\n",
              " 'Association for Computational Linguistics, Online, 54–64. https://doi.org/10. 18653/v1/2020.findings-emnlp.6  [25] Shutian Ma, Jin Xu, and Chengzhi Zhang. 2018.',\n",
              " 'Automatic Identification of Cited Text Spans: A Multi-Classifier Approach over Imbalanced Dataset.',\n",
              " 'Scientometrics 116, 2 (aug 2018), 1303–1330. https://doi.org/10.1007/s11192-018-2754-2 [26] Jessica L.',\n",
              " 'Milstead. 1980.',\n",
              " 'Citation Indexing—Its Theory and Application in Science, Information  Technology and Humanities.',\n",
              " 'Wiley, Oxford (1979), 274, $15.95.',\n",
              " '[27] Henk F.',\n",
              " 'Moed. 2010.',\n",
              " 'Measuring contextual citation impact of scientific journals.',\n",
              " 'Journal of Informetrics 4, 3 (2010), 265–277. https://doi.org/10.1016/j.joi.2010.01. 002  [28] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999.',\n",
              " 'The PageRank Citation Ranking: Bringing Order to the Web.',\n",
              " 'Technical Report 1999- 66.',\n",
              " 'Stanford InfoLab. http://ilpubs.stanford.edu:8090/422/ Previous number = SIDL-WP-1999-0120.',\n",
              " '[29] Vahed Qazvinian and Dragomir Radev. 2010.',\n",
              " 'Identifying Non-Explicit Citing Sentences for Citation-Based Summarization..',\n",
              " 'In Proceedings of the 48th annual meeting of the association for computational linguistics. 555–564.',\n",
              " '[30] Sebastian Ruder, Parsa Ghaffari, and John G.',\n",
              " 'Breslin. 2016.',\n",
              " 'A Hierarchical Model  of Reviews for Aspect-based Sentiment Analysis.',\n",
              " 'In EMNLP.',\n",
              " '[31] Per O.',\n",
              " 'Seglen. 1997.',\n",
              " 'Why the impact factor of journals should not be used for  evaluating research.',\n",
              " 'BMJ 314 (1997), 497.',\n",
              " '[32] Chi Sun, Luyao Huang, and Xipeng Qiu. 2019.',\n",
              " 'Utilizing BERT for Aspect-Based  Sentiment Analysis via Constructing Auxiliary Sentence.',\n",
              " 'In NAACL.',\n",
              " '[33] Simone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006.',\n",
              " 'Automatic classifica-  tion of citation function.',\n",
              " 'In EMNLP.',\n",
              " '[34] Maria Mihaela Trusca, Daan Wassenberg, Flavius Frasincar, and Rommert Dekker. 2020.',\n",
              " 'A Hybrid Approach for Aspect-Based Sentiment Analysis Using Deep Contextual Word Embeddings and Hierarchical Attention.',\n",
              " 'In ICWE.',\n",
              " '[35] Marco Valenzuela, Vu A.',\n",
              " 'Ha, and Oren Etzioni. 2015.',\n",
              " 'Identifying Meaningful  Citations.',\n",
              " 'In AAAI Workshop: Scholarly Big Data.',\n",
              " '[36] Olaf Wallaart and Flavius Frasincar. 2019.',\n",
              " 'A Hybrid Approach for Aspect-Based Sentiment Analysis Using a Lexicalized Domain Ontology and Attentional Neural Models.',\n",
              " 'In ESWC.',\n",
              " '[37] Hongning Wang, Yue Lu, and ChengXiang Zhai. 2010.',\n",
              " 'Latent aspect rating analysis on review text data: a rating regression approach.',\n",
              " 'Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining (2010).',\n",
              " '[38] Hu Xu, Bing Liu, Lei Shu, and Philip S.',\n",
              " 'Yu. 2019.',\n",
              " 'BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis.',\n",
              " 'In NAACL. [39] Chrysoula Zerva, Minh quoc Nghiem, Nhung T.',\n",
              " 'H.',\n",
              " 'Nguyen, and Sophia Anani- adou. 2020.',\n",
              " 'Cited text span identification for scientific summarisation using pre- trained encoders.',\n",
              " 'Scientometrics (7 May 2020). https://doi.org/10.1007/s11192- 020-03455-z  .']"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O9lsV2ij-2vn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "input.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}