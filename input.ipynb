{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h7Lr7k5d1jd"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/ENEE439d-TEXTML/TextML/blob/master/input.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/ENEE439d-TEXTML/TextML/blob/master/input.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPiicLOUd1jj"
      },
      "source": [
        "## Imports and datset setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \"tensorflow-text==2.8.*\" # A dependency of the preprocessing for BERT inputs\n",
        "!pip install -q tf-models-official==2.7.0 # For adamW\n",
        "!pip install focal-loss # focal loss implmnetion for tf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52FpqTzrPr-t",
        "outputId": "aa3191f3-05e4-4c84-96fb-3037c7bbed1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: focal-loss in /usr/local/lib/python3.7/dist-packages (0.0.7)\n",
            "Requirement already satisfied: tensorflow>=2.2 in /usr/local/lib/python3.7/dist-packages (from focal-loss) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.2.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (4.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (57.4.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (13.0.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.14.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (0.24.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.1.2)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.44.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2->focal-loss) (1.0.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2->focal-loss) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.2->focal-loss) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (1.35.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2->focal-loss) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "BPnv0Vlcd3KI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd #basic imports\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split # https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization, bert  # to create AdamW optimizer\n",
        "from focal_loss import SparseCategoricalFocalLoss\n",
        "import official.nlp.bert.tokenization"
      ],
      "metadata": {
        "id": "dkxbtcKbP4AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "465BADPxeNII",
        "outputId": "5c41e75f-656e-4f4e-dc4e-382c081c81e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Sk1zTIA6eOM5",
        "outputId": "f85ece6d-48c3-4925-ede7-7b5f3fda401e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  cited_paper  label                                               text\n",
              "0    A00-2024      0  We analyzed a set of articles and identified s...\n",
              "1    A00-2024      0  Table 3: Example compressions Compression AvgL...\n",
              "2    A00-2024      0  5.3 Related works and discussion Our two-step ...\n",
              "3    A00-2024      0  (1999) proposed a summarization system based o...\n",
              "4    A00-2024      0  We found that the deletion of lead parts did n..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0b1d34ce-7417-4d1d-b1e5-5ceb7d292269\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cited_paper</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We analyzed a set of articles and identified s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>Table 3: Example compressions Compression AvgL...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>5.3 Related works and discussion Our two-step ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>(1999) proposed a summarization system based o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We found that the deletion of lead parts did n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b1d34ce-7417-4d1d-b1e5-5ceb7d292269')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0b1d34ce-7417-4d1d-b1e5-5ceb7d292269 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0b1d34ce-7417-4d1d-b1e5-5ceb7d292269');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "filepath = '/content/drive/MyDrive/Text-ML/full_sentiment_dataset.csv' #'data.csv'\n",
        "df= pd.read_csv(filepath)\n",
        "df1=df.drop(['no','paper','context_a','context_b'],axis=1)\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImSOYF6Py2J8",
        "outputId": "97e66b45-a1e2-4599-a30b-393f76383cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 0    7627\n",
              " 1     829\n",
              "-1     280\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ytc03_FI-b5L",
        "outputId": "9cca8654-8558-4b1f-d532-5e1d3fde30b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\[*\\]|\\(\\d{4}\\)|\\(?((\\(?((\\w+, )*(\\w+ )+)((and|&) ((\\w+ *))?)?,? \\(?\\d{4}\\)?|((\\w+, )*(\\w+ )+)et al\\. ?,? \\(?\\d{4}\\)?\\)?)((; )|( (and|&) ))*)+\n"
          ]
        }
      ],
      "source": [
        "context=df1['text']\n",
        "\n",
        "re1= \"\\(((([A-Za-z]+ *)+(, \\d+))+(; )*)+\\)\" # matches author and author, year\n",
        "re_year=\",? \\(?\\d{4}\\)?\" # match , {4 digits} which may be wrapped in () \n",
        "re_and=\"(and|&) \"\n",
        "re_auth=\"((\\w+, )*(\\w+ )+)\"\n",
        "re_et= re_auth+\"et al\\. ?\"+re_year # matches author et al. , year\n",
        "re_2a= re_auth+\"(\"+re_and+\"((\\w+ *))?)?\"+re_year # matches author and author, year\n",
        "re_sep=\"((; )|( \"+re_and+\"))*\"# match the '; ' gap or ' and ' gap\n",
        "re_para_year=\"\\(\\d{4}\\)\"\n",
        "re_in_brack=\"\\[*\\]\"\n",
        "re_apa =re_in_brack+\"|\"+re_para_year+\"|\"+\"\\(?(\"+\"(\\(?\"+re_2a+\"|\"+re_et+\"\\)?)\"+re_sep+\")+\"\n",
        "print(re_apa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEZIAWQtykDg"
      },
      "outputs": [],
      "source": [
        "def remove_matches(text,regex=re_apa):\n",
        "  text1=text\n",
        "  rem_len=0\n",
        "  pattern= re.compile(regex)\n",
        "  while True:\n",
        "    matches=pattern.search(text1)\n",
        "    #print(matches)\n",
        "    if matches == None:\n",
        "      break\n",
        "\n",
        "    spn=matches.span()\n",
        "    text1=text1[0:spn[0]]+text1[spn[1]:-1]\n",
        "    cit_len=spn[1]-spn[0]\n",
        "    rem_len+=cit_len\n",
        "  \n",
        "  if len(text) >0:\n",
        "    percent_removed=rem_len/len(text)\n",
        "  else:\n",
        "    percent_removed=1\n",
        "  return text1,percent_removed \n",
        "\n",
        "# print(context[5])\n",
        "# remove_citation(context[5],regex=re_apa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmjjH1gp9A6T"
      },
      "outputs": [],
      "source": [
        "output=df1['text'].apply(lambda x: remove_matches(text=x,regex=re_apa)) #df['col1'] = df.apply(lambda x: complex_function(x['col1']), axis=1)\n",
        "df_o = pd.DataFrame(list(output), columns =['clean','p_rem'])\n",
        "output_1=df_o['clean'].apply(lambda x: remove_matches(text=x,regex='[^\\w_0-9 ]+')) \n",
        "df_o_1 = pd.DataFrame(list(output_1), columns =['clean','p_rem'])\n",
        "#df_o.head()\n",
        "\n",
        "df1['text_clean']=df_o_1['clean']\n",
        "df1['text_clean_len']=df_o_1['clean'].apply(len)\n",
        "df1['p_rem']=df_o['p_rem']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "OVt3NtaJDDrx",
        "outputId": "ef3a3df4-5472-4588-fbc0-75b8ba537d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     cited_paper  label                                               text  \\\n",
              "0       A00-2024      0  We analyzed a set of articles and identified s...   \n",
              "1       A00-2024      0  Table 3: Example compressions Compression AvgL...   \n",
              "2       A00-2024      0  5.3 Related works and discussion Our two-step ...   \n",
              "3       A00-2024      0  (1999) proposed a summarization system based o...   \n",
              "4       A00-2024      0  We found that the deletion of lead parts did n...   \n",
              "...          ...    ...                                                ...   \n",
              "8731    W96-0213      1  He has achieved state-of-the art results by ap...   \n",
              "8732    W96-0213      0  B = (Brill and Wu, 1998); M = (Magerman, 1995)...   \n",
              "8733    W96-0213      0  The model we use is similar to that of (Ratnap...   \n",
              "8734    W96-0213      1  Our model exploits the same kind of tag-n-gram...   \n",
              "8735    W96-0213      0  In that table, TBL stands for Brill's transfor...   \n",
              "\n",
              "                                             text_clean  text_clean_len  \\\n",
              "0     We analyzed a set of articles and identified s...             425   \n",
              "1     Table 3 Example compressions Compression AvgLe...             229   \n",
              "2     53 Related works and discussion Our twostep mo...             105   \n",
              "3      proposed a summarization system based on the ...             321   \n",
              "4     We found that the deletion of lead parts did n...              73   \n",
              "...                                                 ...             ...   \n",
              "8731  He has achieved stateofthe art results by appl...             139   \n",
              "8732   B  M  Magerman 1995 O  our data R  Ratnaparkhi 1              48   \n",
              "8733  The model we use is similar to that of Ratnapa...              55   \n",
              "8734  Our model exploits the same kind of tagngram i...             157   \n",
              "8735  In that table TBL stands for Brills transforma...             288   \n",
              "\n",
              "         p_rem  \n",
              "0     0.098765  \n",
              "1     0.260745  \n",
              "2     0.308176  \n",
              "3     0.078804  \n",
              "4     0.408000  \n",
              "...        ...  \n",
              "8731  0.151515  \n",
              "8732  0.421488  \n",
              "8733  0.000000  \n",
              "8734  0.000000  \n",
              "8735  0.000000  \n",
              "\n",
              "[8736 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f443bba3-fbed-4ad8-b146-061361805bb3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cited_paper</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>text_clean</th>\n",
              "      <th>text_clean_len</th>\n",
              "      <th>p_rem</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We analyzed a set of articles and identified s...</td>\n",
              "      <td>We analyzed a set of articles and identified s...</td>\n",
              "      <td>425</td>\n",
              "      <td>0.098765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>Table 3: Example compressions Compression AvgL...</td>\n",
              "      <td>Table 3 Example compressions Compression AvgLe...</td>\n",
              "      <td>229</td>\n",
              "      <td>0.260745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>5.3 Related works and discussion Our two-step ...</td>\n",
              "      <td>53 Related works and discussion Our twostep mo...</td>\n",
              "      <td>105</td>\n",
              "      <td>0.308176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>(1999) proposed a summarization system based o...</td>\n",
              "      <td>proposed a summarization system based on the ...</td>\n",
              "      <td>321</td>\n",
              "      <td>0.078804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A00-2024</td>\n",
              "      <td>0</td>\n",
              "      <td>We found that the deletion of lead parts did n...</td>\n",
              "      <td>We found that the deletion of lead parts did n...</td>\n",
              "      <td>73</td>\n",
              "      <td>0.408000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8731</th>\n",
              "      <td>W96-0213</td>\n",
              "      <td>1</td>\n",
              "      <td>He has achieved state-of-the art results by ap...</td>\n",
              "      <td>He has achieved stateofthe art results by appl...</td>\n",
              "      <td>139</td>\n",
              "      <td>0.151515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8732</th>\n",
              "      <td>W96-0213</td>\n",
              "      <td>0</td>\n",
              "      <td>B = (Brill and Wu, 1998); M = (Magerman, 1995)...</td>\n",
              "      <td>B  M  Magerman 1995 O  our data R  Ratnaparkhi 1</td>\n",
              "      <td>48</td>\n",
              "      <td>0.421488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8733</th>\n",
              "      <td>W96-0213</td>\n",
              "      <td>0</td>\n",
              "      <td>The model we use is similar to that of (Ratnap...</td>\n",
              "      <td>The model we use is similar to that of Ratnapa...</td>\n",
              "      <td>55</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8734</th>\n",
              "      <td>W96-0213</td>\n",
              "      <td>1</td>\n",
              "      <td>Our model exploits the same kind of tag-n-gram...</td>\n",
              "      <td>Our model exploits the same kind of tagngram i...</td>\n",
              "      <td>157</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8735</th>\n",
              "      <td>W96-0213</td>\n",
              "      <td>0</td>\n",
              "      <td>In that table, TBL stands for Brill's transfor...</td>\n",
              "      <td>In that table TBL stands for Brills transforma...</td>\n",
              "      <td>288</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8736 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f443bba3-fbed-4ad8-b146-061361805bb3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f443bba3-fbed-4ad8-b146-061361805bb3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f443bba3-fbed-4ad8-b146-061361805bb3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove under and over sized samples\n",
        "large samples appear to be poorly written"
      ],
      "metadata": {
        "id": "pkNPWqAHKLxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getMidLen(data,label,labelKey='label',lenKey='text_clean_len',lowMod=1,highMod=1):\n",
        "  df1 =data.loc[data[labelKey] == label]\n",
        "  neu_mean=np.mean(list(df1[lenKey]))\n",
        "  neu_std=np.std(list(df1[lenKey]))\n",
        "  df1_no_high = df1.loc[df1[lenKey] < highMod*(neu_mean +neu_std)]\n",
        "  # print(neu_mean)\n",
        "  # print(neu_std)\n",
        "\n",
        "  while neu_std > neu_mean:\n",
        "    neu_mean=np.mean(list(df1_no_high['text_clean_len']))\n",
        "    neu_std=np.std(list(df1_no_high['text_clean_len']))\n",
        "    # print(neu_mean)\n",
        "    # print(neu_std)\n",
        "    df1_no_high = df1.loc[df1['text_clean_len'] < highMod*(neu_mean +neu_std)]\n",
        "\n",
        "  df1_mid = df1_no_high.loc[df1_no_high['text_clean_len'] > lowMod*(neu_mean -neu_std)]\n",
        "\n",
        "  return df1_mid\n",
        "\n",
        "df2 = df1.loc[df1['p_rem'] < .5] #keep sampels with less than half of it are citation\n",
        "\n",
        "df_neu=getMidLen(df2,0,lowMod=2)\n",
        "df_pos=getMidLen(df2,1,lowMod=1,highMod=2)\n",
        "df_neg=getMidLen(df2,-1,lowMod=1,highMod=2)\n",
        "df3= pd.concat([df_neg,df_neu,df_pos])\n",
        "df3['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq-BsMfleXID",
        "outputId": "e70399ca-6212-43b1-d6f1-977448881c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 0    2524\n",
              " 1     746\n",
              "-1     246\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def catagorize(data,labelKey='label'):\n",
        "  rows=len(data.index)\n",
        "  onehots=np.zeros((rows,3),dtype=int)\n",
        "  for i,lab in enumerate(data[labelKey]):\n",
        "    onehots[i][lab+1]=1\n",
        "  return onehots\n",
        "\n",
        "hots=catagorize(df3)\n",
        "df3['label_onehot']=list(hots)\n",
        "df3['label_index']=df3['label']+1"
      ],
      "metadata": {
        "id": "VnsKnaB0AU4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq= np.array(list(df3['label_index'].value_counts(normalize=True,sort=False)))\n",
        "print(freq)\n",
        "class_ratio= 1/freq\n",
        "class_ratio"
      ],
      "metadata": {
        "id": "Mbn2dX1C2Ixs",
        "outputId": "450102aa-de64-4a27-bd4d-998df04080d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.06996587 0.71786121 0.21217292]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([14.29268293,  1.39302694,  4.71313673])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(list(df3['text_clean']), list(df3['label_index']), test_size=0.2, random_state=42)\n",
        "X_train= [[s] for s in X_train]\n",
        "X_test= [[s] for s in X_test]\n",
        "y_train=[[s] for s in list(y_train)]\n",
        "y_test=[[s] for s in list(y_test)]"
      ],
      "metadata": {
        "id": "kKtJq5Baj1Wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train"
      ],
      "metadata": {
        "id": "vLqCA9-R0wxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Choose a BERT model to fine-tune (Taken from tutorial)\n",
        "\n",
        "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
        "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
      ],
      "metadata": {
        "id": "9WK-J5dQpprm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b2b94c-960f-4822-e968-17df9e629824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ],
      "metadata": {
        "id": "YINTv-Uu8HP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_test = X_train[1]\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
        "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
        "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
        "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGAgJwGg08-j",
        "outputId": "5aada793-baf2-44f6-b36e-f4f18741f100"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys       : ['input_word_ids', 'input_type_ids', 'input_mask']\n",
            "Shape      : (1, 128)\n",
            "Word Ids   : [  101  1999  5688 11416  6024  4275  2024  4738  2000 25845  1996  4101]\n",
            "Input Mask : [1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "metadata": {
        "id": "U2o1JW9b9MHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
        "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
        "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
        "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
        "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW0iEt6Z9PKt",
        "outputId": "9336fadd-5d53-44d2-cb27-b5d6362bb523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
            "Pooled Outputs Shape:(1, 512)\n",
            "Pooled Outputs Values:[ 0.99835694 -0.7678578  -0.21300124  0.08411987 -0.08593949  0.98550844\n",
            "  0.9732349  -0.8306031  -0.55687106 -0.95725054 -0.3960305  -0.94115573]\n",
            "Sequence Outputs Shape:(1, 128, 512)\n",
            "Sequence Outputs Values:[[ 3.8915187e-01  2.3267061e-01  6.1780311e-02 ... -9.2866874e-01\n",
            "   4.3027106e-01  8.3279318e-01]\n",
            " [ 4.5310837e-01  7.2308904e-01 -3.2866317e-01 ... -1.1163950e-04\n",
            "  -3.3482751e-01  5.7274055e-01]\n",
            " [-2.5876865e-01  1.4199525e+00 -4.2525381e-01 ...  4.9038833e-01\n",
            "   1.4491324e-01  5.7048750e-01]\n",
            " ...\n",
            " [ 2.6529512e-01 -2.3668993e-01  8.4921330e-02 ...  2.0211086e-02\n",
            "   3.9103544e-01  8.9449620e-01]\n",
            " [ 2.9499257e-01  5.8424294e-01 -6.7344594e-01 ... -1.6148384e+00\n",
            "   1.3211843e+00 -6.0517174e-01]\n",
            " [-1.8697177e-01  5.2527428e-01  9.2377967e-01 ... -5.6088662e-01\n",
            "   1.0293359e+00 -7.8856331e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "  encoder_inputs = preprocessing_layer(text_input)\n",
        "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "  outputs = encoder(encoder_inputs)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(3, activation='softmax', name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "metadata": {
        "id": "XWtmKUBu_3kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model = build_classifier_model()"
      ],
      "metadata": {
        "id": "Z17Cu4Awc3jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_raw_result = classifier_model(tf.constant(text_test))\n",
        "print(bert_raw_result)\n",
        "\n",
        "l =  SparseCategoricalFocalLoss(gamma=2,class_weight=class_ratio)\n",
        "test =tf.convert_to_tensor([1.0])\n",
        "l(test,bert_raw_result)"
      ],
      "metadata": {
        "id": "jHqpunBDTKym",
        "outputId": "7eeee8ba-7e62-496d-8afe-b81bced0b546",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[0.4861831  0.43295795 0.08085901]], shape=(1, 3), dtype=float32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.37495145>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "steps_per_epoch = 200 #tf.data.experimental.cardinality(X_train).numpy()\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.1*num_train_steps)\n",
        "\n",
        "init_lr = 2e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')\n",
        "\n",
        "# def auc_wrapper(y_true,y_pred):\n",
        "#   print(y_true,y_pred)\n",
        "\n",
        "#   y_true=tf.reshape(y_true,[1])\n",
        "#   print(y_true)\n",
        "#   y_true= tf.cast(y_true, tf.int32)\n",
        "#   print(y_true)\n",
        "#   y_true=tf.one_hot(y_true,depth=3)\n",
        "#   print(y_true)\n",
        "#   return tf.keras.metrics.AUC(multi_label=True)(y_true,y_pred)\n",
        "\n",
        "\n",
        "loss =  SparseCategoricalFocalLoss(gamma=2,class_weight=class_ratio) #tf.keras.losses.MeanSquaredError()\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]#, auc_wrapper]#, tf.keras.metrics.AUC(multi_label=True)]\n",
        "\n",
        "\n",
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"citation_BERT_{epoch}\",\n",
        "        save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
        "        monitor=\"val_sparse_categorical_accuracy\",\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.TensorBoard('./logs', update_freq=1)\n",
        "]\n"
      ],
      "metadata": {
        "id": "MB9Af5RMCuqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f'Training model with {tfhub_handle_encoder}')\n",
        "# history = classifier_model.fit(x=X_train,y=y_train, validation_data=(X_test,y_test),epochs=epochs,callbacks= callbacks, verbose=True)"
      ],
      "metadata": {
        "id": "89v_3XqEE3HN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classifier_model.save_weights(''/content/drive/MyDrive/Text-ML/checkpoint1')"
      ],
      "metadata": {
        "id": "Pox0n2xZjdtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "00eGEQhDdMUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%tensorboard --logdir=logs"
      ],
      "metadata": {
        "id": "Gszh9ZNBbQXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1\n"
      ],
      "metadata": {
        "id": "aSNvnkyoJdqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.layers"
      ],
      "metadata": {
        "id": "PR_gDYEQofck",
        "outputId": "69a54e8e-c471-4c07-d64f-319b1f2a37b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.engine.input_layer.InputLayer at 0x7f9151ed3e90>,\n",
              " <tensorflow_hub.keras_layer.KerasLayer at 0x7f9151ee8b10>,\n",
              " <tensorflow_hub.keras_layer.KerasLayer at 0x7f915242de50>,\n",
              " <keras.layers.core.dropout.Dropout at 0x7f914d8d8d10>,\n",
              " <keras.layers.core.dense.Dense at 0x7f914b96d290>]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model.load_weights('/content/drive/MyDrive/Text-ML/checkpoint')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMRXTmWvmBNn",
        "outputId": "bc40f09a-4265-4adc-a8a2-4e30ba43d570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f914d946690>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocesser_model = keras.Model(inputs=classifier_model.input,\n",
        "                                       outputs=classifier_model.get_layer('preprocessing').output)\n",
        "\n",
        "encoder_model = keras.Model(inputs=classifier_model.input,\n",
        "                                       outputs=classifier_model.get_layer('BERT_encoder').output)\n",
        "\n",
        "text_test = X_train[1]\n",
        "test_tensor=tf.convert_to_tensor(text_test)\n",
        "print(\"input:\",test_tensor)\n",
        "\n",
        "p_out=preprocesser_model(test_tensor)\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "print(text_preprocessed[\"input_mask\"])\n",
        "stop_index=0\n",
        "while(text_preprocessed[\"input_mask\"][0][stop_index] == 1):\n",
        "  stop_index+=1\n",
        "print(\"stop id index:\",stop_index)\n",
        "\n",
        "#when using the tokenizer below it ignores the sep and clr \n",
        "input_id_list = text_preprocessed[\"input_word_ids\"][:,1:stop_index-1]\n",
        "print(input_id_list)\n",
        "\n",
        "output = encoder_model(test_tensor)\n",
        "print(output[\"sequence_output\"].shape)\n",
        "valid_entries=output[\"sequence_output\"][:,1:stop_index-1,:]\n",
        "#print(valid_entries)\n",
        "a=tf.math.reduce_mean(valid_entries,-1)\n",
        "print(a)\n",
        "mean=tf.math.reduce_mean(a,-1,keepdims=True)\n",
        "print(mean)\n",
        "std=tf.math.reduce_std(a,-1,keepdims=True)\n",
        "print(std)\n",
        "a1=(a-mean)/std\n",
        "print(a1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPAYp3xOmHbQ",
        "outputId": "c96040df-8c00-4e99-826a-b75300c2d8e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: tf.Tensor([b'In contrast generative models are trained to maximize the joint probability of the training  used transformationbased learning Brill 1995 which for the present purposes can be tought of as a classi cationbased '], shape=(1,), dtype=string)\n",
            "tf.Tensor(\n",
            "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 128), dtype=int32)\n",
            "stop id index: 43\n",
            "tf.Tensor(\n",
            "[[ 1999  5688 11416  6024  4275  2024  4738  2000 25845  1996  4101  9723\n",
            "   1997  1996  2731  2109  8651 15058  2094  4083  7987  8591  2786  2029\n",
            "   2005  1996  2556  5682  2064  2022  7823  2102  1997  2004  1037  2465\n",
            "   2072  4937  3258 15058  2094]], shape=(1, 41), dtype=int32)\n",
            "(1, 128, 512)\n",
            "tf.Tensor(\n",
            "[[-0.01570772 -0.01725309 -0.01938405 -0.01798658 -0.01953679 -0.0186075\n",
            "  -0.02059441 -0.01945992 -0.02168891 -0.01995731 -0.02220257 -0.02151649\n",
            "  -0.01957512 -0.0190657  -0.0206007  -0.01782477 -0.02032277 -0.01809545\n",
            "  -0.01939184 -0.01964385 -0.0191996  -0.02081162 -0.01842325 -0.01786033\n",
            "  -0.01734094 -0.01760162 -0.01826956 -0.01829259 -0.02058168 -0.0192893\n",
            "  -0.02093591 -0.02179031 -0.01840142 -0.01680697 -0.01782913 -0.01782577\n",
            "  -0.01824836 -0.01903335 -0.01998218 -0.01794847 -0.01970354]], shape=(1, 41), dtype=float32)\n",
            "tf.Tensor([[-0.01913638]], shape=(1, 1), dtype=float32)\n",
            "tf.Tensor([[0.00144418]], shape=(1, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 2.374119    1.3040543  -0.17149661  0.7961615  -0.2772607   0.36621532\n",
            "  -1.0095894  -0.22403492 -1.7674632  -0.568443   -2.123136   -1.648074\n",
            "  -0.30379745  0.0489386  -1.0139437   0.90819895 -0.8214962   0.72076994\n",
            "  -0.17689039 -0.3513947  -0.04377956 -1.1599959   0.4937918   0.88357484\n",
            "   1.2432189   1.0627172   0.60021234  0.58426446 -1.0007752  -0.10588659\n",
            "  -1.246055   -1.837675    0.50891036  1.6129591   0.90518093  0.90750504\n",
            "   0.6148949   0.07133655 -0.5856626   0.82254606 -0.39271986]], shape=(1, 41), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from official.nlp import bert \n",
        "import official.nlp.bert.tokenization"
      ],
      "metadata": {
        "id": "bC2uW7zrx7VR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = bert.tokenization.FullTokenizer(\n",
        "    vocab_file='/content/drive/MyDrive/Text-ML/vocab.txt')\n"
      ],
      "metadata": {
        "id": "Lnb_ie7zxpbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocab size:\", len(tokenizer.vocab))\n"
      ],
      "metadata": {
        "id": "mjgJ0WAwRjYb",
        "outputId": "0c2c59c0-0e61-4d8c-eec8-ce6f6c2f0898",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 30522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.tokenize(text_test[0]) \n",
        "t_tok=tf.convert_to_tensor(tokens)\n",
        "\n",
        "indicies=np.ones((len(tokens)),dtype=int)\n",
        "for i,tok in enumerate(tokens):\n",
        "  if '##' in tok:\n",
        "    indicies[i]=0\n",
        "print(indicies)\n",
        "\n",
        "tokens[3][2:]\n",
        "full_words=tokens.copy()\n",
        "a2=((a-mean)/std).numpy()\n",
        "ix=-1\n",
        "for i,tok in enumerate(tokens):\n",
        "  if not indicies[i]:\n",
        "    a2[0][ix]+=a2[0][i]\n",
        "    full_words[ix]+=tok[2:]\n",
        "  else:\n",
        "    ix=i\n",
        "\n",
        "print(full_words)\n",
        "\n",
        "t_full=tf.boolean_mask(tf.convert_to_tensor(full_words,dtype=tf.string),indicies)\n",
        "t_a=tf.boolean_mask(tf.convert_to_tensor(a2),[indicies]).numpy()\n",
        "t_full\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "Hs4VnwCiy0gC",
        "outputId": "3c3581d2-cebc-4066-e913-902cdc4ea174",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0\n",
            " 1 0 0 0]\n",
            "['in', 'contrast', 'generative', '##tive', 'models', 'are', 'trained', 'to', 'maximize', 'the', 'joint', 'probability', 'of', 'the', 'training', 'used', 'transformationbased', '##base', '##d', 'learning', 'brill', '##ill', '1995', 'which', 'for', 'the', 'present', 'purposes', 'can', 'be', 'tought', '##t', 'of', 'as', 'a', 'classi', '##i', 'cationbased', '##ion', '##base', '##d']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(32,), dtype=string, numpy=\n",
              "array([b'in', b'contrast', b'generative', b'models', b'are', b'trained',\n",
              "       b'to', b'maximize', b'the', b'joint', b'probability', b'of',\n",
              "       b'the', b'training', b'used', b'transformationbased', b'learning',\n",
              "       b'brill', b'1995', b'which', b'for', b'the', b'present',\n",
              "       b'purposes', b'can', b'be', b'tought', b'of', b'as', b'a',\n",
              "       b'classi', b'cationbased'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_full[0].numpy()\n"
      ],
      "metadata": {
        "id": "yDhvZkOFMFJ3",
        "outputId": "7812982c-eefb-48ab-95f1-f80809ee08b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"b'in'\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def coloring(text,fore=None,back=None):\n",
        "    txt=text\n",
        "    if fore != None:\n",
        "      txt = \"\\033[38;2;{};{};{}m\".format(fore[0], fore[1], fore[2])+txt\n",
        "    if back != None:\n",
        "      txt = \"\\033[48;2;{};{};{}m\".format(back[0], back[1], back[2])+txt\n",
        "    return txt\n",
        "\n",
        "print(coloring('Hello',back=(255,0,0)) + coloring('Hello', back=(0,0,255)))"
      ],
      "metadata": {
        "id": "X8GvqhLvOR8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5534c82c-e0fe-445b-ee0e-daacbf52a998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[48;2;255;0;0mHello\u001b[48;2;0;0;255mHello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing PDF extraction and writing\n"
      ],
      "metadata": {
        "id": "TD_FQ6ioGFGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2 #package to read in"
      ],
      "metadata": {
        "id": "okVJyqQIGNGI",
        "outputId": "65463818-3c72-45e0-d564-fc92934345ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading PyPDF2-1.27.12-py3-none-any.whl (80 kB)\n",
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 20 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 30 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 40 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 51 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 61 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 71 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 80 kB 3.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-1.27.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2"
      ],
      "metadata": {
        "id": "-og96jZiHYBr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path='/content/drive/MyDrive/Text-ML/phocus.pdf'\n",
        "reader = PyPDF2.PdfFileReader(path)"
      ],
      "metadata": {
        "id": "edHCPWdaGPHy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader.documentInfo"
      ],
      "metadata": {
        "id": "dhD5DsFAH-PT",
        "outputId": "d2b69410-08be-4765-a0ac-726914a3364a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'/CreationDate': 'D:20220117013737Z',\n",
              " '/Creator': 'LaTeX with acmart  and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX',\n",
              " '/ModDate': 'D:20220117013737Z',\n",
              " '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
              " '/Producer': 'pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2',\n",
              " '/Subject': '',\n",
              " '/Title': 'Phocus: Picking Valuable Research from a Sea of Citations',\n",
              " '/Trapped': '/False'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reader.numPages\n",
        "#reader.getPage(1)"
      ],
      "metadata": {
        "id": "Oiz6vMkZIH1o",
        "outputId": "952f9db9-b64e-466d-def8-d902fdecf0fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reader.getPage(6).extractText() #works ok"
      ],
      "metadata": {
        "id": "d0-DR_7fIZlv",
        "outputId": "21868822-bbaa-4964-fa1c-5b66aa1dbb81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"P ho cus: Picking Valuable Research from a Sea of Citations\\n, ,\\n[14]\\nZhengjie Gao, Ao Feng, Xinyu Song, and Xi Wu. 2019. Target-Dep endent Senti-\\nment Classi˙cation With BERT.\\nIEEE Access\\n7 (2019), 154290\\x15154299.\\n[15]\\nBorja Gonzalez-Pereira, Vicente Guerrero-Bote, and Felix Moya-Anegon.\\n2009. The SJR indicator: A new indicator of journals' scienti˙c prestige.\\narXiv:0912.4141 [cs.DL]\\n[16]\\nKaiming He, Xiangyu Zhang, Shao qing Ren, and Jian Sun. 2015. De ep Residual\\nLearning for Image Re cognition. arXiv:1512.03385 [cs.CV]\\n[17]\\nJorge E. Hirsch. 2005. An index to quantify an individual's scienti˙c research\\noutput.\\nPro c. Natl. Acad. Sci. USA\\n102 (2005), 16569\\x1516572.\\n[18]\\nMickel Hoang, Oskar Alija Bihorac, and Jacob o Rouces. 2019. Asp e ct-Base d\\nSentiment Analysis using BERT. In\\nNODALIDA\\n.\\n[19]\\nMinghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li, and Yiwei Lv. 2019.\\nOp en-Domain Targete d Sentiment Analysis via Span-Base d Extraction and Clas-\\nsi˙cation. In\\nPro ce e dings of the 57th Annual Me eting of the Asso ciation for Comp u-\\ntational Linguistics\\n. Asso ciation for Computational Linguistics, F lorence, Italy,\\n537\\x15546. https://doi.org/10.18653/v1/P19- 1051\\n[20]\\nDavid Jurgens, Srijan Kumar, Raine Ho over, Daniel A. McFarland, and Dan\\nJurafsky. 2016. Citation Classi˙cation for Behavioral Analysis of a Scienti˙c Field.\\nArXiv\\nabs/1609.00435 (2016).\\n[21]\\nDain Kaplan, Ryu Iida, and Takenobu Tokunaga. 2009. Automatic extraction of\\ncitation contexts for research pap er summarization: A coreference-chain base d\\napproach. In\\nPro ce e dings of the 2009 Workshop on Text and Citation Analysis for\\nScholarly Digital Libraries (NLPIR4DL)\\n. 88\\x1595.\\n[22]\\nDain Kaplan, Takenobu Tokunaga, and Simone Teufel. 2016. Citation Blo ck\\nDetermination Using Textual Coherence.\\nJournal of Information Pro cessing\\n24, 3\\n(2016), 540\\x15553. https://doi.org/10.2197/ipsjjip.24.540\\n[23]\\nHaixia Liu. 2017. Sentiment Analysis of Citations Using Word2ve c.\\nArXiv\\nabs/1704.00177 (2017).\\n[24]\\nHuaishao Luo, Lei Ji, Tianrui Li, Daxin Jiang, and Nan Duan. 2020. GRACE:\\nGradient Harmonize d and Cascade d Lab eling for Asp e ct-base d Sentiment Anal-\\nysis. In\\nFindings of the Asso ciation for Computational Linguistics: EMNLP 2020\\n.\\nAsso ciation for Computational Linguistics, Online, 54\\x1564. https://doi.org/10.\\n18653/v1/2020.˙ndings- emnlp.6\\n[25]\\nShutian M a, Jin Xu, and Chengzhi Zhang. 2018. Automatic Identi˙cation of Cite d\\nText Spans: A Multi-Classi˙er Approach over Imbalance d Dataset.\\nScientometrics\\n116, 2 (aug 2018), 1303\\x151330. https://doi.org/10.1007/s11192- 018- 2754- 2\\n[26]\\nJessica L. Milstead. 1980. Citation Inde The or y and Application in Science,\\nTe chnology and Humanities. Wiley, O xford (1979), 274, $15.95.\\nInformation\\nPro cessing and Management\\n16 (1980).\\n[27]\\nHenk F. Mo e d. 2010. Measuring contextual citation impact of scienti˙c journals.\\nJournal of Informetrics\\n4, 3 (2 010), 265\\x15277. https://doi.org/10.1016/j.joi.2010.01.\\n002\\n[28]\\nLawrence Page, Sergey Brin, Raje ev Motwani, and Terr y Winograd. 1999.\\nThe\\nPageRank Citation Ranking: Bringing Order to the Web.\\nTe chnical Rep ort 1999-\\n66. Stanford InfoLab. http://ilpubs.stanford.e du:8090/422/ Previous numb er =\\nSIDL-WP-1999-0120.\\n[29]\\nVahe d Qazvinian and Dragomir Radev. 2010. Identifying Non-Explicit Citing\\nSentences for Citation-Base d Summarization.. In\\nPro ce e dings of the 48th annual\\nme eting of the asso ciation for computational linguistics\\n. 555\\x15564.\\n[30]\\nSebastian Ruder, Parsa Gha˛ari, and John G. Breslin. 2016. A Hierarchical Mo del\\nof Reviews for Asp e ct-base d Sentiment Analysis. In\\nEMNLP\\n.\\n[31]\\nPer O. Seglen. 1997. Why the impact factor of journals should not b e use d for\\nevaluating research.\\nBMJ\\n314 (1997), 497.\\n[32]\\nChi Sun, Luyao Huang, and Xip eng Qiu. 2019. Utilizing BERT for Asp e ct-Base d\\nSentiment Analysis via Constructing Auxiliar y Sentence. In\\nNAACL\\n.\\n[33]\\nSimone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006. Automatic classi˙ca-\\ntion of citation function. In\\nEMNLP\\n.\\n[34]\\nMaria Mihaela Trusca, Daan Wassenb erg, F lavius Frasincar, and Rommert Dekker.\\n2020. A Hybrid Approach for Asp e ct-Bas e d Sentiment Analysis Using De ep\\nContextual Word Emb e ddings and Hierarchical Attention. In\\nICWE\\n.\\n[35]\\nMarco Valenzuela, Vu A. Ha, and Oren Etzioni. 2015. Identifying Meaningful\\nCitations. In\\nAAAI Workshop: Scholarly Big Data\\n.\\n[36]\\nOlaf Wallaart and F lavius Frasincar. 2019. A Hybrid Approach for Asp e ct-Base d\\nSentiment Analysis Using a Lexicalize d Domain Ontology and Attentional Neural\\nMo dels. In\\nESWC\\n.\\n[37]\\nHongning Wang, Yue Lu, and ChengXiang Zhai. 2010. Latent asp e ct rating\\nanalysis on review text data: a rating regression approach.\\nPro ce e dings of the 16th\\nACM SIGKDD international conference on Knowle dge discover y and data mining\\n(2010).\\n[38]\\nHu Xu, Bing Liu, Lei Shu, and P hilip S. Yu. 2019. BERT Post-Training for Review\\nReading Comprehension and Asp e ct-base d Sentiment Analysis. In\\nNAACL\\n.\\n[39]\\nChr ysoula Zer va, Minh quo c Nghiem, Nhung T. H. Nguyen, and Sophia Anani-\\nadou. 2020. Cite d text span identi˙cation for scienti˙c summarisation using pre-\\ntraine d enco ders.\\nScientometrics\\n(7 May 2020). https://doi.org/10.1007/s11192-\\n020- 03455- z\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "writer = PyPDF2.PdfFileWriter()"
      ],
      "metadata": {
        "id": "MrxSkbtyIo17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pdfminer.six"
      ],
      "metadata": {
        "id": "zPD706w-I9zR",
        "outputId": "98e46524-6182-48d2-d8fb-7acc80cfb468",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20220319-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Collecting cryptography\n",
            "  Downloading cryptography-37.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 37.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six) (2.21)\n",
            "Installing collected packages: cryptography, pdfminer.six\n",
            "Successfully installed cryptography-37.0.1 pdfminer.six-20220319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from io import StringIO\n",
        "\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfparser import PDFParser"
      ],
      "metadata": {
        "id": "hKrdAGGRJPJB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_pdf_to_string(file_path):\n",
        "\toutput_string = StringIO()\n",
        "\twith open(file_path, 'rb') as in_file:\n",
        "\t    parser = PDFParser(in_file)\n",
        "\t    doc = PDFDocument(parser)\n",
        "\t    rsrcmgr = PDFResourceManager()\n",
        "\t    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
        "\t    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "\t    for page in PDFPage.create_pages(doc):\n",
        "\t        interpreter.process_page(page)\n",
        "\n",
        "\treturn(output_string.getvalue())\n"
      ],
      "metadata": {
        "id": "HkUe38wlJSov"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=convert_pdf_to_string(path)\n",
        "text1 = text.replace('\\x0c','')\n",
        "text2 = text1.split('.\\n\\n')\n",
        "refine=[t.replace('\\n',' ') for t in text2]"
      ],
      "metadata": {
        "id": "gvnafXPUKIOZ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents=re.split('^(al)',refine[10])"
      ],
      "metadata": {
        "id": "dBqIUd9_KRST"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sents"
      ],
      "metadata": {
        "id": "9DLAuknWMscj",
        "outputId": "a2fd2777-4d7c-4684-c959-5840a66eadca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2.1 Citation Classification In fact, there are already many kinds of research that have focused on citation classification. For example, Teufel et al',\n",
              " ' classify citation intents into 12 classes, using simple regular match to ex- tract features. Valenzuela et al',\n",
              " ' divide citations into 4 classes: highly influential, background, method and results citations, using SVM with an RBF kernel and random forests, taking 13 features into consideration: total number of direct citations, number of direct cita- tions per section, the total number of indirect citations and number of indirect citations per section, author overlap, is considered help- ful, citation appears in table and caption, 1/number of references, number of paper citations/all citations, the similarity between ab- stracts, PageRank[28], number of total citing papers after transitive closure, and field of the cited paper. While Jurgens et al',\n",
              " ' define 7 classes of citation intents: background, motivation, uses, extension, continuation, comparison or contrast, and future, with a Random Forest classifier trained using 4 types of features: structural fea- tures, lexical, morphological and grammatical features, field, and usage. Cohan et al',\n",
              " ' propose a multitask model using BiLSTM and attention mechanism to classify citation intents that is the primary task and predict the section where the citation occurs and where a sentence needs a citation that is auxiliary tasks and is used to assist the primary task2. They categorize intents into 3 classes:  background information, method, and result comparison. Besides, Cohan builds a citation intent dataset SciCite. Those works sim- ply classify citations according to intents but ignore the sentiment citing paper towards references, which is vital']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tabula-py"
      ],
      "metadata": {
        "id": "AGfGfz-XOD34",
        "outputId": "4851b22d-20b9-4d4f-e8ae-088708805025",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tabula-py\n",
            "  Downloading tabula_py-2.3.0-py3-none-any.whl (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tabula-py) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from tabula-py) (1.3.5)\n",
            "Collecting distro\n",
            "  Downloading distro-1.7.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.3->tabula-py) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25.3->tabula-py) (1.15.0)\n",
            "Installing collected packages: distro, tabula-py\n",
            "Successfully installed distro-1.7.0 tabula-py-2.3.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "input.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}